{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install polars datasets sentencepiece transformers sentence_transformers unbabel-comet keras keras-nlp scikit-learn jax tensorflow tensorflow-text tf-keras matplotlib fasttext wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Disable HF Tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Enable mixed precision training\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    token_limit: int = 128\n",
    "    dataset_limit: int = 10\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 2e-4\n",
    "    model_id: str = \"google/gemma-2b-it\"\n",
    "    accumulate_grad_batches: int = 4\n",
    "    precision: str = \"bf16-mixed\"\n",
    "    lora_rank: int = 4\n",
    "    lora_alpha: int = 8\n",
    "    lora_dropout: float = 0.1\n",
    "\n",
    "training_config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import polars as pl\n",
    "\n",
    "ds = load_dataset(\"Helsinki-NLP/europarl\", \"cs-en\")\n",
    "df = ds[\"train\"].to_polars()\n",
    "\n",
    "# Reduce the dataset to our training limit\n",
    "df = df.head(training_config.dataset_limit)\n",
    "\n",
    "# Let's check the structure\n",
    "print(df.schema)\n",
    "print(\"\\nFirst row:\")\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the translation column\n",
    "df_norm = df.select(\n",
    "    [\n",
    "        pl.col(\"translation\").struct.field(\"en\").alias(\"en\"),\n",
    "        pl.col(\"translation\").struct.field(\"cs\").alias(\"cs\"),\n",
    "        pl.col(\"translation\").struct.field(\"cs\").str.len_chars().alias(\"cs_len\"),\n",
    "        pl.col(\"translation\").struct.field(\"en\").str.len_chars().alias(\"en_len\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Let's check the normalized structure\n",
    "print(df_norm.schema)\n",
    "print(\"\\nFirst row:\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to preprocess the data to get achieve high quality results.\n",
    "\n",
    "First, let's clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(df):\n",
    "    \"\"\"\n",
    "    Clean the texts by replacing multiple spaces with a single space and stripping leading and trailing spaces.\n",
    "    We also filter out very short texts (less than 3 characters) and texts where one language is more than 2.5x longer than the other.\n",
    "    \"\"\"\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            # Clean the English text\n",
    "            pl.col(\"en\")\n",
    "            .str.replace_all(r\"\\s+\", \" \")\n",
    "            .str.strip_chars()\n",
    "            .alias(\"en_clean\"),\n",
    "            # Clean the Czech text\n",
    "            pl.col(\"cs\")\n",
    "            .str.replace_all(r\"\\s+\", \" \")\n",
    "            .str.strip_chars()\n",
    "            .alias(\"cs_clean\"),\n",
    "        ]\n",
    "    ).filter(\n",
    "        # Filter out rows with non a-Z characters\n",
    "        ~pl.col(\"cs_clean\").str.contains(r\"^[a-zA-Z]+$\")\n",
    "        & ~pl.col(\"en_clean\").str.contains(r\"^[a-zA-Z]+$\")\n",
    "        # Filter out very short texts (less than 3 characters)\n",
    "        & (pl.col(\"cs_len\") >= 3)\n",
    "        & (pl.col(\"en_len\") >= 3)\n",
    "        # This helps remove poor quality or misaligned translations\n",
    "        & (pl.col(\"cs_len\") / pl.col(\"en_len\") <= 2.5)  # Czech text not too long compared to English\n",
    "        & (pl.col(\"en_len\") / pl.col(\"cs_len\") <= 2.5)  # English text not too long compared to Czech\n",
    "    )\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = clean_texts(df_norm)\n",
    "print(f\"Dataset shape after cleaning: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a language detection check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import wget\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detect_language(df, threshold=0.8, batch_size=100):\n",
    "    \"\"\"\n",
    "    Detect the language of the text.\n",
    "    \"\"\"\n",
    "    print(f\"Detecting language of the text with threshold {threshold} and batch size {batch_size}\")\n",
    "\n",
    "    # Load the fasttext model\n",
    "    model_path = \"models/fasttext/lid.176.bin\"\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        wget.download(\"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\", out=model_path)\n",
    "\n",
    "    model = fasttext.load_model(model_path)\n",
    "\n",
    "    def predict_language(text):\n",
    "        \"\"\"\n",
    "        Check if the text is in the expected language.\n",
    "        \"\"\"\n",
    "        prediction = model.predict(text, k=1)\n",
    "        return prediction\n",
    "\n",
    "    # Process in batches\n",
    "    cs_clean = df['cs_clean'].to_list()\n",
    "    en_clean = df['en_clean'].to_list()\n",
    "\n",
    "    cs_results = []\n",
    "    en_results = []\n",
    "    bad_languages = []\n",
    "\n",
    "    for i in range(0, len(cs_clean), batch_size):\n",
    "        batch_cs = cs_clean[i:i+batch_size]\n",
    "        batch_en = en_clean[i:i+batch_size]\n",
    "\n",
    "        # Check if the text is in the expected language\n",
    "        for text in tqdm(batch_cs, desc=\"Checking Czech language\"):\n",
    "            result = predict_language(text)\n",
    "            if result[0][0] == \"__label__cs\":\n",
    "                cs_results.append(True)\n",
    "            else:\n",
    "                cs_results.append(False)\n",
    "                bad_languages.append(result[0][0])\n",
    "\n",
    "        for text in tqdm(batch_en, desc=\"Checking English language\"):\n",
    "            result = predict_language(text)\n",
    "            if result[0][0] == \"__label__en\":\n",
    "                en_results.append(True)\n",
    "            else:\n",
    "                en_results.append(False)\n",
    "                bad_languages.append(result[0][0])\n",
    "\n",
    "    print(f\"Detected bad languages: {bad_languages}\")\n",
    "\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            pl.Series(\"is_valid_cs\", cs_results),\n",
    "            pl.Series(\"is_valid_en\", en_results)\n",
    "        ]\n",
    "    ).filter(pl.col(\"is_valid_cs\") & pl.col(\"is_valid_en\"))\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = detect_language(df_norm)\n",
    "print(f\"Dataset shape after language detection: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out the rows based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "def filter_similar_texts(df, threshold=0.8, batch_size=100):\n",
    "    \"\"\"\n",
    "    Filter out the rows based on similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Filtering out rows with similarity less than {threshold}\")\n",
    "    print(f\"Processing in batches of {batch_size} rows\")\n",
    "    \n",
    "    # Process in batches\n",
    "    cs_clean = df['cs_clean'].to_list()\n",
    "    en_clean = df['en_clean'].to_list()\n",
    "    \n",
    "    similarities = []\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for i in range(0, len(cs_clean), batch_size):\n",
    "        batch_cs = cs_clean[i:i+batch_size]\n",
    "        batch_en = en_clean[i:i+batch_size]\n",
    "        \n",
    "        # Compute the embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings_cs = model.encode(batch_cs, convert_to_tensor=True)\n",
    "            embeddings_en = model.encode(batch_en, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute the similarity\n",
    "        batch_similarities = torch.nn.functional.cosine_similarity(embeddings_cs, embeddings_en, dim=1)\n",
    "        similarities.extend(batch_similarities.cpu().numpy())\n",
    "        \n",
    "    return df.with_columns(pl.Series(\"similarity\", similarities)).filter(pl.col(\"similarity\") > threshold)\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = filter_similar_texts(df_norm)\n",
    "print(f\"Filtered dataset shape: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform an alignment check. We will use COMET to check the quality of the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from comet import load_from_checkpoint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the COMET model\n",
    "model_path = snapshot_download(\"Unbabel/wmt22-comet-da\")\n",
    "model_checkpoint_path = f\"{model_path}/checkpoints/model.ckpt\"\n",
    "model = load_from_checkpoint(model_checkpoint_path)\n",
    "\n",
    "def filter_by_quality(df, treshold = 0.4, batch_size = 100):\n",
    "    \"\"\"\n",
    "    Filter out the rows based on the quality of the alignment.\n",
    "    \"\"\"\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Get texts\n",
    "    cs_clean = df[\"cs_clean\"].to_list()\n",
    "    en_clean = df[\"en_clean\"].to_list()\n",
    "\n",
    "    # Prepare data for COMET\n",
    "    data = [\n",
    "        {\"src\": cs, \"mt\": en, \"ref\": None}\n",
    "        for cs, en in zip(cs_clean, en_clean)\n",
    "    ]\n",
    "    \n",
    "    # Get scores from COMET\n",
    "    predictions = model.predict(data, batch_size=batch_size)\n",
    "    scores = predictions.scores\n",
    "\n",
    "    return df.with_columns(\n",
    "        [pl.Series(\"quality_score\", scores)]\n",
    "    ).filter(pl.col(\"quality_score\") > treshold)\n",
    "\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = filter_by_quality(df_norm)\n",
    "print(f\"Filtered dataset shape: {df_norm.shape}\")\n",
    "print(df_norm.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the distribution of the quality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distribution\n",
    "print(\n",
    "    df_norm.select(\n",
    "        [\n",
    "            pl.col(\"quality_score\").quantile(0.25).alias(\"25th_percentile\"),\n",
    "            pl.col(\"quality_score\").quantile(0.5).alias(\"median\"),\n",
    "            pl.col(\"quality_score\").quantile(0.75).alias(\"75th_percentile\"),\n",
    "            pl.col(\"quality_score\").mean().alias(\"mean\"),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the quality of the alignment is pretty good for most of the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's normalize the dataset to the instruction format and save the dataset to a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction_format(x):\n",
    "    \"\"\"Format input text into an instruction format for translation.\n",
    "\n",
    "    Args:\n",
    "        x: Dictionary containing 'en_clean' (English text) and 'cs_clean' (Czech translation)\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted instruction string with the following structure:\n",
    "            <start_of_turn>user\n",
    "            Přelož tento text z angličtiny do češtiny.\n",
    "            \"{English text}\"\n",
    "            <end_of_turn>\n",
    "            <start_of_turn>model\n",
    "            {Czech translation}<end_of_turn>\n",
    "    \"\"\"\n",
    "    return f\"<start_of_turn>user\\nPřelož tento text z angličtiny do češtiny.\\n\\\"{x['en_clean']}\\\"<end_of_turn>\\n<start_of_turn>model\\n{x['cs_clean']}<end_of_turn>\"\n",
    "\n",
    "finetune_df = df_norm.with_columns(\n",
    "    [\n",
    "        pl.struct([\"en_clean\", \"cs_clean\"])\n",
    "        .map_elements(instruction_format, return_dtype=pl.Utf8)\n",
    "        .alias(\"instruction\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(finetune_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we need to tokenize the dataset. First we will load the tokenizer and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(training_config.model_id)\n",
    "\n",
    "# Test the tokenizer\n",
    "print(tokenizer(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {finetune_df.shape}\")\n",
    "print(f\"Token limit: {training_config.token_limit}\")\n",
    "\n",
    "finetune_df = finetune_df.with_columns(\n",
    "    [\n",
    "        pl.col(\"instruction\")\n",
    "        .map_elements(lambda x: tokenizer(x).input_ids, return_dtype=pl.List(pl.Int32))\n",
    "        .alias(\"tokens\")\n",
    "    ]\n",
    ").filter(\n",
    "    pl.col(\"tokens\").list.len() <= training_config.token_limit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the token distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "token_lengths = finetune_df[\"tokens\"].map_elements(len, return_dtype=pl.Int32)\n",
    "print(\"Token length statistics:\")\n",
    "print(f\"Mean: {token_lengths.mean()}\")\n",
    "print(f\"Median: {token_lengths.median()}\")\n",
    "print(f\"95th percentile: {token_lengths.quantile(0.95)}\")\n",
    "print(f\"Max: {token_lengths.max()}\")\n",
    "\n",
    "# Plot the token length distribution\n",
    "plt.hist(token_lengths, bins=50)\n",
    "plt.title(\"Token Length Distribution\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, filter to the token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_df = finetune_df.select(\n",
    "    [\n",
    "        pl.col(\"instruction\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape post-tokenization: {finetune_df.shape}\")\n",
    "print(finetune_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally, let's calculate the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout rate is the percentage of rows that are filtered out.\n",
    "dropout_rate = (len(df) - len(finetune_df)) / len(df)\n",
    "print(f\"Dropout rate: {dropout_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout rate is pretty high, but it's a good thing that we have a lot of data to work with and we aim for high quality results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is ready for the next step. We will use it to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split into train and temp (80/20)\n",
    "train_df, temp_df = train_test_split(finetune_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then split temp into validation and test (50/50, resulting in 10/10 of original)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save all splits to parquet files\n",
    "train_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_train.parquet\", compression=\"zstd\"\n",
    ")\n",
    "valid_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_valid.parquet\", compression=\"zstd\"\n",
    ")\n",
    "test_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_test.parquet\", compression=\"zstd\"\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(valid_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model - PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pytorch_lightning\n",
    "import logging\n",
    "class GemmaLightningModule(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.std_logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Load base model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_id)\n",
    "\n",
    "        # Test prompts for monitoring\n",
    "        self.test_prompts = [\n",
    "            \"Přelož tento text z angličtiny do češtiny:\\n'I love machine learning and AI.'\",\n",
    "            \"Přelož tento text z angličtiny do češtiny:\\n'The weather is beautiful today.'\",\n",
    "            \"Přelož tento text z angličtiny do češtiny:\\n'Can you help me with directions to the train station?'\",\n",
    "        ]\n",
    "\n",
    "        # Setup LoRA\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=config.lora_rank,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            lora_dropout=config.lora_dropout,\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Generate sample translations after each validation epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "\n",
    "        for prompt in self.test_prompts:\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=self.config.token_limit\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            results.append([prompt, generated_text])\n",
    "        \n",
    "        # Log the results\n",
    "        for prompt, generated_text in results:\n",
    "            self.std_logger.info(f\"Generated text for prompt: {prompt}\\n{generated_text}\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        decay_params = []\n",
    "        no_decay_params = []\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if any(exclude_term in name for exclude_term in [\"bias\", \"scale\"]):\n",
    "                no_decay_params.append(param)\n",
    "            else:\n",
    "                decay_params.append(param)\n",
    "\n",
    "        optimizer_params = [\n",
    "            {\n",
    "                'params': decay_params,\n",
    "                'weight_decay': 0.01\n",
    "            },\n",
    "            {\n",
    "                'params': no_decay_params,\n",
    "                'weight_decay': 0.0   # No weight decay for excluded parameters\n",
    "            }\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optimizer_params,\n",
    "            lr=self.config.learning_rate,\n",
    "            eps=1e-6, # Default is 1e-8, but we need to use mixed precision training\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GemmaDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        # Ensure we're not returning tensors with an extra dimension\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        # Convert to tensors manually\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(\n",
    "                encodings[\"input_ids\"], dtype=torch.long\n",
    "            ),\n",
    "            \"attention_mask\": torch.tensor(\n",
    "                encodings[\"attention_mask\"], dtype=torch.long\n",
    "            ),\n",
    "            \"labels\": torch.tensor(\n",
    "                encodings[\"input_ids\"], dtype=torch.long\n",
    "            ),  # Added labels for training\n",
    "        }\n",
    "\n",
    "\n",
    "# Import the model\n",
    "model = AutoModelForCausalLM.from_pretrained(training_config.model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(training_config.model_id)\n",
    "\n",
    "# Create datasets\n",
    "train_ds = GemmaDataset(\n",
    "    train_df[\"instruction\"].to_list(), tokenizer, training_config.token_limit\n",
    ")\n",
    "valid_ds = GemmaDataset(\n",
    "    valid_df[\"instruction\"].to_list(), tokenizer, training_config.token_limit\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=training_config.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=training_config.batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor, RichProgressBar\n",
    "import wandb\n",
    "\n",
    "# Setup Wandb logger\n",
    "wandb_logger = pl.loggers.WandbLogger(\n",
    "    project=\"gemma-cs\",\n",
    "    name=f\"gemma-cs-{training_config.lora_rank}r\",\n",
    "    config={\n",
    "        \"model_name\": training_config.model_id,\n",
    "        \"lora_rank\": training_config.lora_rank,\n",
    "        \"batch_size\": training_config.batch_size,\n",
    "        \"learning_rate\": training_config.learning_rate,\n",
    "        \"epochs\": training_config.epochs,\n",
    "        \"token_limit\": training_config.token_limit,\n",
    "        \"accumulate_grad_batches\": training_config.accumulate_grad_batches,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    # Save best models\n",
    "    ModelCheckpoint(\n",
    "        dirpath=f\"checkpoints/gemma-cs-{training_config.lora_rank}r\",\n",
    "        filename=\"model-{epoch:02d}-{val/loss:.2f}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=3,\n",
    "        save_last=True,\n",
    "    ),\n",
    "    # Early stopping\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\", min_delta=0.001),\n",
    "    # Monitor learning rate\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "    # Rich progress bar\n",
    "    RichProgressBar(),\n",
    "]\n",
    "\n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=training_config.epochs,\n",
    "    precision=training_config.precision,\n",
    "    accumulate_grad_batches=training_config.accumulate_grad_batches,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=callbacks,\n",
    "    # Log gradients & model parameters\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.25,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = GemmaLightningModule(training_config)\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "wandb.finish()\n",
    "\n",
    "# Save the model\n",
    "model.model.save_pretrained(\n",
    "    f\"models/gemma-cs-{training_config.lora_rank}r-final\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(training_config.model_id)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the inference of our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Přelož tento text z angličtiny do češtiny:\\n\"Hello, world!\"'\n",
    "print(\n",
    "    gemma_lm.generate(\n",
    "        prompt,\n",
    "        max_length=training_config.token_limit,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LoRA on the model\n",
    "gemma_lm.backbone.enable_lora(rank=training_config.lora_rank)\n",
    "gemma_lm.summary()\n",
    "\n",
    "# Control memory usage\n",
    "gemma_lm.preprocessor.sequence_length = training_config.token_limit\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=training_config.learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    epsilon=1e-6, # Default is 1e-8, but we need to use mixed precision training\n",
    ")\n",
    "\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "# Setup loss\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Setup metrics\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "# Compile the model\n",
    "gemma_lm.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the dataset to Keras expected format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_df[\"instruction\"].to_list()\n",
    "valid_dataset = valid_df[\"instruction\"].to_list()\n",
    "\n",
    "print(f\"Train dataset shape: {len(train_dataset)}\")\n",
    "print(train_dataset[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Valid dataset shape: {len(valid_dataset)}\")\n",
    "print(valid_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now ready for training. Let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "import wandb\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch} finished\")\n",
    "        model_name = f\"gemma_cs_{training_config.lora_rank}_epoch{epoch+1}.lora.h5\"\n",
    "        gemma_lm.backbone.save_lora_weights(model_name)\n",
    "        print(f\"Model {model_name} saved\")\n",
    "\n",
    "        # Eval on prompt\n",
    "        prompt = \"Přelož tento text z angličtiny do češtiny.\\n\\\"Hello, world!\\\"\"\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(\n",
    "            f\"Response: {gemma_lm.generate(prompt, max_length=training_config.token_limit)}\"\n",
    "        )\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(config={\n",
    "    \"learning_rate\": training_config.learning_rate,\n",
    "    \"epochs\": training_config.epochs,\n",
    "    \"batch_size\": training_config.batch_size,\n",
    "    \"lora_rank\": training_config.lora_rank,\n",
    "    \"token_limit\": training_config.token_limit,\n",
    "    \"model\": \"gemma2_instruct_2b_en\",\n",
    "    \"task\": \"czech-context\"\n",
    "})\n",
    "\n",
    "# Convert lists to TensorFlow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices(valid_dataset)\n",
    "\n",
    "# Configure datasets for performance\n",
    "train_ds = train_ds.batch(training_config.batch_size)\n",
    "valid_ds = valid_ds.batch(training_config.batch_size)\n",
    "\n",
    "# Enable prefetching to improve performance\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "history = gemma_lm.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=training_config.epochs,\n",
    "    callbacks=[CustomCallback(), WandbMetricsLogger()],\n",
    ")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.plot(history.history[\"sparse_categorical_accuracy\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-czech-adaptation-iLYBdvQN-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
