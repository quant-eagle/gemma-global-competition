{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jirka/.pyenv/versions/3.10.15/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (1.18.0)\n",
      "Requirement already satisfied: datasets in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (4.47.1)\n",
      "Requirement already satisfied: sentence_transformers in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: unbabel-comet in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: keras-nlp in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: jax in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.4.38)\n",
      "Requirement already satisfied: tensorflow in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (2.18.0)\n",
      "Collecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.18.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: filelock in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scipy in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\n",
      "Requirement already satisfied: Pillow in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (4.25.5)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (2.5.0.post0)\n",
      "Requirement already satisfied: sacrebleu<3.0.0,>=2.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (2.5.1)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: absl-py in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: keras-hub==0.18.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras-nlp) (0.18.1)\n",
      "Requirement already satisfied: kagglehub in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (0.3.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: jaxlib<=0.4.38,>=0.4.38 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from jax) (0.4.38)\n",
      "Requirement already satisfied: opt_einsum in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: setuptools in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.17.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.11.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: portalocker in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.9.0)\n",
      "Requirement already satisfied: colorama in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.4.6)\n",
      "Requirement already satisfied: lxml in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (5.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: networkx in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Downloading tensorflow_text-2.18.1-cp310-cp310-macosx_11_0_arm64.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorflow-text\n",
      "Successfully installed tensorflow-text-2.18.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install polars datasets sentencepiece transformers sentence_transformers unbabel-comet keras keras-nlp scikit-learn jax tensorflow tensorflow-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable HF Tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set the backend\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema([('translation', Struct({'cs': String, 'en': String}))])\n",
      "\n",
      "First row:\n",
      "shape: (1, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ translation                     │\n",
      "│ ---                             │\n",
      "│ struct[2]                       │\n",
      "╞═════════════════════════════════╡\n",
      "│ {\"Následný postup na základě u… │\n",
      "└─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import polars as pl\n",
    "\n",
    "ds = load_dataset(\"Helsinki-NLP/europarl\", \"cs-en\")\n",
    "df = ds[\"train\"].to_polars()\n",
    "\n",
    "# Reduce the dataset to 1000 rows\n",
    "df = df.head(1000)\n",
    "\n",
    "# Let's check the structure\n",
    "print(df.schema)\n",
    "print(\"\\nFirst row:\")\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema([('en', String), ('cs', String), ('cs_len', UInt32), ('en_len', UInt32)])\n",
      "\n",
      "First row:\n",
      "shape: (1, 4)\n",
      "┌─────────────────────────────────┬─────────────────────────────────┬────────┬────────┐\n",
      "│ en                              ┆ cs                              ┆ cs_len ┆ en_len │\n",
      "│ ---                             ┆ ---                             ┆ ---    ┆ ---    │\n",
      "│ str                             ┆ str                             ┆ u32    ┆ u32    │\n",
      "╞═════════════════════════════════╪═════════════════════════════════╪════════╪════════╡\n",
      "│ Action taken on Parliament's r… ┆ Následný postup na základě usn… ┆ 57     ┆ 53     │\n",
      "└─────────────────────────────────┴─────────────────────────────────┴────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# Normalize the translation column\n",
    "df_norm = df.select(\n",
    "    [\n",
    "        pl.col(\"translation\").struct.field(\"en\").alias(\"en\"),\n",
    "        pl.col(\"translation\").struct.field(\"cs\").alias(\"cs\"),\n",
    "        pl.col(\"translation\").struct.field(\"cs\").str.len_chars().alias(\"cs_len\"),\n",
    "        pl.col(\"translation\").struct.field(\"en\").str.len_chars().alias(\"en_len\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Let's check the normalized structure\n",
    "print(df_norm.schema)\n",
    "print(\"\\nFirst row:\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to preprocess the data to get achieve high quality results.\n",
    "\n",
    "First, let's clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1000, 4)\n",
      "Dataset shape after cleaning: (840, 6)\n",
      "shape: (1, 6)\n",
      "┌────────────────────┬───────────────────┬────────┬────────┬───────────────────┬───────────────────┐\n",
      "│ en                 ┆ cs                ┆ cs_len ┆ en_len ┆ en_clean          ┆ cs_clean          │\n",
      "│ ---                ┆ ---               ┆ ---    ┆ ---    ┆ ---               ┆ ---               │\n",
      "│ str                ┆ str               ┆ u32    ┆ u32    ┆ str               ┆ str               │\n",
      "╞════════════════════╪═══════════════════╪════════╪════════╪═══════════════════╪═══════════════════╡\n",
      "│ Action taken on    ┆ Následný postup   ┆ 57     ┆ 53     ┆ Action taken on   ┆ Následný postup   │\n",
      "│ Parliament's r…    ┆ na základě usn…   ┆        ┆        ┆ Parliament's r…   ┆ na základě usn…   │\n",
      "└────────────────────┴───────────────────┴────────┴────────┴───────────────────┴───────────────────┘\n"
     ]
    }
   ],
   "source": [
    "def clean_texts(df):\n",
    "    \"\"\"\n",
    "    Clean the texts by replacing multiple spaces with a single space and stripping leading and trailing spaces.\n",
    "    We also filter out very short texts (less than 3 characters) and texts where one language is more than 2.5x longer than the other.\n",
    "    \"\"\"\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            # Clean the English text\n",
    "            pl.col(\"en\")\n",
    "            .str.replace_all(r\"\\s+\", \" \")\n",
    "            .str.strip_chars()\n",
    "            .alias(\"en_clean\"),\n",
    "            # Clean the Czech text\n",
    "            pl.col(\"cs\")\n",
    "            .str.replace_all(r\"\\s+\", \" \")\n",
    "            .str.strip_chars()\n",
    "            .alias(\"cs_clean\"),\n",
    "        ]\n",
    "    ).filter(\n",
    "        # Filter out rows with non a-Z characters\n",
    "        ~pl.col(\"cs_clean\").str.contains(r\"^[a-zA-Z]+$\")\n",
    "        & ~pl.col(\"en_clean\").str.contains(r\"^[a-zA-Z]+$\")\n",
    "        # Filter out very short texts (less than 3 characters)\n",
    "        & (pl.col(\"cs_len\") >= 3)\n",
    "        & (pl.col(\"en_len\") >= 3)\n",
    "        # This helps remove poor quality or misaligned translations\n",
    "        & (pl.col(\"cs_len\") / pl.col(\"en_len\") <= 2.5)  # Czech text not too long compared to English\n",
    "        & (pl.col(\"en_len\") / pl.col(\"cs_len\") <= 2.5)  # English text not too long compared to Czech\n",
    "    )\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = clean_texts(df_norm)\n",
    "print(f\"Dataset shape after cleaning: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's tokenize the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset shape: (840, 8)\n",
      "shape: (1, 8)\n",
      "┌─────────────┬─────────────┬────────┬────────┬─────────────┬────────────┬────────────┬────────────┐\n",
      "│ en          ┆ cs          ┆ cs_len ┆ en_len ┆ en_clean    ┆ cs_clean   ┆ en_tokens  ┆ cs_tokens  │\n",
      "│ ---         ┆ ---         ┆ ---    ┆ ---    ┆ ---         ┆ ---        ┆ ---        ┆ ---        │\n",
      "│ str         ┆ str         ┆ u32    ┆ u32    ┆ str         ┆ str        ┆ object     ┆ object     │\n",
      "╞═════════════╪═════════════╪════════╪════════╪═════════════╪════════════╪════════════╪════════════╡\n",
      "│ Action      ┆ Následný    ┆ 57     ┆ 53     ┆ Action      ┆ Následný   ┆ tensor([[  ┆ tensor([[2 │\n",
      "│ taken on    ┆ postup na   ┆        ┆        ┆ taken on    ┆ postup na  ┆ 57945,     ┆ 40806,     │\n",
      "│ Parliament' ┆ základě     ┆        ┆        ┆ Parliament' ┆ základě    ┆ 39958,     ┆ 1673,      │\n",
      "│ s r…        ┆ usn…        ┆        ┆        ┆ s r…        ┆ usn…       ┆ 9…         ┆ 2098…      │\n",
      "└─────────────┴─────────────┴────────┴────────┴─────────────┴────────────┴────────────┴────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartTokenizer\n",
    "\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "\n",
    "def tokenize_texts(df):\n",
    "    \"\"\"\n",
    "    Tokenize the pairs of texts.\n",
    "    \"\"\"\n",
    "    return df.with_columns([\n",
    "        pl.col(\"en_clean\").map_elements(lambda x: tokenizer.encode(x, return_tensors=\"pt\")).alias(\"en_tokens\"),\n",
    "        pl.col(\"cs_clean\").map_elements(lambda x: tokenizer.encode(x, return_tensors=\"pt\")).alias(\"cs_tokens\")\n",
    "    ])\n",
    "\n",
    "# Tokenize the texts\n",
    "df_norm = tokenize_texts(df_norm)\n",
    "print(f\"Tokenized dataset shape: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out the rows based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (840, 8)\n",
      "Filtering out rows with similarity less than 0.8\n",
      "Processing in batches of 100 rows\n",
      "Filtered dataset shape: (780, 9)\n",
      "shape: (1, 9)\n",
      "┌────────────┬────────────┬────────┬────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ en         ┆ cs         ┆ cs_len ┆ en_len ┆ … ┆ cs_clean   ┆ en_tokens  ┆ cs_tokens  ┆ similarit │\n",
      "│ ---        ┆ ---        ┆ ---    ┆ ---    ┆   ┆ ---        ┆ ---        ┆ ---        ┆ y         │\n",
      "│ str        ┆ str        ┆ u32    ┆ u32    ┆   ┆ str        ┆ object     ┆ object     ┆ ---       │\n",
      "│            ┆            ┆        ┆        ┆   ┆            ┆            ┆            ┆ f32       │\n",
      "╞════════════╪════════════╪════════╪════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ Action     ┆ Následný   ┆ 57     ┆ 53     ┆ … ┆ Následný   ┆ tensor([[  ┆ tensor([[2 ┆ 0.800876  │\n",
      "│ taken on   ┆ postup na  ┆        ┆        ┆   ┆ postup na  ┆ 57945,     ┆ 40806,     ┆           │\n",
      "│ Parliament ┆ základě    ┆        ┆        ┆   ┆ základě    ┆ 39958,     ┆ 1673,      ┆           │\n",
      "│ 's r…      ┆ usn…       ┆        ┆        ┆   ┆ usn…       ┆ 9…         ┆ 2098…      ┆           │\n",
      "└────────────┴────────────┴────────┴────────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "def filter_similar_texts(df, threshold=0.8, batch_size=100):\n",
    "    \"\"\"\n",
    "    Filter out the rows based on similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Filtering out rows with similarity less than {threshold}\")\n",
    "    print(f\"Processing in batches of {batch_size} rows\")\n",
    "    \n",
    "    # Process in batches\n",
    "    cs_clean = df['cs_clean'].to_list()\n",
    "    en_clean = df['en_clean'].to_list()\n",
    "    \n",
    "    similarities = []\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for i in range(0, len(cs_clean), batch_size):\n",
    "        batch_cs = cs_clean[i:i+batch_size]\n",
    "        batch_en = en_clean[i:i+batch_size]\n",
    "        \n",
    "        # Compute the embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings_cs = model.encode(batch_cs, convert_to_tensor=True)\n",
    "            embeddings_en = model.encode(batch_en, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute the similarity\n",
    "        batch_similarities = torch.nn.functional.cosine_similarity(embeddings_cs, embeddings_en, dim=1)\n",
    "        similarities.extend(batch_similarities.cpu().numpy())\n",
    "        \n",
    "    return df.with_columns(pl.Series(\"similarity\", similarities)).filter(pl.col(\"similarity\") > threshold)\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = filter_similar_texts(df_norm)\n",
    "print(f\"Filtered dataset shape: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform an alignment check. We will use COMET to check the quality of the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c340e9ddb64493b2b93cc69178fe0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (780, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset shape: (630, 10)\n",
      "shape: (10, 10)\n",
      "┌────────────┬────────────┬────────┬────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ en         ┆ cs         ┆ cs_len ┆ en_len ┆ … ┆ en_tokens  ┆ cs_tokens  ┆ similarity ┆ quality_s │\n",
      "│ ---        ┆ ---        ┆ ---    ┆ ---    ┆   ┆ ---        ┆ ---        ┆ ---        ┆ core      │\n",
      "│ str        ┆ str        ┆ u32    ┆ u32    ┆   ┆ object     ┆ object     ┆ f32        ┆ ---       │\n",
      "│            ┆            ┆        ┆        ┆   ┆            ┆            ┆            ┆ f64       │\n",
      "╞════════════╪════════════╪════════╪════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ Action     ┆ Následný   ┆ 57     ┆ 53     ┆ … ┆ tensor([[  ┆ tensor([[2 ┆ 0.800876   ┆ 0.463111  │\n",
      "│ taken on   ┆ postup na  ┆        ┆        ┆   ┆ 57945,     ┆ 40806,     ┆            ┆           │\n",
      "│ Parliament ┆ základě    ┆        ┆        ┆   ┆ 39958,     ┆ 1673,      ┆            ┆           │\n",
      "│ 's r…      ┆ usn…       ┆        ┆        ┆   ┆ 9…         ┆ 2098…      ┆            ┆           │\n",
      "│ Documents  ┆ Předložení ┆ 31     ┆ 31     ┆ … ┆ tensor([[  ┆ tensor([[  ┆ 0.819701   ┆ 0.456948  │\n",
      "│ received:  ┆ dokumentů: ┆        ┆        ┆   ┆ 43101,     ┆ 38494,     ┆            ┆           │\n",
      "│ see        ┆ viz zápi…  ┆        ┆        ┆   ┆ 7,  7520…  ┆ 365,       ┆            ┆           │\n",
      "│ Minute…    ┆            ┆        ┆        ┆   ┆            ┆ 3589…      ┆            ┆           │\n",
      "│ Written    ┆ Písemná    ┆ 57     ┆ 42     ┆ … ┆ tensor([[1 ┆ tensor([[1 ┆ 0.909596   ┆ 0.42378   │\n",
      "│ statements ┆ prohlášení ┆        ┆        ┆   ┆ 28538,     ┆ 01903,     ┆            ┆           │\n",
      "│ (Rule      ┆ (článek    ┆        ┆        ┆   ┆ 63805,     ┆ 5765,      ┆            ┆           │\n",
      "│ 116):…     ┆ 116…       ┆        ┆        ┆   ┆ …          ┆ 111…       ┆            ┆           │\n",
      "│ Texts of   ┆ Texty      ┆ 35     ┆ 57     ┆ … ┆ tensor([[  ┆ tensor([[  ┆ 0.895811   ┆ 0.400278  │\n",
      "│ agreements ┆ smluv      ┆        ┆        ┆   ┆ 24129,     ┆ 24129,     ┆            ┆           │\n",
      "│ forwarded  ┆ dodané     ┆        ┆        ┆   ┆ 7,    11…  ┆ 53,  1405… ┆            ┆           │\n",
      "│ …          ┆ Radou: viz ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│            ┆ …          ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ Membership ┆ Složení    ┆ 29     ┆ 37     ┆ … ┆ tensor([[  ┆ tensor([[  ┆ 0.911823   ┆ 0.432201  │\n",
      "│ of Parliam ┆ Parlamentu ┆        ┆        ┆   ┆ 74057,     ┆ 83241,     ┆            ┆           │\n",
      "│ ent: see … ┆ : viz      ┆        ┆        ┆   ┆ 16070,     ┆ 35898,     ┆            ┆           │\n",
      "│            ┆ zápis      ┆        ┆        ┆   ┆ 11…        ┆ 1486…      ┆            ┆           │\n",
      "│ Membership ┆ Členství   ┆ 44     ┆ 53     ┆ … ┆ tensor([[  ┆ tensor([[2 ┆ 0.935291   ┆ 0.454606  │\n",
      "│ of         ┆ ve         ┆        ┆        ┆   ┆ 74057,     ┆ 09396,     ┆            ┆           │\n",
      "│ committees ┆ výborech a ┆        ┆        ┆   ┆ 16070,     ┆ 22374,     ┆            ┆           │\n",
      "│ and d…     ┆ delegac…   ┆        ┆        ┆   ┆ 11…        ┆ 17…        ┆            ┆           │\n",
      "│ Agenda for ┆ Pořad      ┆ 42     ┆ 36     ┆ … ┆ tensor([[  ┆ tensor([[  ┆ 0.929281   ┆ 0.472327  │\n",
      "│ next       ┆ jednání    ┆        ┆        ┆   ┆ 89660,     ┆ 663,       ┆            ┆           │\n",
      "│ sitting:   ┆ příštího   ┆        ┆        ┆   ┆ 100,       ┆ 72849,     ┆            ┆           │\n",
      "│ see M…     ┆ zasedán…   ┆        ┆        ┆   ┆ 1173…      ┆ 14140…     ┆            ┆           │\n",
      "│ Closure of ┆ Ukončení   ┆ 17     ┆ 18     ┆ … ┆ tensor([[  ┆ tensor([[  ┆ 0.800042   ┆ 0.451879  │\n",
      "│ sitting    ┆ zasedání   ┆        ┆        ┆   ┆ 51053,     ┆ 345,       ┆            ┆           │\n",
      "│            ┆            ┆        ┆        ┆   ┆ 56851,     ┆ 1716,      ┆            ┆           │\n",
      "│            ┆            ┆        ┆        ┆   ┆ 11…        ┆ 2819…      ┆            ┆           │\n",
      "│ (The       ┆ (La seduta ┆ 30     ┆ 38     ┆ … ┆ tensor([[  ┆ tensor([[  ┆ 0.903965   ┆ 0.481405  │\n",
      "│ sitting    ┆ è tolta    ┆        ┆        ┆   ┆ 15,        ┆ 15,        ┆            ┆           │\n",
      "│ was closed ┆ alle       ┆        ┆        ┆   ┆ 3957,      ┆ 2729,      ┆            ┆           │\n",
      "│ at 11.…    ┆ 23.55)     ┆        ┆        ┆   ┆ 12984…     ┆ 173…       ┆            ┆           │\n",
      "│ Opening of ┆ Zahájení   ┆ 17     ┆ 22     ┆ … ┆ tensor([[  ┆ tensor([[  ┆ 0.835594   ┆ 0.483115  │\n",
      "│ the        ┆ zasedání   ┆        ┆        ┆   ┆ 13527,     ┆ 825,       ┆            ┆           │\n",
      "│ sitting    ┆            ┆        ┆        ┆   ┆ 214,       ┆ 11861,     ┆            ┆           │\n",
      "│            ┆            ┆        ┆        ┆   ┆ 11…        ┆ 23…        ┆            ┆           │\n",
      "└────────────┴────────────┴────────┴────────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from comet import load_from_checkpoint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the COMET model\n",
    "model_path = snapshot_download(\"Unbabel/wmt22-comet-da\")\n",
    "model_checkpoint_path = f\"{model_path}/checkpoints/model.ckpt\"\n",
    "model = load_from_checkpoint(model_checkpoint_path)\n",
    "\n",
    "def filter_by_quality(df, treshold = 0.4, batch_size = 100):\n",
    "    \"\"\"\n",
    "    Filter out the rows based on the quality of the alignment.\n",
    "    \n",
    "    This actually means the quality of the alignment, not the quality of the translation.\n",
    "    Typical interpretations for the quality of the alignment in COMET:\n",
    "    < 0: Poor quality\n",
    "    0 to 0.3: Fair quality\n",
    "    0.3 to 0.6: Good quality\n",
    "    > 0.6: Excellent quality\n",
    "    \"\"\"\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Get texts\n",
    "    cs_clean = df[\"cs_clean\"].to_list()\n",
    "    en_clean = df[\"en_clean\"].to_list()\n",
    "\n",
    "    # Prepare data for COMET\n",
    "    data = [\n",
    "        {\"src\": cs, \"mt\": en, \"ref\": None}\n",
    "        for cs, en in zip(cs_clean, en_clean)\n",
    "    ]\n",
    "    \n",
    "    # Get scores from COMET\n",
    "    predictions = model.predict(data, batch_size=batch_size)\n",
    "    scores = predictions.scores\n",
    "\n",
    "    return df.with_columns(\n",
    "        [pl.Series(\"quality_score\", scores)]\n",
    "    ).filter(pl.col(\"quality_score\") > treshold)\n",
    "\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = filter_by_quality(df_norm)\n",
    "print(f\"Filtered dataset shape: {df_norm.shape}\")\n",
    "print(df_norm.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the distribution of the quality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌─────────────────┬──────────┬─────────────────┬─────────┐\n",
      "│ 25th_percentile ┆ median   ┆ 75th_percentile ┆ mean    │\n",
      "│ ---             ┆ ---      ┆ ---             ┆ ---     │\n",
      "│ f64             ┆ f64      ┆ f64             ┆ f64     │\n",
      "╞═════════════════╪══════════╪═════════════════╪═════════╡\n",
      "│ 0.432053        ┆ 0.456948 ┆ 0.52894         ┆ 0.49383 │\n",
      "└─────────────────┴──────────┴─────────────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "# Look at distribution\n",
    "print(\n",
    "    df_norm.select(\n",
    "        [\n",
    "            pl.col(\"quality_score\").quantile(0.25).alias(\"25th_percentile\"),\n",
    "            pl.col(\"quality_score\").quantile(0.5).alias(\"median\"),\n",
    "            pl.col(\"quality_score\").quantile(0.75).alias(\"75th_percentile\"),\n",
    "            pl.col(\"quality_score\").mean().alias(\"mean\"),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the quality of the alignment is pretty good for most of the rows.\n",
    "\n",
    "Now let's calculate the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout rate: 37.00%\n"
     ]
    }
   ],
   "source": [
    "# Dropout rate is the percentage of rows that are filtered out.\n",
    "dropout_rate = (len(df) - len(df_norm)) / len(df)\n",
    "print(f\"Dropout rate: {dropout_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the dataset to a parquet file for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns we can save to parquet (no tokens)\n",
    "processed_df = df_norm.select(\n",
    "    [\"en_clean\", \"cs_clean\", \"cs_len\", \"en_len\", \"similarity\", \"quality_score\"]\n",
    ")\n",
    "\n",
    "# Now save to parquet\n",
    "processed_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_clean.parquet\", compression=\"zstd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is ready for the next step. We will use it to train the model.\n",
    "\n",
    "Before doing so, we will create a dataset format that is suitable for the model instruction format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────────┬─────────────────────────────────┬─────────────────────────────────┐\n",
      "│ instruction         ┆ en_clean                        ┆ cs_clean                        │\n",
      "│ ---                 ┆ ---                             ┆ ---                             │\n",
      "│ str                 ┆ str                             ┆ str                             │\n",
      "╞═════════════════════╪═════════════════════════════════╪═════════════════════════════════╡\n",
      "│ <start_of_turn>user ┆ Action taken on Parliament's r… ┆ Následný postup na základě usn… │\n",
      "│ Přelož ten…         ┆                                 ┆                                 │\n",
      "│ <start_of_turn>user ┆ Documents received: see Minute… ┆ Předložení dokumentů: viz zápi… │\n",
      "│ Přelož ten…         ┆                                 ┆                                 │\n",
      "│ <start_of_turn>user ┆ Written statements (Rule 116):… ┆ Písemná prohlášení (článek 116… │\n",
      "│ Přelož ten…         ┆                                 ┆                                 │\n",
      "│ <start_of_turn>user ┆ Texts of agreements forwarded … ┆ Texty smluv dodané Radou: viz … │\n",
      "│ Přelož ten…         ┆                                 ┆                                 │\n",
      "│ <start_of_turn>user ┆ Membership of Parliament: see … ┆ Složení Parlamentu: viz zápis   │\n",
      "│ Přelož ten…         ┆                                 ┆                                 │\n",
      "└─────────────────────┴─────────────────────────────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "def instruction_format(x):\n",
    "    return f\"<start_of_turn>user\\nPřelož tento text z angličtiny do češtiny.\\n\\\"{x['en_clean']}\\\"<end_of_turn>\\n<start_of_turn>model\\n{x['cs_clean']}<end_of_turn>\"\n",
    "\n",
    "# Now let's create the finetune dataset\n",
    "train_df = processed_df.select(\n",
    "    [\n",
    "        pl.struct([\"en_clean\", \"cs_clean\"]).map_elements(\n",
    "            instruction_format,\n",
    "            return_dtype=pl.Utf8\n",
    "        ).alias(\"instruction\"),\n",
    "        pl.col(\"en_clean\"),\n",
    "        pl.col(\"cs_clean\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is ready for the next step. We will use it to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 504\n",
      "Validation size: 63\n",
      "Test size: 63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split into train and temp (80/20)\n",
    "train_df_main, temp_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then split temp into validation and test (50/50, resulting in 10/10 of original)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save all splits to parquet files\n",
    "train_df_main.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_train.parquet\", compression=\"zstd\"\n",
    ")\n",
    "valid_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_valid.parquet\", compression=\"zstd\"\n",
    ")\n",
    "test_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_test.parquet\", compression=\"zstd\"\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df_main)}\")\n",
    "print(f\"Validation size: {len(valid_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma2/keras/gemma2_instruct_2b_en/1/download/assets/tokenizer/vocabulary.spm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.04M/4.04M [00:00<00:00, 4.53MB/s]\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "model_id = \"gemma2_instruct_2b_en\"\n",
    "\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-czech-adaptation-iLYBdvQN-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
