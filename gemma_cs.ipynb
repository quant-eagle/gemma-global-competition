{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (1.18.0)\n",
      "Requirement already satisfied: datasets in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (4.47.1)\n",
      "Requirement already satisfied: sentence_transformers in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: unbabel-comet in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: keras-nlp in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: jax in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.4.38)\n",
      "Requirement already satisfied: tensorflow in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-text in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (2.18.1)\n",
      "Requirement already satisfied: tf-keras in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: matplotlib in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: fasttext in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.9.3)\n",
      "Requirement already satisfied: wget in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.2)\n",
      "Requirement already satisfied: filelock in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scipy in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\n",
      "Requirement already satisfied: Pillow in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (4.25.5)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (2.5.0.post0)\n",
      "Requirement already satisfied: sacrebleu<3.0.0,>=2.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (2.5.1)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: absl-py in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: keras-hub==0.18.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras-nlp) (0.18.1)\n",
      "Requirement already satisfied: kagglehub in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (0.3.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: jaxlib<=0.4.38,>=0.4.38 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from jax) (0.4.38)\n",
      "Requirement already satisfied: opt_einsum in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: setuptools in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.17.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.11.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: portalocker in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.9.0)\n",
      "Requirement already satisfied: colorama in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.4.6)\n",
      "Requirement already satisfied: lxml in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (5.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: networkx in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install polars datasets sentencepiece transformers sentence_transformers unbabel-comet keras keras-nlp scikit-learn jax tensorflow tensorflow-text tf-keras matplotlib fasttext wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Disable HF Tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Enable mixed precision training\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    token_limit: int = 128\n",
    "    dataset_limit: int = 10\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 2e-4\n",
    "    model_id: str = \"google/gemma-2b-it\"\n",
    "    accumulate_grad_batches: int = 4\n",
    "    precision: str = \"bf16-mixed\"\n",
    "    lora_rank: int = 4\n",
    "    lora_alpha: int = 8\n",
    "    lora_dropout: float = 0.1\n",
    "\n",
    "training_config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema([('translation', Struct({'cs': String, 'en': String}))])\n",
      "\n",
      "First row:\n",
      "shape: (1, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ translation                     │\n",
      "│ ---                             │\n",
      "│ struct[2]                       │\n",
      "╞═════════════════════════════════╡\n",
      "│ {\"Následný postup na základě u… │\n",
      "└─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import polars as pl\n",
    "\n",
    "ds = load_dataset(\"Helsinki-NLP/europarl\", \"cs-en\")\n",
    "df = ds[\"train\"].to_polars()\n",
    "\n",
    "# Reduce the dataset to our training limit\n",
    "df = df.head(training_config.dataset_limit)\n",
    "\n",
    "# Let's check the structure\n",
    "print(df.schema)\n",
    "print(\"\\nFirst row:\")\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema([('en', String), ('cs', String), ('cs_len', UInt32), ('en_len', UInt32)])\n",
      "\n",
      "First row:\n",
      "shape: (1, 4)\n",
      "┌─────────────────────────────────┬─────────────────────────────────┬────────┬────────┐\n",
      "│ en                              ┆ cs                              ┆ cs_len ┆ en_len │\n",
      "│ ---                             ┆ ---                             ┆ ---    ┆ ---    │\n",
      "│ str                             ┆ str                             ┆ u32    ┆ u32    │\n",
      "╞═════════════════════════════════╪═════════════════════════════════╪════════╪════════╡\n",
      "│ Action taken on Parliament's r… ┆ Následný postup na základě usn… ┆ 57     ┆ 53     │\n",
      "└─────────────────────────────────┴─────────────────────────────────┴────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# Normalize the translation column\n",
    "df_norm = df.select(\n",
    "    [\n",
    "        pl.col(\"translation\").struct.field(\"en\").alias(\"en\"),\n",
    "        pl.col(\"translation\").struct.field(\"cs\").alias(\"cs\"),\n",
    "        pl.col(\"translation\").struct.field(\"cs\").str.len_chars().alias(\"cs_len\"),\n",
    "        pl.col(\"translation\").struct.field(\"en\").str.len_chars().alias(\"en_len\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Let's check the normalized structure\n",
    "print(df_norm.schema)\n",
    "print(\"\\nFirst row:\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to preprocess the data to get achieve high quality results.\n",
    "\n",
    "First, let's clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10, 4)\n",
      "Dataset shape after cleaning: (10, 6)\n",
      "shape: (1, 6)\n",
      "┌────────────────────┬───────────────────┬────────┬────────┬───────────────────┬───────────────────┐\n",
      "│ en                 ┆ cs                ┆ cs_len ┆ en_len ┆ en_clean          ┆ cs_clean          │\n",
      "│ ---                ┆ ---               ┆ ---    ┆ ---    ┆ ---               ┆ ---               │\n",
      "│ str                ┆ str               ┆ u32    ┆ u32    ┆ str               ┆ str               │\n",
      "╞════════════════════╪═══════════════════╪════════╪════════╪═══════════════════╪═══════════════════╡\n",
      "│ Action taken on    ┆ Následný postup   ┆ 57     ┆ 53     ┆ Action taken on   ┆ Následný postup   │\n",
      "│ Parliament's r…    ┆ na základě usn…   ┆        ┆        ┆ Parliament's r…   ┆ na základě usn…   │\n",
      "└────────────────────┴───────────────────┴────────┴────────┴───────────────────┴───────────────────┘\n"
     ]
    }
   ],
   "source": [
    "def clean_texts(df):\n",
    "    \"\"\"\n",
    "    Clean the texts by replacing multiple spaces with a single space and stripping leading and trailing spaces.\n",
    "    We also filter out very short texts (less than 3 characters) and texts where one language is more than 2.5x longer than the other.\n",
    "    \"\"\"\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            # Clean the English text\n",
    "            pl.col(\"en\")\n",
    "            .str.replace_all(r\"\\s+\", \" \")\n",
    "            .str.strip_chars()\n",
    "            .alias(\"en_clean\"),\n",
    "            # Clean the Czech text\n",
    "            pl.col(\"cs\")\n",
    "            .str.replace_all(r\"\\s+\", \" \")\n",
    "            .str.strip_chars()\n",
    "            .alias(\"cs_clean\"),\n",
    "        ]\n",
    "    ).filter(\n",
    "        # Filter out rows with non a-Z characters\n",
    "        ~pl.col(\"cs_clean\").str.contains(r\"^[a-zA-Z]+$\")\n",
    "        & ~pl.col(\"en_clean\").str.contains(r\"^[a-zA-Z]+$\")\n",
    "        # Filter out very short texts (less than 3 characters)\n",
    "        & (pl.col(\"cs_len\") >= 3)\n",
    "        & (pl.col(\"en_len\") >= 3)\n",
    "        # This helps remove poor quality or misaligned translations\n",
    "        & (pl.col(\"cs_len\") / pl.col(\"en_len\") <= 2.5)  # Czech text not too long compared to English\n",
    "        & (pl.col(\"en_len\") / pl.col(\"cs_len\") <= 2.5)  # English text not too long compared to Czech\n",
    "    )\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = clean_texts(df_norm)\n",
    "print(f\"Dataset shape after cleaning: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a language detection check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10, 6)\n",
      "Detecting language of the text with threshold 0.8 and batch size 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking Czech language: 100%|██████████| 10/10 [00:00<00:00, 33907.07it/s]\n",
      "Checking English language: 100%|██████████| 10/10 [00:00<00:00, 61052.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected bad languages: ['__label__it']\n",
      "Dataset shape after language detection: (9, 8)\n",
      "shape: (1, 8)\n",
      "┌─────────────┬─────────────┬────────┬────────┬─────────────┬────────────┬────────────┬────────────┐\n",
      "│ en          ┆ cs          ┆ cs_len ┆ en_len ┆ en_clean    ┆ cs_clean   ┆ is_valid_c ┆ is_valid_e │\n",
      "│ ---         ┆ ---         ┆ ---    ┆ ---    ┆ ---         ┆ ---        ┆ s          ┆ n          │\n",
      "│ str         ┆ str         ┆ u32    ┆ u32    ┆ str         ┆ str        ┆ ---        ┆ ---        │\n",
      "│             ┆             ┆        ┆        ┆             ┆            ┆ bool       ┆ bool       │\n",
      "╞═════════════╪═════════════╪════════╪════════╪═════════════╪════════════╪════════════╪════════════╡\n",
      "│ Action      ┆ Následný    ┆ 57     ┆ 53     ┆ Action      ┆ Následný   ┆ true       ┆ true       │\n",
      "│ taken on    ┆ postup na   ┆        ┆        ┆ taken on    ┆ postup na  ┆            ┆            │\n",
      "│ Parliament' ┆ základě     ┆        ┆        ┆ Parliament' ┆ základě    ┆            ┆            │\n",
      "│ s r…        ┆ usn…        ┆        ┆        ┆ s r…        ┆ usn…       ┆            ┆            │\n",
      "└─────────────┴─────────────┴────────┴────────┴─────────────┴────────────┴────────────┴────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import wget\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detect_language(df, threshold=0.8, batch_size=100):\n",
    "    \"\"\"\n",
    "    Detect the language of the text.\n",
    "    \"\"\"\n",
    "    print(f\"Detecting language of the text with threshold {threshold} and batch size {batch_size}\")\n",
    "\n",
    "    # Load the fasttext model\n",
    "    model_path = \"models/fasttext/lid.176.bin\"\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        wget.download(\"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\", out=model_path)\n",
    "\n",
    "    model = fasttext.load_model(model_path)\n",
    "\n",
    "    def predict_language(text):\n",
    "        \"\"\"\n",
    "        Check if the text is in the expected language.\n",
    "        \"\"\"\n",
    "        prediction = model.predict(text, k=1)\n",
    "        return prediction\n",
    "\n",
    "    # Process in batches\n",
    "    cs_clean = df['cs_clean'].to_list()\n",
    "    en_clean = df['en_clean'].to_list()\n",
    "\n",
    "    cs_results = []\n",
    "    en_results = []\n",
    "    bad_languages = []\n",
    "\n",
    "    for i in range(0, len(cs_clean), batch_size):\n",
    "        batch_cs = cs_clean[i:i+batch_size]\n",
    "        batch_en = en_clean[i:i+batch_size]\n",
    "\n",
    "        # Check if the text is in the expected language\n",
    "        for text in tqdm(batch_cs, desc=\"Checking Czech language\"):\n",
    "            result = predict_language(text)\n",
    "            if result[0][0] == \"__label__cs\":\n",
    "                cs_results.append(True)\n",
    "            else:\n",
    "                cs_results.append(False)\n",
    "                bad_languages.append(result[0][0])\n",
    "\n",
    "        for text in tqdm(batch_en, desc=\"Checking English language\"):\n",
    "            result = predict_language(text)\n",
    "            if result[0][0] == \"__label__en\":\n",
    "                en_results.append(True)\n",
    "            else:\n",
    "                en_results.append(False)\n",
    "                bad_languages.append(result[0][0])\n",
    "\n",
    "    print(f\"Detected bad languages: {bad_languages}\")\n",
    "\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            pl.Series(\"is_valid_cs\", cs_results),\n",
    "            pl.Series(\"is_valid_en\", en_results)\n",
    "        ]\n",
    "    ).filter(pl.col(\"is_valid_cs\") & pl.col(\"is_valid_en\"))\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = detect_language(df_norm)\n",
    "print(f\"Dataset shape after language detection: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out the rows based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (9, 8)\n",
      "Filtering out rows with similarity less than 0.8\n",
      "Processing in batches of 100 rows\n",
      "Filtered dataset shape: (9, 9)\n",
      "shape: (1, 9)\n",
      "┌────────────┬────────────┬────────┬────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ en         ┆ cs         ┆ cs_len ┆ en_len ┆ … ┆ cs_clean   ┆ is_valid_c ┆ is_valid_e ┆ similarit │\n",
      "│ ---        ┆ ---        ┆ ---    ┆ ---    ┆   ┆ ---        ┆ s          ┆ n          ┆ y         │\n",
      "│ str        ┆ str        ┆ u32    ┆ u32    ┆   ┆ str        ┆ ---        ┆ ---        ┆ ---       │\n",
      "│            ┆            ┆        ┆        ┆   ┆            ┆ bool       ┆ bool       ┆ f32       │\n",
      "╞════════════╪════════════╪════════╪════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ Action     ┆ Následný   ┆ 57     ┆ 53     ┆ … ┆ Následný   ┆ true       ┆ true       ┆ 0.800876  │\n",
      "│ taken on   ┆ postup na  ┆        ┆        ┆   ┆ postup na  ┆            ┆            ┆           │\n",
      "│ Parliament ┆ základě    ┆        ┆        ┆   ┆ základě    ┆            ┆            ┆           │\n",
      "│ 's r…      ┆ usn…       ┆        ┆        ┆   ┆ usn…       ┆            ┆            ┆           │\n",
      "└────────────┴────────────┴────────┴────────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "def filter_similar_texts(df, threshold=0.8, batch_size=100):\n",
    "    \"\"\"\n",
    "    Filter out the rows based on similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Filtering out rows with similarity less than {threshold}\")\n",
    "    print(f\"Processing in batches of {batch_size} rows\")\n",
    "    \n",
    "    # Process in batches\n",
    "    cs_clean = df['cs_clean'].to_list()\n",
    "    en_clean = df['en_clean'].to_list()\n",
    "    \n",
    "    similarities = []\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for i in range(0, len(cs_clean), batch_size):\n",
    "        batch_cs = cs_clean[i:i+batch_size]\n",
    "        batch_en = en_clean[i:i+batch_size]\n",
    "        \n",
    "        # Compute the embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings_cs = model.encode(batch_cs, convert_to_tensor=True)\n",
    "            embeddings_en = model.encode(batch_en, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute the similarity\n",
    "        batch_similarities = torch.nn.functional.cosine_similarity(embeddings_cs, embeddings_en, dim=1)\n",
    "        similarities.extend(batch_similarities.cpu().numpy())\n",
    "        \n",
    "    return df.with_columns(pl.Series(\"similarity\", similarities)).filter(pl.col(\"similarity\") > threshold)\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = filter_similar_texts(df_norm)\n",
    "print(f\"Filtered dataset shape: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform an alignment check. We will use COMET to check the quality of the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8da0fc0d784aa596b834504d627346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (9, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset shape: (8, 10)\n",
      "shape: (8, 10)\n",
      "┌────────────┬────────────┬────────┬────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ en         ┆ cs         ┆ cs_len ┆ en_len ┆ … ┆ is_valid_c ┆ is_valid_e ┆ similarity ┆ quality_s │\n",
      "│ ---        ┆ ---        ┆ ---    ┆ ---    ┆   ┆ s          ┆ n          ┆ ---        ┆ core      │\n",
      "│ str        ┆ str        ┆ u32    ┆ u32    ┆   ┆ ---        ┆ ---        ┆ f32        ┆ ---       │\n",
      "│            ┆            ┆        ┆        ┆   ┆ bool       ┆ bool       ┆            ┆ f64       │\n",
      "╞════════════╪════════════╪════════╪════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ Action     ┆ Následný   ┆ 57     ┆ 53     ┆ … ┆ true       ┆ true       ┆ 0.800876   ┆ 0.463111  │\n",
      "│ taken on   ┆ postup na  ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ Parliament ┆ základě    ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 's r…      ┆ usn…       ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ Documents  ┆ Předložení ┆ 31     ┆ 31     ┆ … ┆ true       ┆ true       ┆ 0.819701   ┆ 0.456948  │\n",
      "│ received:  ┆ dokumentů: ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ see        ┆ viz zápi…  ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ Minute…    ┆            ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ Written    ┆ Písemná    ┆ 57     ┆ 42     ┆ … ┆ true       ┆ true       ┆ 0.909596   ┆ 0.42378   │\n",
      "│ statements ┆ prohlášení ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ (Rule      ┆ (článek    ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 116):…     ┆ 116…       ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ Texts of   ┆ Texty      ┆ 35     ┆ 57     ┆ … ┆ true       ┆ true       ┆ 0.895811   ┆ 0.400277  │\n",
      "│ agreements ┆ smluv      ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ forwarded  ┆ dodané     ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ …          ┆ Radou: viz ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│            ┆ …          ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ Membership ┆ Složení    ┆ 29     ┆ 37     ┆ … ┆ true       ┆ true       ┆ 0.911823   ┆ 0.432201  │\n",
      "│ of Parliam ┆ Parlamentu ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ ent: see … ┆ : viz      ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│            ┆ zápis      ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ Membership ┆ Členství   ┆ 44     ┆ 53     ┆ … ┆ true       ┆ true       ┆ 0.935291   ┆ 0.454606  │\n",
      "│ of         ┆ ve         ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ committees ┆ výborech a ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ and d…     ┆ delegac…   ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ Agenda for ┆ Pořad      ┆ 42     ┆ 36     ┆ … ┆ true       ┆ true       ┆ 0.929281   ┆ 0.472326  │\n",
      "│ next       ┆ jednání    ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ sitting:   ┆ příštího   ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ see M…     ┆ zasedán…   ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "│ Closure of ┆ Ukončení   ┆ 17     ┆ 18     ┆ … ┆ true       ┆ true       ┆ 0.800042   ┆ 0.451879  │\n",
      "│ sitting    ┆ zasedání   ┆        ┆        ┆   ┆            ┆            ┆            ┆           │\n",
      "└────────────┴────────────┴────────┴────────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from comet import load_from_checkpoint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the COMET model\n",
    "model_path = snapshot_download(\"Unbabel/wmt22-comet-da\")\n",
    "model_checkpoint_path = f\"{model_path}/checkpoints/model.ckpt\"\n",
    "model = load_from_checkpoint(model_checkpoint_path)\n",
    "\n",
    "def filter_by_quality(df, treshold = 0.4, batch_size = 100):\n",
    "    \"\"\"\n",
    "    Filter out the rows based on the quality of the alignment.\n",
    "    \"\"\"\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Get texts\n",
    "    cs_clean = df[\"cs_clean\"].to_list()\n",
    "    en_clean = df[\"en_clean\"].to_list()\n",
    "\n",
    "    # Prepare data for COMET\n",
    "    data = [\n",
    "        {\"src\": cs, \"mt\": en, \"ref\": None}\n",
    "        for cs, en in zip(cs_clean, en_clean)\n",
    "    ]\n",
    "    \n",
    "    # Get scores from COMET\n",
    "    predictions = model.predict(data, batch_size=batch_size)\n",
    "    scores = predictions.scores\n",
    "\n",
    "    return df.with_columns(\n",
    "        [pl.Series(\"quality_score\", scores)]\n",
    "    ).filter(pl.col(\"quality_score\") > treshold)\n",
    "\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = filter_by_quality(df_norm)\n",
    "print(f\"Filtered dataset shape: {df_norm.shape}\")\n",
    "print(df_norm.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the distribution of the quality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 4)\n",
      "┌─────────────────┬──────────┬─────────────────┬──────────┐\n",
      "│ 25th_percentile ┆ median   ┆ 75th_percentile ┆ mean     │\n",
      "│ ---             ┆ ---      ┆ ---             ┆ ---      │\n",
      "│ f64             ┆ f64      ┆ f64             ┆ f64      │\n",
      "╞═════════════════╪══════════╪═════════════════╪══════════╡\n",
      "│ 0.432201        ┆ 0.454606 ┆ 0.456948        ┆ 0.444391 │\n",
      "└─────────────────┴──────────┴─────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Look at distribution\n",
    "print(\n",
    "    df_norm.select(\n",
    "        [\n",
    "            pl.col(\"quality_score\").quantile(0.25).alias(\"25th_percentile\"),\n",
    "            pl.col(\"quality_score\").quantile(0.5).alias(\"median\"),\n",
    "            pl.col(\"quality_score\").quantile(0.75).alias(\"75th_percentile\"),\n",
    "            pl.col(\"quality_score\").mean().alias(\"mean\"),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the quality of the alignment is pretty good for most of the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's normalize the dataset to the instruction format and save the dataset to a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 11)\n",
      "┌────────────┬────────────┬────────┬────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ en         ┆ cs         ┆ cs_len ┆ en_len ┆ … ┆ is_valid_e ┆ similarity ┆ quality_sc ┆ instructi │\n",
      "│ ---        ┆ ---        ┆ ---    ┆ ---    ┆   ┆ n          ┆ ---        ┆ ore        ┆ on        │\n",
      "│ str        ┆ str        ┆ u32    ┆ u32    ┆   ┆ ---        ┆ f32        ┆ ---        ┆ ---       │\n",
      "│            ┆            ┆        ┆        ┆   ┆ bool       ┆            ┆ f64        ┆ str       │\n",
      "╞════════════╪════════════╪════════╪════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ Action     ┆ Následný   ┆ 57     ┆ 53     ┆ … ┆ true       ┆ 0.800876   ┆ 0.463111   ┆ <start_of │\n",
      "│ taken on   ┆ postup na  ┆        ┆        ┆   ┆            ┆            ┆            ┆ _turn>use │\n",
      "│ Parliament ┆ základě    ┆        ┆        ┆   ┆            ┆            ┆            ┆ r         │\n",
      "│ 's r…      ┆ usn…       ┆        ┆        ┆   ┆            ┆            ┆            ┆ Přelož    │\n",
      "│            ┆            ┆        ┆        ┆   ┆            ┆            ┆            ┆ ten…      │\n",
      "└────────────┴────────────┴────────┴────────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "def instruction_format(x):\n",
    "    \"\"\"Format input text into an instruction format for translation.\n",
    "\n",
    "    Args:\n",
    "        x: Dictionary containing 'en_clean' (English text) and 'cs_clean' (Czech translation)\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted instruction string with the following structure:\n",
    "            <start_of_turn>user\n",
    "            Přelož tento text z angličtiny do češtiny.\n",
    "            \"{English text}\"\n",
    "            <end_of_turn>\n",
    "            <start_of_turn>model\n",
    "            {Czech translation}<end_of_turn>\n",
    "    \"\"\"\n",
    "    return f\"<start_of_turn>user\\nPřelož tento text z angličtiny do češtiny.\\n\\\"{x['en_clean']}\\\"<end_of_turn>\\n<start_of_turn>model\\n{x['cs_clean']}<end_of_turn>\"\n",
    "\n",
    "finetune_df = df_norm.with_columns(\n",
    "    [\n",
    "        pl.struct([\"en_clean\", \"cs_clean\"])\n",
    "        .map_elements(instruction_format, return_dtype=pl.Utf8)\n",
    "        .alias(\"instruction\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(finetune_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we need to tokenize the dataset. First we will load the tokenizer and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 4521, 235269, 2134, 235341], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(training_config.model_id)\n",
    "\n",
    "# Test the tokenizer\n",
    "print(tokenizer(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (8, 11)\n",
      "Token limit: 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {finetune_df.shape}\")\n",
    "print(f\"Token limit: {training_config.token_limit}\")\n",
    "\n",
    "finetune_df = finetune_df.with_columns(\n",
    "    [\n",
    "        pl.col(\"instruction\")\n",
    "        .map_elements(lambda x: tokenizer(x).input_ids, return_dtype=pl.List(pl.Int32))\n",
    "        .alias(\"tokens\")\n",
    "    ]\n",
    ").filter(\n",
    "    pl.col(\"tokens\").list.len() <= training_config.token_limit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the token distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token length statistics:\n",
      "Mean: 47.75\n",
      "Median: 48.5\n",
      "95th percentile: 63.0\n",
      "Max: 63\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "token_lengths = finetune_df[\"tokens\"].map_elements(len, return_dtype=pl.Int32)\n",
    "print(\"Token length statistics:\")\n",
    "print(f\"Mean: {token_lengths.mean()}\")\n",
    "print(f\"Median: {token_lengths.median()}\")\n",
    "print(f\"95th percentile: {token_lengths.quantile(0.95)}\")\n",
    "print(f\"Max: {token_lengths.max()}\")\n",
    "\n",
    "# Plot the token length distribution\n",
    "plt.hist(token_lengths, bins=50)\n",
    "plt.title(\"Token Length Distribution\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, filter to the token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape post-tokenization: (8, 1)\n",
      "shape: (1, 1)\n",
      "┌─────────────────────┐\n",
      "│ instruction         │\n",
      "│ ---                 │\n",
      "│ str                 │\n",
      "╞═════════════════════╡\n",
      "│ <start_of_turn>user │\n",
      "│ Přelož ten…         │\n",
      "└─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "finetune_df = finetune_df.select(\n",
    "    [\n",
    "        pl.col(\"instruction\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape post-tokenization: {finetune_df.shape}\")\n",
    "print(finetune_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally, let's calculate the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout rate: 20.00%\n"
     ]
    }
   ],
   "source": [
    "# Dropout rate is the percentage of rows that are filtered out.\n",
    "dropout_rate = (len(df) - len(finetune_df)) / len(df)\n",
    "print(f\"Dropout rate: {dropout_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout rate is pretty high, but it's a good thing that we have a lot of data to work with and we aim for high quality results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is ready for the next step. We will use it to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6\n",
      "Validation size: 1\n",
      "Test size: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split into train and temp (80/20)\n",
    "train_df, temp_df = train_test_split(finetune_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then split temp into validation and test (50/50, resulting in 10/10 of original)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save all splits to parquet files\n",
    "train_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_train.parquet\", compression=\"zstd\"\n",
    ")\n",
    "valid_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_valid.parquet\", compression=\"zstd\"\n",
    ")\n",
    "test_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_test.parquet\", compression=\"zstd\"\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(valid_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model - PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pytorch_lightning\n",
    "import logging\n",
    "class GemmaLightningModule(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.std_logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Load base model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_id)\n",
    "\n",
    "        # Test prompts for monitoring\n",
    "        self.test_prompts = [\n",
    "            \"Přelož tento text z angličtiny do češtiny:\\n'I love machine learning and AI.'\",\n",
    "            \"Přelož tento text z angličtiny do češtiny:\\n'The weather is beautiful today.'\",\n",
    "            \"Přelož tento text z angličtiny do češtiny:\\n'Can you help me with directions to the train station?'\",\n",
    "        ]\n",
    "\n",
    "        # Setup LoRA\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=config.lora_rank,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            lora_dropout=config.lora_dropout,\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Generate sample translations after each validation epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "\n",
    "        for prompt in self.test_prompts:\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=self.config.token_limit\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            results.append([prompt, generated_text])\n",
    "        \n",
    "        # Log the results\n",
    "        for prompt, generated_text in results:\n",
    "            self.std_logger.info(f\"Generated text for prompt: {prompt}\\n{generated_text}\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        decay_params = []\n",
    "        no_decay_params = []\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if any(exclude_term in name for exclude_term in [\"bias\", \"scale\"]):\n",
    "                no_decay_params.append(param)\n",
    "            else:\n",
    "                decay_params.append(param)\n",
    "\n",
    "        optimizer_params = [\n",
    "            {\n",
    "                'params': decay_params,\n",
    "                'weight_decay': 0.01\n",
    "            },\n",
    "            {\n",
    "                'params': no_decay_params,\n",
    "                'weight_decay': 0.0   # No weight decay for excluded parameters\n",
    "            }\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optimizer_params,\n",
    "            lr=self.config.learning_rate,\n",
    "            eps=1e-6, # Default is 1e-8, but we need to use mixed precision training\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b23ec745d664e3fba47f83369ba3655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GemmaDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        # Ensure we're not returning tensors with an extra dimension\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        # Convert to tensors manually\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(\n",
    "                encodings[\"input_ids\"], dtype=torch.long\n",
    "            ),\n",
    "            \"attention_mask\": torch.tensor(\n",
    "                encodings[\"attention_mask\"], dtype=torch.long\n",
    "            ),\n",
    "            \"labels\": torch.tensor(\n",
    "                encodings[\"input_ids\"], dtype=torch.long\n",
    "            ),  # Added labels for training\n",
    "        }\n",
    "\n",
    "\n",
    "# Import the model\n",
    "model = AutoModelForCausalLM.from_pretrained(training_config.model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(training_config.model_id)\n",
    "\n",
    "# Create datasets\n",
    "train_ds = GemmaDataset(\n",
    "    train_df[\"instruction\"].to_list(), tokenizer, training_config.token_limit\n",
    ")\n",
    "valid_ds = GemmaDataset(\n",
    "    valid_df[\"instruction\"].to_list(), tokenizer, training_config.token_limit\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=training_config.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=training_config.batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5599f82b46b404ea69f7e197e14fc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/jirka/projects/competitions/gemma-global-competition/checkpoints/gemma-cs-4r exists and is not empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                 </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model │ PeftModelForCausalLM │  2.5 B │ train │\n",
       "└───┴───────┴──────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ PeftModelForCausalLM │  2.5 B │ train │\n",
       "└───┴───────┴──────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 460 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 2.5 B                                                                                        \n",
       "<span style=\"font-weight: bold\">Total params</span>: 2.5 B                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 10.0 K                                                                     \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 326                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 258                                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 460 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 2.5 B                                                                                        \n",
       "\u001b[1mTotal params\u001b[0m: 2.5 B                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 10.0 K                                                                     \n",
       "\u001b[1mModules in train mode\u001b[0m: 326                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 258                                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe44872c2a31470989e5fe38d9f43b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-package\n",
       "s/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. \n",
       "Disabling\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-package\n",
       "s/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. \n",
       "Disabling\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'I love machine learning and AI.'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'I love machine learning and AI.'\n",
      "\n",
      "Tento text je v angličtině a pokládá se pod pokutu \"I love\". V češtině se může překreslit jako \"Jám nadšán počítačová věda a AI\".\n",
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'The weather is beautiful today.'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'The weather is beautiful today.'\n",
      "\n",
      "Pamatujte, že toto je pouze příklad a že k výpočetu počasí se používá různé metódy.\n",
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'Can you help me with directions to the train station?'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'Can you help me with directions to the train station?'\n",
      "\n",
      "**Český překlad:**\n",
      "\n",
      "Je mi znatné vyhledat cestu k stanici trnu?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-package\n",
       "s/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval \n",
       "Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training \n",
       "epoch.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-package\n",
       "s/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval \n",
       "Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training \n",
       "epoch.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'I love machine learning and AI.'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'I love machine learning and AI.'\n",
      "\n",
      "Tímto jsem uvolnil vědeckou škálu a rozvíjející se oblast, která se zabývá mocnou kombinací algoritmu a počítačové vědy.\n",
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'The weather is beautiful today.'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'The weather is beautiful today.'\n",
      "\n",
      "Tato řešima je však nekontextní, protože se jedná o krátký časový termín. Známos, že \"beautiful\" je adjektiv, který se používá k označení vlastností, které\n",
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'Can you help me with directions to the train station?'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'Can you help me with directions to the train station?'\n",
      "\n",
      "Zde je text v angličtině: \"Can you help me with directions to the train station?\"\n",
      "\n",
      "**Přeložené možnosti:**\n",
      "\n",
      "* Jistě, jsem vám může pomoci s mapami a odkazy na traťový stan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'I love machine learning and AI.'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'I love machine learning and AI.'\n",
      "\n",
      "Tady je překlad:\n",
      "\"Jám můj oblíh v oblasti vývoje a AI.\"\n",
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'The weather is beautiful today.'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'The weather is beautiful today.'\n",
      "\n",
      "V češtině se to může vyjádřit jako:\n",
      "\n",
      "* 'Sobota je krásný weer.'\n",
      "* 'Má sluneční počasí dnes.'\n",
      "* 'Dnes je krásný počasí.'\n",
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'Can you help me with directions to the train station?'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'Can you help me with directions to the train station?'\n",
      "\n",
      "Přečišťuji text tak, aby odpovídal specifickému zadání.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'I love machine learning and AI.'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'I love machine learning and AI.'\n",
      "\n",
      "Tímto se odkazuje na oblast technologií, která se zabývá vývojem a výpočety počítačových systémů.\n",
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'The weather is beautiful today.'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'The weather is beautiful today.'\n",
      "\n",
      "Tímto jsem se pokusil přeložit toto slovo do češtiny, ale jsem se zhruba vyhrát.\n",
      "\n",
      "**České překlady:**\n",
      "\n",
      "* The weather is beautiful today.\n",
      "* The weather is\n",
      "Generated text for prompt: Přelož tento text z angličtiny do češtiny:\n",
      "'Can you help me with directions to the train station?'\n",
      "Přelož tento text z angličtiny do češtiny:\n",
      "'Can you help me with directions to the train station?'\n",
      "\n",
      "Tímto dokazujeme, že vám můžeme pomoci s informacemi o nádražce.\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:151\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:370\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    368\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:151\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:305\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_evaluation_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# enable train mode again\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:350\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_end\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 350\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, hook_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:222\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 222\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:335\u001b[0m, in \u001b[0;36mModelCheckpoint.on_validation_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_topk_checkpoint(trainer, monitor_candidates)\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_last_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_candidates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:696\u001b[0m, in \u001b[0;36mModelCheckpoint._save_last_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 696\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_remove_checkpoint(trainer, previous, filepath):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:390\u001b[0m, in \u001b[0;36mModelCheckpoint._save_checkpoint\u001b[0;34m(self, trainer, filepath)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, filepath: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_global_step_saved \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mglobal_step\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1367\u001b[0m, in \u001b[0;36mTrainer.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m   1366\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mdump_checkpoint(weights_only)\n\u001b[0;32m-> 1367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer.save_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:491\u001b[0m, in \u001b[0;36mStrategy.save_checkpoint\u001b[0;34m(self, checkpoint, filepath, storage_options)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_global_zero:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/lightning_fabric/plugins/io/torch_io.py:58\u001b[0m, in \u001b[0;36mTorchCheckpointIO.save_checkpoint\u001b[0;34m(self, checkpoint, path, storage_options)\u001b[0m\n\u001b[1;32m     57\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43m_atomic_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:90\u001b[0m, in \u001b[0;36m_atomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mtransaction, fs\u001b[38;5;241m.\u001b[39mopen(urlpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytesbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/fsspec/implementations/local.py:426\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m GemmaLightningModule(training_config)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor, RichProgressBar\n",
    "import wandb\n",
    "\n",
    "# Setup Wandb logger\n",
    "wandb_logger = pl.loggers.WandbLogger(\n",
    "    project=\"gemma-cs\",\n",
    "    name=f\"gemma-cs-{training_config.lora_rank}r\",\n",
    "    config={\n",
    "        \"model_name\": training_config.model_id,\n",
    "        \"lora_rank\": training_config.lora_rank,\n",
    "        \"batch_size\": training_config.batch_size,\n",
    "        \"learning_rate\": training_config.learning_rate,\n",
    "        \"epochs\": training_config.epochs,\n",
    "        \"token_limit\": training_config.token_limit,\n",
    "        \"accumulate_grad_batches\": training_config.accumulate_grad_batches,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    # Save best models\n",
    "    ModelCheckpoint(\n",
    "        dirpath=f\"checkpoints/gemma-cs-{training_config.lora_rank}r\",\n",
    "        filename=\"model-{epoch:02d}-{val/loss:.2f}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=3,\n",
    "        save_last=True,\n",
    "    ),\n",
    "    # Early stopping\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\", min_delta=0.001),\n",
    "    # Monitor learning rate\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "    # Rich progress bar\n",
    "    RichProgressBar(),\n",
    "]\n",
    "\n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=training_config.epochs,\n",
    "    precision=training_config.precision,\n",
    "    accumulate_grad_batches=training_config.accumulate_grad_batches,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=callbacks,\n",
    "    # Log gradients & model parameters\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.25,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = GemmaLightningModule(training_config)\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "wandb.finish()\n",
    "\n",
    "# Save the model\n",
    "model.model.save_pretrained(\n",
    "    f\"models/gemma-cs-{training_config.lora_rank}r-final\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(training_config.model_id)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the inference of our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Přelož tento text z angličtiny do češtiny:\\n\"Hello, world!\"'\n",
    "print(\n",
    "    gemma_lm.generate(\n",
    "        prompt,\n",
    "        max_length=training_config.token_limit,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LoRA on the model\n",
    "gemma_lm.backbone.enable_lora(rank=training_config.lora_rank)\n",
    "gemma_lm.summary()\n",
    "\n",
    "# Control memory usage\n",
    "gemma_lm.preprocessor.sequence_length = training_config.token_limit\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=training_config.learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    epsilon=1e-6, # Default is 1e-8, but we need to use mixed precision training\n",
    ")\n",
    "\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "# Setup loss\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Setup metrics\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "# Compile the model\n",
    "gemma_lm.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the dataset to Keras expected format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_df[\"instruction\"].to_list()\n",
    "valid_dataset = valid_df[\"instruction\"].to_list()\n",
    "\n",
    "print(f\"Train dataset shape: {len(train_dataset)}\")\n",
    "print(train_dataset[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Valid dataset shape: {len(valid_dataset)}\")\n",
    "print(valid_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now ready for training. Let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "import wandb\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch} finished\")\n",
    "        model_name = f\"gemma_cs_{training_config.lora_rank}_epoch{epoch+1}.lora.h5\"\n",
    "        gemma_lm.backbone.save_lora_weights(model_name)\n",
    "        print(f\"Model {model_name} saved\")\n",
    "\n",
    "        # Eval on prompt\n",
    "        prompt = \"Přelož tento text z angličtiny do češtiny.\\n\\\"Hello, world!\\\"\"\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(\n",
    "            f\"Response: {gemma_lm.generate(prompt, max_length=training_config.token_limit)}\"\n",
    "        )\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(config={\n",
    "    \"learning_rate\": training_config.learning_rate,\n",
    "    \"epochs\": training_config.epochs,\n",
    "    \"batch_size\": training_config.batch_size,\n",
    "    \"lora_rank\": training_config.lora_rank,\n",
    "    \"token_limit\": training_config.token_limit,\n",
    "    \"model\": \"gemma2_instruct_2b_en\",\n",
    "    \"task\": \"czech-context\"\n",
    "})\n",
    "\n",
    "# Convert lists to TensorFlow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices(valid_dataset)\n",
    "\n",
    "# Configure datasets for performance\n",
    "train_ds = train_ds.batch(training_config.batch_size)\n",
    "valid_ds = valid_ds.batch(training_config.batch_size)\n",
    "\n",
    "# Enable prefetching to improve performance\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "history = gemma_lm.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=training_config.epochs,\n",
    "    callbacks=[CustomCallback(), WandbMetricsLogger()],\n",
    ")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.plot(history.history[\"sparse_categorical_accuracy\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-czech-adaptation-iLYBdvQN-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
