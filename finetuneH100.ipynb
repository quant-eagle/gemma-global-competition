{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸŒ Czech Language Adaptation of Gemma Language Model\n",
                "\n",
                "**Author:** Jirka Helmich  \n",
                "**Last Updated:** 2024-01-06  \n",
                "**License:** MIT\n",
                "\n",
                "## ğŸ“‹ Overview\n",
                "\n",
                "This notebook demonstrates the fine-tuning process of the Gemma language model for Czech language understanding and generation. We focus on creating a robust multilingual model capable of handling various Czech-specific NLP tasks.\n",
                "\n",
                "### ğŸ¯ Key Objectives\n",
                "\n",
                "1. Adapt Gemma for superior Czech language processing\n",
                "2. Support translation and text generation tasks\n",
                "3. Comprehensive benchmarking on Czech-specific metrics\n",
                "\n",
                "### ğŸ“Š Data Sources\n",
                "\n",
                "1. **ParaCrawl v9**\n",
                "   - EN-CS parallel corpus (~52M pairs)\n",
                "   - [Source](https://paracrawl.eu/v9)\n",
                "\n",
                "2. **Czech Books Descriptions**\n",
                "   - Book descriptions in Czech\n",
                "   - [Source](https://huggingface.co/datasets/vojtam/czech_books_descriptions)\n",
                "\n",
                "### ğŸ› ï¸ Technical Requirements\n",
                "\n",
                "```python\n",
                "Python >= 3.10\n",
                "polars >= 0.20.0\n",
                "datasets >= 2.15.0\n",
                "tqdm >= 4.66.0\n",
                "fasttext >= 0.9.2\n",
                "torch >= 2.0.0\n",
                "transformers >= 4.36.0\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1ï¸âƒ£ Environment Setup\n",
                "\n",
                "First, let's set up our environment with all required dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
                        "\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "# Install core dependencies\n",
                "%pip install -q datasets polars tqdm fasttext torch transformers>=4.47.1 wandb seaborn matplotlib numpy peft>=0.14.0 evaluate huggingface_hub bitsandbytes>=0.45.0\n",
                "\n",
                "# Import common libraries\n",
                "import polars as pl\n",
                "from pathlib import Path\n",
                "import logging\n",
                "from tqdm.auto import tqdm\n",
                "from typing import Optional, Dict, List, Union\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Configure plotting\n",
                "plt.style.use('seaborn-v0_8-paper')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2ï¸âƒ£ Data Processing Pipeline\n",
                "\n",
                "Our data processing pipeline is optimized for handling large-scale text data efficiently:\n",
                "\n",
                "1. ğŸ“¥ **Data Loading**: Streaming large files with chunked processing\n",
                "2. ğŸ§¹ **Text Cleaning**: Efficient Czech text validation and normalization\n",
                "3. ğŸ”„ **Format Conversion**: Optimized Alpaca format transformation\n",
                "4. ğŸ’¾ **Storage**: Compressed Parquet format with optimal chunk sizes\n",
                "\n",
                "### 2.1 Core Data Processing Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gzip\n",
                "\n",
                "class ParaCrawlDataLoader:\n",
                "    \"\"\"Optimized loader for ParaCrawl dataset with chunked processing.\"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        source_lang: str = \"en\",\n",
                "        target_lang: str = \"cs\",\n",
                "        chunk_size: int = 500_000,  # Increased for better throughput\n",
                "        data_dir: Optional[str] = None,\n",
                "        cache_dir: Optional[str] = None\n",
                "    ):\n",
                "        self.source_lang = source_lang\n",
                "        self.target_lang = target_lang\n",
                "        self.chunk_size = chunk_size\n",
                "        self.base_url = \"https://web-language-models.s3.amazonaws.com/paracrawl/release9\"\n",
                "        \n",
                "        # Setup directories\n",
                "        self.data_dir = Path(data_dir or \"./data\")\n",
                "        self.cache_dir = Path(cache_dir or \"./cache\")\n",
                "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
                "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        # File paths\n",
                "        self.filename = f\"{source_lang}-{target_lang}.txt.gz\"\n",
                "        self.filepath = self.data_dir / self.filename\n",
                "        self.processed_path = self.cache_dir / f\"{source_lang}-{target_lang}.parquet\"\n",
                "        \n",
                "        self.logger = logging.getLogger(__name__)\n",
                "    \n",
                "    def _validate_file(self, filepath: Path) -> bool:\n",
                "        \"\"\"Validate downloaded file integrity.\"\"\"\n",
                "        if not filepath.exists():\n",
                "            return False\n",
                "            \n",
                "        try:\n",
                "            with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
                "                # Try to read first few lines\n",
                "                for _ in range(5):\n",
                "                    line = f.readline()\n",
                "                    if not line or '\\t' not in line:\n",
                "                        return False\n",
                "            return True\n",
                "        except Exception:\n",
                "            return False\n",
                "    \n",
                "    def download_data(self) -> None:\n",
                "        \"\"\"Download dataset with progress tracking.\"\"\"\n",
                "        if self.filepath.exists() and self._validate_file(self.filepath):\n",
                "            self.logger.info(\"Using existing valid download\")\n",
                "            return\n",
                "            \n",
                "        url = f\"{self.base_url}/{self.source_lang}-{self.target_lang}/{self.filename}\"\n",
                "        self.logger.info(f\"Downloading from {url}\")\n",
                "        \n",
                "        try:\n",
                "            import urllib.request\n",
                "            response = urllib.request.urlopen(url)\n",
                "            total_size = int(response.headers['Content-Length'])\n",
                "            \n",
                "            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
                "                urllib.request.urlretrieve(\n",
                "                    url,\n",
                "                    self.filepath,\n",
                "                    reporthook=lambda count, block_size, _: pbar.update(block_size)\n",
                "                )\n",
                "                \n",
                "            if not self._validate_file(self.filepath):\n",
                "                raise ValueError(\"Downloaded file appears to be corrupt\")\n",
                "                \n",
                "        except Exception as e:\n",
                "            self.logger.error(f\"Download failed: {e}\")\n",
                "            if self.filepath.exists():\n",
                "                self.filepath.unlink()\n",
                "            raise\n",
                "    \n",
                "    def process_chunk(self, chunk: List[str]) -> pl.DataFrame:\n",
                "        \"\"\"Process a chunk of text data efficiently.\"\"\"\n",
                "        if not chunk:\n",
                "            return pl.DataFrame()\n",
                "            \n",
                "        # Split and filter in one pass\n",
                "        pairs = [\n",
                "            line.strip().split(\"\\t\") \n",
                "            for line in chunk \n",
                "            if \"\\t\" in line\n",
                "        ]\n",
                "        \n",
                "        # Filter invalid pairs\n",
                "        valid_pairs = [\n",
                "            p for p in pairs \n",
                "            if len(p) == 2 and all(0 < len(text) < 1000 for text in p)\n",
                "        ]\n",
                "        \n",
                "        if not valid_pairs:\n",
                "            return pl.DataFrame()\n",
                "        \n",
                "        # Create DataFrame efficiently\n",
                "        return pl.DataFrame(\n",
                "            valid_pairs,\n",
                "            schema=[self.source_lang, self.target_lang],\n",
                "            orient=\"row\"\n",
                "        )\n",
                "    \n",
                "    def load_dataframe(self) -> pl.DataFrame:\n",
                "        \"\"\"Load and process data in memory-efficient chunks.\"\"\"\n",
                "        if self.processed_path.exists():\n",
                "            self.logger.info(f\"Loading cached processed data from {self.processed_path}\")\n",
                "            return pl.read_parquet(self.processed_path)\n",
                "        \n",
                "        self.download_data()\n",
                "        chunks = []\n",
                "        total_rows = 0\n",
                "        \n",
                "        self.logger.info(\"Processing raw data file...\")\n",
                "        with gzip.open(self.filepath, \"rt\", encoding=\"utf-8\") as f:\n",
                "            with tqdm(desc=\"Processing chunks\") as pbar:\n",
                "                while True:\n",
                "                    chunk = []\n",
                "                    for _ in range(self.chunk_size):\n",
                "                        line = f.readline()\n",
                "                        if not line:\n",
                "                            break\n",
                "                        chunk.append(line)\n",
                "                    \n",
                "                    if not chunk:\n",
                "                        break\n",
                "                        \n",
                "                    df_chunk = self.process_chunk(chunk)\n",
                "                    if not df_chunk.is_empty():\n",
                "                        chunks.append(df_chunk)\n",
                "                        total_rows += len(df_chunk)\n",
                "                    \n",
                "                    pbar.update(len(chunk))\n",
                "                    pbar.set_postfix({\"valid_rows\": total_rows})\n",
                "        \n",
                "        # Combine chunks and save\n",
                "        self.logger.info(f\"Combining {len(chunks)} chunks with {total_rows:,} total rows\")\n",
                "        df = pl.concat(chunks)\n",
                "        \n",
                "        self.logger.info(f\"Saving processed data to {self.processed_path}\")\n",
                "        df.write_parquet(\n",
                "            self.processed_path,\n",
                "            compression=\"zstd\",\n",
                "            compression_level=3\n",
                "        )\n",
                "        \n",
                "        return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from concurrent.futures import ThreadPoolExecutor\n",
                "import unicodedata\n",
                "from typing import Optional, List\n",
                "import polars as pl\n",
                "from pathlib import Path\n",
                "import logging\n",
                "from tqdm.notebook import tqdm\n",
                "import fasttext\n",
                "from dataclasses import dataclass\n",
                "from enum import Enum\n",
                "\n",
                "\n",
                "class TextIssue(Enum):\n",
                "    INVALID_CHARS = \"invalid_characters\"\n",
                "    NON_CZECH = \"non_czech_language\"\n",
                "    LOW_CONFIDENCE = \"low_language_confidence\"\n",
                "    TOO_SHORT = \"too_short\"\n",
                "    NO_ISSUES = \"no_issues\"\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class TextQuality:\n",
                "    original: str\n",
                "    cleaned: Optional[str]\n",
                "    issues: List[TextIssue]\n",
                "    confidence: float\n",
                "\n",
                "    @property\n",
                "    def is_valid(self) -> bool:\n",
                "        return TextIssue.NO_ISSUES in self.issues\n",
                "\n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "import unicodedata\n",
                "from typing import Optional, List\n",
                "import polars as pl\n",
                "from pathlib import Path\n",
                "import logging\n",
                "from tqdm.notebook import tqdm\n",
                "import fasttext\n",
                "\n",
                "\n",
                "class CzechTextCleaner:\n",
                "    \"\"\"Efficient Czech text validation and cleaning for large datasets.\"\"\"\n",
                "\n",
                "    FASTTEXT_MODEL_URL = (\n",
                "        \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\"\n",
                "    )\n",
                "\n",
                "    def __init__(self, model_dir: Optional[str] = None):\n",
                "        # Character sets for validation\n",
                "        self.czech_chars = frozenset(\n",
                "            \"aÃ¡bcÄdÄeÃ©Ä›fghiÃ­jklmnÅˆoÃ³pqrÅ™sÅ¡tÅ¥uÃºÅ¯vwxyÃ½zÅ¾AÃBCÄŒDÄEÃ‰ÄšFGHIÃJKLMNÅ‡OÃ“PQRÅ˜SÅ TÅ¤UÃšÅ®VWXYÃZÅ½\"\n",
                "        )\n",
                "        self.czech_punctuation = frozenset(',.!?-â€“â€”()[]{}/\\\\\"\\'Â»Â«â€\"â€Ÿ\"\\'')\n",
                "        self.czech_numbers = frozenset(\"0123456789\")\n",
                "        self.valid_chars = (\n",
                "            self.czech_chars\n",
                "            | self.czech_punctuation\n",
                "            | self.czech_numbers\n",
                "            | {\" \"}\n",
                "        )\n",
                "\n",
                "        # Setup model and logging\n",
                "        self.model_dir = Path(model_dir or \"models\")\n",
                "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
                "        self.model_path = self.model_dir / \"lid.176.ftz\"\n",
                "        self.logger = logging.getLogger(__name__)\n",
                "\n",
                "        # Load FastText model\n",
                "        self._setup_fasttext()\n",
                "\n",
                "    def _setup_fasttext(self) -> None:\n",
                "        \"\"\"Initialize FastText model.\"\"\"\n",
                "        if not self.model_path.exists():\n",
                "            self._download_model()\n",
                "        self.model = fasttext.load_model(str(self.model_path))\n",
                "\n",
                "    def _download_model(self) -> None:\n",
                "        \"\"\"Download language model with progress tracking.\"\"\"\n",
                "        import urllib.request\n",
                "\n",
                "        response = urllib.request.urlopen(self.FASTTEXT_MODEL_URL)\n",
                "        total_size = int(response.headers[\"Content-Length\"])\n",
                "\n",
                "        with tqdm(\n",
                "            total=total_size,\n",
                "            unit=\"B\",\n",
                "            unit_scale=True,\n",
                "            desc=\"Downloading model\",\n",
                "        ) as pbar:\n",
                "            urllib.request.urlretrieve(\n",
                "                self.FASTTEXT_MODEL_URL,\n",
                "                self.model_path,\n",
                "                lambda count, block_size, _: pbar.update(block_size),\n",
                "            )\n",
                "\n",
                "    def _is_valid_czech(self, text: str) -> bool:\n",
                "        \"\"\"Check if text is valid Czech with good confidence.\"\"\"\n",
                "        if not isinstance(text, str) or len(text.strip()) < 2:\n",
                "            return False\n",
                "\n",
                "        # Check characters\n",
                "        if not all(c in self.valid_chars or c.isspace() for c in text):\n",
                "            return False\n",
                "\n",
                "        # Detect language\n",
                "        text = \" \".join(text.split())\n",
                "        pred = self.model.predict(text)\n",
                "        lang, conf = pred[0][0].replace(\"__label__\", \"\"), pred[1][0]\n",
                "\n",
                "        return lang == \"cs\" and conf >= 0.8\n",
                "\n",
                "    def _clean_text(self, text: str) -> Optional[str]:\n",
                "        \"\"\"Clean text if valid, return None if invalid.\"\"\"\n",
                "        if not self._is_valid_czech(text):\n",
                "            return None\n",
                "\n",
                "        # Unicode normalization\n",
                "        text = unicodedata.normalize(\"NFKC\", text)\n",
                "\n",
                "        # Quote and dash normalization\n",
                "        text = text.replace('\"', \"â€\").replace('\"', '\"').replace(\"-\", \"â€“\")\n",
                "\n",
                "        # Initial whitespace normalization\n",
                "        text = \" \".join(text.split())\n",
                "\n",
                "        # Thorough punctuation cleanup\n",
                "        for punct in \",.!?\":\n",
                "            # Remove all spaces before punctuation\n",
                "            text = text.replace(f\" {punct}\", punct)\n",
                "            # Replace any runs of spaces after punctuation with a single space\n",
                "            # First add space if missing\n",
                "            text = text.replace(f\"{punct}\", f\"{punct} \")\n",
                "            # Then collapse multiple spaces\n",
                "            while f\"{punct}  \" in text:\n",
                "                text = text.replace(f\"{punct}  \", f\"{punct} \")\n",
                "\n",
                "        # Final whitespace cleanup\n",
                "        text = \" \".join(text.split())\n",
                "        return text.strip()\n",
                "\n",
                "    def clean_dataframe(\n",
                "        self, df: pl.DataFrame, text_columns: List[str], num_threads: int = 8\n",
                "    ) -> pl.DataFrame:\n",
                "        \"\"\"Clean text columns and drop rows with invalid texts.\"\"\"\n",
                "        pl.Config.set_streaming_chunk_size(10000)\n",
                "\n",
                "        for col in text_columns:\n",
                "            self.logger.info(f\"Processing column: {col}\")\n",
                "\n",
                "            # Process texts in parallel\n",
                "            texts = df[col].to_list()\n",
                "            with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
                "                with tqdm(total=len(texts), desc=f\"Cleaning {col}\") as pbar:\n",
                "                    futures = []\n",
                "                    for text in texts:\n",
                "                        future = executor.submit(self._clean_text, text)\n",
                "                        future.add_done_callback(lambda p: pbar.update(1))\n",
                "                        futures.append(future)\n",
                "\n",
                "                    cleaned_texts = [future.result() for future in futures]\n",
                "\n",
                "            # Update column with cleaned texts\n",
                "            df = df.with_columns([pl.Series(col, cleaned_texts)])\n",
                "\n",
                "            # Drop rows where cleaning failed (null values)\n",
                "            initial_rows = len(df)\n",
                "            df = df.filter(~pl.col(col).is_null())\n",
                "            kept_rows = len(df)\n",
                "\n",
                "            self.logger.info(\n",
                "                f\"Kept {kept_rows:,} valid rows out of {initial_rows:,} \"\n",
                "                f\"({kept_rows/initial_rows:.1%})\"\n",
                "            )\n",
                "\n",
                "        return df\n",
                "\n",
                "    def analyze_parallel_stats(self, df: pl.DataFrame) -> dict:\n",
                "        \"\"\"Analyze parallel corpus statistics\"\"\"\n",
                "        return {\n",
                "            \"total_pairs\": len(df),\n",
                "            \"unique_cs\": df[\"cs\"].n_unique(),\n",
                "            \"unique_en\": df[\"en\"].n_unique(),\n",
                "            \"avg_cs_len\": df[\"cs\"].str.len_chars().mean(),\n",
                "            \"avg_en_len\": df[\"en\"].str.len_chars().mean(),\n",
                "            \"cs_vocab_size\": df[\"cs\"].str.split(\" \").explode().n_unique(),\n",
                "            \"en_vocab_size\": df[\"en\"].str.split(\" \").explode().n_unique(),\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AlpacaConverter:\n",
                "    \"\"\"Memory-efficient converter to Alpaca instruction format.\"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        instruction_templates: Optional[Dict[str, str]] = None,\n",
                "        chunk_size: int = 100_000\n",
                "    ):\n",
                "        self.chunk_size = chunk_size\n",
                "        self.iso_to_lang = {\n",
                "            \"en\": \"angliÄtiny\",\n",
                "            \"cs\": \"ÄeÅ¡tiny\",\n",
                "        }\n",
                "        self.instruction_templates = instruction_templates or {\n",
                "            'translation': \"PÅ™eloÅ¾ tento text z {source_lang} do {target_lang}\",\n",
                "        }\n",
                "        self.logger = logging.getLogger(__name__)\n",
                "    \n",
                "    def create_instruction(self, task_type: str, **kwargs) -> str:\n",
                "        \"\"\"Create instruction from template.\"\"\"\n",
                "        template = self.instruction_templates.get(task_type)\n",
                "        if not template:\n",
                "            raise ValueError(f\"Unknown task type: {task_type}\")\n",
                "        return template.format(**kwargs)\n",
                "    \n",
                "    def create_translation_examples(\n",
                "        self,\n",
                "        df: pl.DataFrame,\n",
                "        source_lang: str,\n",
                "        target_lang: str,\n",
                "        output_path: Union[str, Path]\n",
                "    ) -> None:\n",
                "        \"\"\"Convert translation pairs to Alpaca format.\"\"\"\n",
                "        output_path = Path(output_path)\n",
                "        \n",
                "        # Check if file already exists\n",
                "        if output_path.exists():\n",
                "            self.logger.info(f\"Using existing processed file: {output_path}\")\n",
                "            return\n",
                "            \n",
                "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        iso_to_lang = {\n",
                "            \"en\": \"angliÄtiny\",\n",
                "            \"cs\": \"ÄeÅ¡tiny\",\n",
                "        }\n",
                "        instruction = self.create_instruction(\n",
                "            'translation',\n",
                "            source_lang=iso_to_lang[source_lang],\n",
                "            target_lang=iso_to_lang[target_lang]\n",
                "        )\n",
                "        \n",
                "        # Process in chunks and collect all chunks\n",
                "        chunks = []\n",
                "        total_rows = 0\n",
                "        with tqdm(total=len(df), desc=\"Converting translations\") as pbar:\n",
                "            for i in range(0, len(df), self.chunk_size):\n",
                "                chunk = df.slice(i, min(self.chunk_size, len(df) - i))\n",
                "                \n",
                "                # Create Alpaca format efficiently\n",
                "                alpaca_df = pl.DataFrame({\n",
                "                    \"instruction\": [instruction] * len(chunk),\n",
                "                    \"input\": chunk[source_lang],\n",
                "                    \"output\": chunk[target_lang]\n",
                "                })\n",
                "                chunks.append(alpaca_df)\n",
                "                total_rows += len(chunk)\n",
                "                pbar.update(len(chunk))\n",
                "                pbar.set_postfix({\"total_rows\": total_rows})\n",
                "        \n",
                "        # Combine all chunks and save at once\n",
                "        self.logger.info(f\"Combining {len(chunks)} chunks with {total_rows:,} total rows\")\n",
                "        final_df = pl.concat(chunks)\n",
                "        \n",
                "        self.logger.info(f\"Saving {total_rows:,} examples to {output_path}\")\n",
                "        final_df.write_parquet(\n",
                "            output_path,\n",
                "            compression=\"zstd\",\n",
                "            compression_level=3\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Data Processing Pipeline\n",
                "\n",
                "Now let's use our optimized classes to process the datasets:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:datasets:PyTorch version 2.1.0+cu118 available.\n",
                        "INFO:datasets:Polars version 1.19.0 available.\n",
                        "INFO:__main__:Loading cached processed data from cache/en-cs.parquet\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading and analyzing ParaCrawl dataset...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:__main__:Processing column: cs\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Raw ParaCrawl dataset:\n",
                        "shape: (5, 2)\n",
                        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                        "â”‚ en                              â”† cs                              â”‚\n",
                        "â”‚ ---                             â”† ---                             â”‚\n",
                        "â”‚ str                             â”† str                             â”‚\n",
                        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
                        "â”‚ Offering various dining optionâ€¦ â”† HostÃ© se mohou najÃ­st v restauâ€¦ â”‚\n",
                        "â”‚ As families grow in size, so tâ€¦ â”† ÄŒÃ­m vÄ›tÅ¡Ã­ rodina, tÃ­m vÃ­ce poÅ¾â€¦ â”‚\n",
                        "â”‚ Weather in Barueri: no precipiâ€¦ â”† PoÄasÃ­ v Barueri: pÅ™ehÃ¡Åˆky - 0â€¦ â”‚\n",
                        "â”‚ Local Time: SÄ«dÄ« SÄlim, Egypt   â”† MÃ­stnÃ­ Äas: Al Husayniyah, Egyâ€¦ â”‚\n",
                        "â”‚ Then let him patiently wait anâ€¦ â”† Pak nechÅ¥ trpÄ›livÄ› ÄekÃ¡ a peÄlâ€¦ â”‚\n",
                        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                        "\n",
                        "Cleaning Czech texts...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "99caa7505f184765929972ecd31930a6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Cleaning cs:   0%|          | 0/50000 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:__main__:Kept 28,674 valid rows out of 50,000 (57.3%)\n",
                        "INFO:__main__:Using existing processed file: data/processed/paracrawl_alpaca.parquet\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'total_pairs': 28674, 'unique_cs': 28565, 'unique_en': 28657, 'avg_cs_len': 89.32743949222292, 'avg_en_len': 91.80592174094998, 'cs_vocab_size': 94016, 'en_vocab_size': 66755}\n",
                        "Cleaned ParaCrawl dataset:\n",
                        "shape: (5, 2)\n",
                        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                        "â”‚ en                              â”† cs                              â”‚\n",
                        "â”‚ ---                             â”† ---                             â”‚\n",
                        "â”‚ str                             â”† str                             â”‚\n",
                        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
                        "â”‚ Offering various dining optionâ€¦ â”† HostÃ© se mohou najÃ­st v restauâ€¦ â”‚\n",
                        "â”‚ As families grow in size, so tâ€¦ â”† ÄŒÃ­m vÄ›tÅ¡Ã­ rodina, tÃ­m vÃ­ce poÅ¾â€¦ â”‚\n",
                        "â”‚ Brojenje - Total count of messâ€¦ â”† PoÄet â€“ CelkovÃ½ poÄet poselstvâ€¦ â”‚\n",
                        "â”‚ The entire route is about 151 â€¦ â”† CelÃ¡ trasa mÄ›Å™Ã­ cca 151 km.     â”‚\n",
                        "â”‚ Sort by Most Subscribed         â”† TÅ™Ã­dit podle NejlÃ©pe hodnocenÃ©  â”‚\n",
                        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                        "\n",
                        "Converting to Alpaca format...\n",
                        "Processing translations...\n"
                    ]
                }
            ],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Initialize components with optimized settings\n",
                "loader = ParaCrawlDataLoader(chunk_size=500_000)\n",
                "cleaner = CzechTextCleaner()\n",
                "converter = AlpacaConverter(chunk_size=100_000)\n",
                "\n",
                "# Setup paths\n",
                "data_dir = Path(\"data\")\n",
                "processed_dir = data_dir / \"processed\"\n",
                "processed_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "paracrawl_path = processed_dir / \"paracrawl_alpaca.parquet\"\n",
                "books_path = processed_dir / \"books_alpaca.parquet\"\n",
                "\n",
                "# Load ParaCrawl\n",
                "print(\"Loading and analyzing ParaCrawl dataset...\")\n",
                "df_paracrawl = loader.load_dataframe()\n",
                "df_paracrawl = df_paracrawl.slice(0, 50000)\n",
                "\n",
                "print(\"Raw ParaCrawl dataset:\")\n",
                "print(df_paracrawl.head())\n",
                "\n",
                "# Store raw data for comparison\n",
                "raw_paracrawl = df_paracrawl.clone()\n",
                "\n",
                "# Clean Czech texts\n",
                "print(\"\\nCleaning Czech texts...\")\n",
                "df_paracrawl = cleaner.clean_dataframe(\n",
                "    df_paracrawl,\n",
                "    text_columns=[\"cs\"],\n",
                "    num_threads=24\n",
                ")\n",
                "\n",
                "# Analyze stats\n",
                "stats = cleaner.analyze_parallel_stats(df_paracrawl)\n",
                "print(stats)\n",
                "\n",
                "print(\"Cleaned ParaCrawl dataset:\")\n",
                "print(df_paracrawl.head())\n",
                "\n",
                "# Convert to Alpaca format with progress tracking\n",
                "print(\"\\nConverting to Alpaca format...\")\n",
                "\n",
                "# Process translations\n",
                "print(\"Processing translations...\")\n",
                "converter.create_translation_examples(\n",
                "    df_paracrawl,\n",
                "    source_lang=\"en\",\n",
                "    target_lang=\"cs\",\n",
                "    output_path=paracrawl_path,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's examine the processed dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading processed datasets...\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Translations dataset:\n",
                        "shape: (5, 3)\n",
                        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                        "â”‚ instruction         â”† input                           â”† output                          â”‚\n",
                        "â”‚ ---                 â”† ---                             â”† ---                             â”‚\n",
                        "â”‚ str                 â”† str                             â”† str                             â”‚\n",
                        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
                        "â”‚ PÅ™eloÅ¾ tento text z â”† Offering various dining optionâ€¦ â”† HostÃ© se mohou najÃ­st v restauâ€¦ â”‚\n",
                        "â”‚ angliÄtinyâ€¦         â”†                                 â”†                                 â”‚\n",
                        "â”‚ PÅ™eloÅ¾ tento text z â”† As families grow in size, so tâ€¦ â”† ÄŒÃ­m vÄ›tÅ¡Ã­ rodina, tÃ­m vÃ­ce poÅ¾â€¦ â”‚\n",
                        "â”‚ angliÄtinyâ€¦         â”†                                 â”†                                 â”‚\n",
                        "â”‚ PÅ™eloÅ¾ tento text z â”† Brojenje - Total count of messâ€¦ â”† PoÄet â€“ CelkovÃ½ poÄet poselstvâ€¦ â”‚\n",
                        "â”‚ angliÄtinyâ€¦         â”†                                 â”†                                 â”‚\n",
                        "â”‚ PÅ™eloÅ¾ tento text z â”† The entire route is about 151 â€¦ â”† CelÃ¡ trasa mÄ›Å™Ã­ cca 151 km.     â”‚\n",
                        "â”‚ angliÄtinyâ€¦         â”†                                 â”†                                 â”‚\n",
                        "â”‚ PÅ™eloÅ¾ tento text z â”† Sort by Most Subscribed         â”† TÅ™Ã­dit podle NejlÃ©pe hodnocenÃ©  â”‚\n",
                        "â”‚ angliÄtinyâ€¦         â”†                                 â”†                                 â”‚\n",
                        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
                    ]
                }
            ],
            "source": [
                "# Load processed datasets\n",
                "print(\"Loading processed datasets...\")\n",
                "translations_ds = load_dataset(\n",
                "    \"parquet\",\n",
                "    data_files=str(paracrawl_path)\n",
                ")\n",
                "\n",
                "print(\"Translations dataset:\")\n",
                "print(translations_ds[\"train\"].to_polars().head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3ï¸âƒ£ Model Training Setup\n",
                "\n",
                "Now that our data is prepared, we'll set up the model training pipeline.\n",
                "\n",
                "The training pipeline will include:\n",
                "\n",
                "1. ğŸ¤– **Model Configuration**\n",
                "   - Gemma 2B base model\n",
                "   - Mixed precision (bfloat16)\n",
                "   - Gradient accumulation\n",
                "\n",
                "2. ğŸ“Š **Training Loop**\n",
                "   - Custom data collation\n",
                "   - Efficient batching\n",
                "   - Progress tracking\n",
                "\n",
                "3. ğŸ“ˆ **Evaluation**\n",
                "   - Translation metrics\n",
                "   - Text quality assessment\n",
                "   - Error analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First let's configure the hardware settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "# Configure hardware settings\n",
                "DEVICE = \"cuda\"\n",
                "DTYPE = torch.bfloat16\n",
                "\n",
                "# For H100 we can enable TF32 for matrix multiplications\n",
                "torch.backends.cuda.matmul.allow_tf32 = True\n",
                "torch.backends.cudnn.allow_tf32 = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TrainingConfig:\n",
                "    \"\"\"Configuration for Gemma fine-tuning\"\"\"\n",
                "\n",
                "    # Model settings\n",
                "    model_name: str = \"google/gemma-2-2b-it\"\n",
                "    max_length: int = 512\n",
                "\n",
                "    # Early stopping settings\n",
                "    early_stopping_patience: int = 3\n",
                "    early_stopping_threshold: float = 0.01\n",
                "   \n",
                "    # Reduced batch sizes to fit in memory\n",
                "    train_batch_size: int = 32     # Reduced from 64\n",
                "    eval_batch_size: int = 64       # Reduced from 32\n",
                "    gradient_accumulation_steps: int = 8  # Increased to maintain effective batch size\n",
                "\n",
                "    # LoRA settings (unchanged)\n",
                "    lora_r: int = 16\n",
                "    lora_alpha: int = 32\n",
                "    lora_dropout: float = 0.05\n",
                "\n",
                "    # Training settings\n",
                "    learning_rate: float = 5e-4   # Adjusted for new effective batch size\n",
                "    weight_decay: float = 0.01\n",
                "    warmup_ratio: float = 0.03\n",
                "    max_grad_norm: float = 1.0    # Reduced to prevent memory spikes\n",
                "\n",
                "    # Evaluation settings (adjusted for memory)\n",
                "    num_epochs: int = 2\n",
                "    eval_steps: int = 100         # More frequent evaluation with smaller batches\n",
                "    save_steps: int = 200\n",
                "\n",
                "    # Paths\n",
                "    output_dir: str = \"models/gemma-cs-translator\"\n",
                "\n",
                "\n",
                "config = TrainingConfig()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's prepare the dataset with specific format for the model:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset\n",
                "\n",
                "class GemmaChatDataset(Dataset):\n",
                "    \"\"\"Dataset for Gemma chat format\"\"\"\n",
                "\n",
                "    def __init__(self, data, tokenizer, max_length=512):\n",
                "        self.data = data\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        item = self.data[idx]\n",
                "\n",
                "        # Create chat format\n",
                "        chat = [\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": f\"{item['instruction']}\\n\\n{item['input']}\",\n",
                "            }\n",
                "        ]\n",
                "\n",
                "        # Apply chat template\n",
                "        input_text = self.tokenizer.apply_chat_template(\n",
                "            chat, tokenize=False, add_generation_prompt=True\n",
                "        )\n",
                "\n",
                "        # Add expected output\n",
                "        full_text = f\"{input_text}{item['output']}<end_of_turn>\"\n",
                "\n",
                "        # Tokenize\n",
                "        encodings = self.tokenizer(\n",
                "            full_text,\n",
                "            max_length=self.max_length,\n",
                "            padding=\"max_length\",\n",
                "            truncation=True,\n",
                "            return_tensors=\"pt\",\n",
                "        )\n",
                "\n",
                "        # Create attention mask and labels\n",
                "        input_ids = encodings[\"input_ids\"][0]\n",
                "        attention_mask = encodings[\"attention_mask\"][0]\n",
                "\n",
                "        return {\n",
                "            \"input_ids\": input_ids,\n",
                "            \"attention_mask\": attention_mask,\n",
                "            \"labels\": input_ids.clone(),\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we need to prepare the model and tokenizer for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType\n",
                "\n",
                "def setup_model_and_tokenizer(config: TrainingConfig):\n",
                "    \"\"\"Initialize model with LoRA and quantization\"\"\"\n",
                "\n",
                "    # Initialize tokenizer\n",
                "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
                "\n",
                "    # Quantization config\n",
                "    quant_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    )\n",
                "\n",
                "    # Load base model\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        config.model_name,\n",
                "        quantization_config=quant_config,\n",
                "        device_map=\"auto\",\n",
                "        torch_dtype=DTYPE,\n",
                "        trust_remote_code=True,\n",
                "    )\n",
                "\n",
                "    # Prepare model for k-bit training\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "    # LoRA configuration\n",
                "    lora_config = LoraConfig(\n",
                "        r=config.lora_r,\n",
                "        lora_alpha=config.lora_alpha,\n",
                "        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "        lora_dropout=config.lora_dropout,\n",
                "        bias=\"none\",\n",
                "        task_type=TaskType.CAUSAL_LM,\n",
                "    )\n",
                "\n",
                "    # Apply LoRA\n",
                "    model = get_peft_model(model, lora_config)\n",
                "\n",
                "    return model, tokenizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we need to prepare the dataset for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_datasets(data_path: str, tokenizer, config: TrainingConfig):\n",
                "    \"\"\"Load and split datasets for training\"\"\"\n",
                "\n",
                "    # Load the processed dataset\n",
                "    dataset = load_dataset(\"parquet\", data_files=data_path)[\"train\"]\n",
                "\n",
                "    # Split into train/val/test\n",
                "    splits = dataset.train_test_split(test_size=0.2, seed=42)\n",
                "    train_data = splits[\"train\"]\n",
                "\n",
                "    # Further split test into val/test\n",
                "    test_splits = splits[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
                "    val_data = test_splits[\"train\"]\n",
                "    test_data = test_splits[\"test\"]\n",
                "\n",
                "    print(f\"Train size: {len(train_data)}\")\n",
                "    print(f\"Val size: {len(val_data)}\")\n",
                "    print(f\"Test size: {len(test_data)}\")\n",
                "\n",
                "    # Create custom datasets\n",
                "    train_dataset = GemmaChatDataset(train_data, tokenizer, config.max_length)\n",
                "    val_dataset = GemmaChatDataset(val_data, tokenizer, config.max_length)\n",
                "    test_dataset = GemmaChatDataset(test_data, tokenizer, config.max_length)\n",
                "\n",
                "    return train_dataset, val_dataset, test_dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's prepare the training loop with custom data collation and efficient batching."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "import evaluate\n",
                "import numpy as np\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "\n",
                "def get_compute_metrics(tokenizer):\n",
                "    \"\"\"Create compute_metrics function with access to tokenizer\"\"\"\n",
                "\n",
                "    def compute_metrics(eval_preds):\n",
                "        \"\"\"Compute BLEU and other metrics for translation evaluation\"\"\"\n",
                "        bleu_metric = evaluate.load(\"bleu\")\n",
                "\n",
                "        predictions, labels = eval_preds\n",
                "\n",
                "        # Decode predictions\n",
                "        predictions = np.where(\n",
                "            predictions != -100, predictions, tokenizer.pad_token_id\n",
                "        )\n",
                "        decoded_preds = tokenizer.batch_decode(\n",
                "            predictions, skip_special_tokens=True\n",
                "        )\n",
                "\n",
                "        # Clean up predictions (remove template parts)\n",
                "        cleaned_preds = []\n",
                "        for pred in decoded_preds:\n",
                "            # Extract only the translation part after the template\n",
                "            try:\n",
                "                translation = (\n",
                "                    pred.split(\"<start_of_turn>model\\n\")[1]\n",
                "                    .split(\"<end_of_turn>\")[0]\n",
                "                    .strip()\n",
                "                )\n",
                "            except IndexError:\n",
                "                translation = pred  # Fallback if splitting fails\n",
                "            cleaned_preds.append(translation)\n",
                "\n",
                "        # Decode labels\n",
                "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "        decoded_labels = tokenizer.batch_decode(\n",
                "            labels,\n",
                "            skip_special_tokens=True,\n",
                "        )\n",
                "\n",
                "        # Clean up references (remove template parts)\n",
                "        cleaned_refs = []\n",
                "        for ref in decoded_labels:\n",
                "            try:\n",
                "                translation = (\n",
                "                    ref.split(\"<start_of_turn>model\\n\")[1]\n",
                "                    .split(\"<end_of_turn>\")[0]\n",
                "                    .strip()\n",
                "                )\n",
                "            except IndexError:\n",
                "                translation = ref  # Fallback if splitting fails\n",
                "            cleaned_refs.append([translation])\n",
                "\n",
                "        # Compute BLEU\n",
                "        bleu_score = bleu_metric.compute(\n",
                "            predictions=cleaned_preds, references=cleaned_refs\n",
                "        )\n",
                "\n",
                "        return {\n",
                "            \"bleu\": bleu_score[\"bleu\"],\n",
                "        }\n",
                "\n",
                "    return compute_metrics\n",
                "\n",
                "\n",
                "def get_training_args(config: TrainingConfig):\n",
                "    return TrainingArguments(\n",
                "        output_dir=config.output_dir,\n",
                "        num_train_epochs=config.num_epochs,\n",
                "        per_device_train_batch_size=config.train_batch_size,\n",
                "        per_device_eval_batch_size=config.eval_batch_size,\n",
                "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
                "        \n",
                "        # H100-specific optimizations\n",
                "        bf16=True,  # H100 has excellent bfloat16 support\n",
                "        tf32=True,  # Enable tensor float 32 for faster matrix multiplications\n",
                "        gradient_checkpointing=True,\n",
                "        \n",
                "        # Optimizer settings\n",
                "        learning_rate=config.learning_rate,\n",
                "        weight_decay=config.weight_decay,\n",
                "        warmup_ratio=config.warmup_ratio,\n",
                "        max_grad_norm=1.0,\n",
                "        optim=\"adamw_torch_fused\",  # Use fused optimizer for better performance\n",
                "        \n",
                "        # Evaluation and saving\n",
                "        evaluation_strategy=\"steps\",\n",
                "        eval_steps=config.eval_steps,\n",
                "        save_strategy=\"steps\",\n",
                "        save_steps=config.save_steps,\n",
                "        \n",
                "        # Logging\n",
                "        logging_strategy=\"steps\",\n",
                "        logging_steps=100,\n",
                "        logging_first_step=True,\n",
                "        report_to=[\"wandb\"],\n",
                "        \n",
                "        # Model selection\n",
                "        load_best_model_at_end=True,\n",
                "        metric_for_best_model=\"bleu\",\n",
                "        greater_is_better=True,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now before proceeding with the training, let's validate the pipeline with small sample of data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d5ffb9e7c08945d4b2c6a5cfc7cf0cd5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Data Pipeline Validation ===\n",
                        "\n",
                        "1. Input IDs shape: torch.Size([512])\n",
                        "\n",
                        "2. Decoded input:\n",
                        "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos><bos><start_of_turn>user\n",
                        "PÅ™eloÅ¾ tento text z angliÄtiny do ÄeÅ¡tiny\n",
                        "\n",
                        "Offering various dining options, Snack Dar el Bacha and Fouquet's Marrakech are about 5-minute walk away.<end_of_turn>\n",
                        "<start_of_turn>model\n",
                        "HostÃ© se mohou najÃ­st v restauraci Fritos a Snack Dar el Bacha umÃ­stÄ›nÃ© ve vzdÃ¡lenosti 5 minut pÄ›Å¡ky od hotelu.<end_of_turn>\n",
                        "\n",
                        "3. Metrics computation test:\n",
                        "{'bleu': 1.0}\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def validate_data_pipeline():\n",
                "    \"\"\"Validate the entire data pipeline\"\"\"\n",
                "    # Load a small subset\n",
                "    dataset = load_dataset(\n",
                "        \"parquet\", data_files=\"data/processed/paracrawl_alpaca.parquet\"\n",
                "    )[\"train\"].select(range(5))\n",
                "\n",
                "    # Initialize components\n",
                "    _, tokenizer = setup_model_and_tokenizer(config)\n",
                "    train_dataset = GemmaChatDataset(dataset, tokenizer)\n",
                "\n",
                "    # Check a sample\n",
                "    sample = train_dataset[0]\n",
                "\n",
                "    print(\"=== Data Pipeline Validation ===\")\n",
                "    print(\"\\n1. Input IDs shape:\", sample[\"input_ids\"].shape)\n",
                "    print(\"\\n2. Decoded input:\")\n",
                "    print(tokenizer.decode(sample[\"input_ids\"]))\n",
                "\n",
                "    # Test compute_metrics\n",
                "    dummy_preds = (\n",
                "        sample[\"input_ids\"].unsqueeze(0),\n",
                "        sample[\"labels\"].unsqueeze(0),\n",
                "    )\n",
                "    metrics = get_compute_metrics(tokenizer)(dummy_preds)\n",
                "\n",
                "    print(\"\\n3. Metrics computation test:\")\n",
                "    print(metrics)\n",
                "\n",
                "    return True\n",
                "\n",
                "\n",
                "validate_data_pipeline()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's define the training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjirkax\u001b[0m (\u001b[33mjirkax-individual\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.19.2"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/root/gemma-global-competition/wandb/run-20250109_025419-ymd8b3x9</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/jirkax-individual/gemma-cs-translator/runs/ymd8b3x9' target=\"_blank\">super-sunset-18</a></strong> to <a href='https://wandb.ai/jirkax-individual/gemma-cs-translator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/jirkax-individual/gemma-cs-translator' target=\"_blank\">https://wandb.ai/jirkax-individual/gemma-cs-translator</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/jirkax-individual/gemma-cs-translator/runs/ymd8b3x9' target=\"_blank\">https://wandb.ai/jirkax-individual/gemma-cs-translator/runs/ymd8b3x9</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b092b5dcd5cd479ba633279d86753df3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train size: 22939\n",
                        "Val size: 2867\n",
                        "Test size: 2868\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
                        "  warnings.warn(\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
                        "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='5' max='178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [  5/178 00:45 < 43:21, 0.07 it/s, Epoch 0.04/2]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[14], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer, tokenizer\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m trainer, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_translation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[14], line 51\u001b[0m, in \u001b[0;36mtrain_translation_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     42\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     43\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping_callback],\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2522\u001b[0m )\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2530\u001b[0m ):\n\u001b[1;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3687\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3685\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3687\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3688\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2248\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "import wandb\n",
                "from transformers import Trainer\n",
                "from transformers import EarlyStoppingCallback\n",
                "\n",
                "def train_translation_model():\n",
                "    \"\"\"Main training function with full pipeline\"\"\"\n",
                "    # Initialize wandb\n",
                "    wandb.init(\n",
                "        project=\"gemma-cs-translator\",\n",
                "        config={\n",
                "            \"model\": config.model_name,\n",
                "            \"lora_r\": config.lora_r,\n",
                "            \"num_epochs\": config.num_epochs,\n",
                "            \"train_batch_size\": config.train_batch_size,\n",
                "            \"eval_batch_size\": config.eval_batch_size,\n",
                "            \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
                "            \"learning_rate\": config.learning_rate,\n",
                "            \"early_stopping_patience\": config.early_stopping_patience,\n",
                "            \"early_stopping_threshold\": config.early_stopping_threshold,\n",
                "        },\n",
                "    )\n",
                "\n",
                "    # Setup model and tokenizer\n",
                "    model, tokenizer = setup_model_and_tokenizer(config)\n",
                "\n",
                "    # Prepare datasets\n",
                "    train_dataset, val_dataset, _ = prepare_datasets(\n",
                "        \"data/processed/paracrawl_alpaca.parquet\", tokenizer, config\n",
                "    )\n",
                "\n",
                "    # Setup training arguments\n",
                "    training_args = get_training_args(config)\n",
                "\n",
                "    # Create early stopping callback\n",
                "    early_stopping_callback = EarlyStoppingCallback(\n",
                "        early_stopping_patience=config.early_stopping_patience,\n",
                "        early_stopping_threshold=config.early_stopping_threshold,\n",
                "    )\n",
                "\n",
                "    # Initialize trainer\n",
                "    trainer = Trainer(\n",
                "        model=model,\n",
                "        args=training_args,\n",
                "        train_dataset=train_dataset,\n",
                "        eval_dataset=val_dataset,\n",
                "        compute_metrics=get_compute_metrics(tokenizer),\n",
                "        callbacks=[early_stopping_callback],\n",
                "    )\n",
                "\n",
                "    # Train\n",
                "    train_result = trainer.train()\n",
                "\n",
                "    # Save final model\n",
                "    trainer.save_model(f\"{config.output_dir}/final\")\n",
                "\n",
                "    # Log metrics\n",
                "    metrics = train_result.metrics\n",
                "    trainer.log_metrics(\"train\", metrics)\n",
                "\n",
                "    return trainer, tokenizer\n",
                "\n",
                "\n",
                "# Run training\n",
                "trainer, tokenizer = train_translation_model()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now post training we can evaluate the model on the test dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TranslationInference:\n",
                "    def __init__(self, model_path: str):\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
                "        self.model = AutoModelForCausalLM.from_pretrained(\n",
                "            model_path,\n",
                "            device_map=\"auto\",\n",
                "            torch_dtype=torch.bfloat16,\n",
                "            trust_remote_code=True,\n",
                "        ).eval()\n",
                "        \n",
                "        # Cache the prompt template\n",
                "        self.prompt = \"PÅ™eloÅ¾ tento text z angliÄtiny do ÄeÅ¡tiny:\\n\\n\"\n",
                "       \n",
                "    def translate(self, text: str) -> str:\n",
                "        chat = [\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": f\"PÅ™eloÅ¾ tento text z angliÄtiny do ÄeÅ¡tiny:\\n\\n{text}\"\n",
                "            }\n",
                "        ]\n",
                "        \n",
                "        prompt = self.tokenizer.apply_chat_template(\n",
                "            chat,\n",
                "            tokenize=False,\n",
                "            add_generation_prompt=True\n",
                "        )\n",
                "        \n",
                "        inputs = self.tokenizer(\n",
                "            prompt,\n",
                "            return_tensors=\"pt\",\n",
                "            truncation=True,\n",
                "            max_length=256\n",
                "        ).to(self.model.device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = self.model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=256,\n",
                "                do_sample=False,\n",
                "                num_beams=1\n",
                "            )\n",
                "        \n",
                "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        # Extract only the model's translation\n",
                "        translation = response.split(\"model\\n\")[-1].split(\"<end_of_turn>\")[0].strip()\n",
                "        return translation\n",
                "       \n",
                "    def translate_batch(self, texts: List[str], max_length: int = 256) -> List[str]:\n",
                "        # Create chat format for each text\n",
                "        chats = [\n",
                "            [{\n",
                "                \"role\": \"user\",\n",
                "                \"content\": f\"PÅ™eloÅ¾ tento text z angliÄtiny do ÄeÅ¡tiny:\\n\\n{text}\"\n",
                "            }] for text in texts\n",
                "        ]\n",
                "        \n",
                "        # Apply chat template to each\n",
                "        prompts = [\n",
                "            self.tokenizer.apply_chat_template(\n",
                "                chat,\n",
                "                tokenize=False,\n",
                "                add_generation_prompt=True\n",
                "            ) for chat in chats\n",
                "        ]\n",
                "        \n",
                "        # Tokenize batch\n",
                "        inputs = self.tokenizer(\n",
                "            prompts,\n",
                "            return_tensors=\"pt\",\n",
                "            padding=True,\n",
                "            truncation=True,\n",
                "            max_length=max_length,\n",
                "        ).to(self.model.device)\n",
                "        \n",
                "        # Generate translations\n",
                "        with torch.no_grad():\n",
                "            outputs = self.model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=max_length,\n",
                "                do_sample=False,\n",
                "                num_beams=1,\n",
                "                early_stopping=True,\n",
                "            )\n",
                "        \n",
                "        # Decode outputs\n",
                "        responses = self.tokenizer.batch_decode(\n",
                "            outputs, \n",
                "            skip_special_tokens=True,\n",
                "        )\n",
                "        \n",
                "        # Extract only the model's translations\n",
                "        translations = [\n",
                "            response.split(\"model\\n\")[-1].split(\"<end_of_turn>\")[0].strip()\n",
                "            for response in responses\n",
                "        ]\n",
                "        \n",
                "        return translations\n",
                "\n",
                "def test_model(test_dataset, model_path: str):\n",
                "    \"\"\"Evaluate model on test dataset\"\"\"\n",
                "    inference = TranslationInference(model_path)\n",
                "    bleu_metric = evaluate.load(\"bleu\")\n",
                "    \n",
                "    # Process in batches\n",
                "    batch_size = 8\n",
                "    predictions = []\n",
                "    references = []\n",
                "    \n",
                "    print(\"Running inference...\")\n",
                "    for i in tqdm(range(0, min(100, len(test_dataset)), batch_size)):\n",
                "        batch = test_dataset[i:i + batch_size]\n",
                "        \n",
                "        # Decode input_ids to get source texts\n",
                "        source_texts = []\n",
                "        batch_refs = []\n",
                "        \n",
                "        for j in range(len(batch['input_ids'])):\n",
                "            # Get non-padding tokens\n",
                "            mask = batch['attention_mask'][j] == 1\n",
                "            input_ids = batch['input_ids'][j][mask]\n",
                "            label_ids = batch['labels'][j][mask]\n",
                "            \n",
                "            # Decode source text\n",
                "            input_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
                "            # Extract English text after the instruction\n",
                "            if \"PÅ™eloÅ¾ tento text z angliÄtiny do ÄeÅ¡tiny:\" in input_text:\n",
                "                source = input_text.split(\"PÅ™eloÅ¾ tento text z angliÄtiny do ÄeÅ¡tiny:\")[-1].strip()\n",
                "            else:\n",
                "                source = input_text.strip()\n",
                "            source_texts.append(source)\n",
                "            \n",
                "            # Decode reference translation\n",
                "            reference = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
                "            if \"<end_of_turn>\" in reference:\n",
                "                reference = reference.split(\"<end_of_turn>\")[0].strip()\n",
                "            batch_refs.append([reference])\n",
                "        \n",
                "        # Get translations for batch\n",
                "        translations = inference.translate_batch(source_texts)\n",
                "        \n",
                "        predictions.extend(translations)\n",
                "        references.extend(batch_refs)\n",
                "    \n",
                "    # Calculate BLEU score\n",
                "    bleu_score = bleu_metric.compute(\n",
                "        predictions=predictions,\n",
                "        references=references\n",
                "    )\n",
                "    \n",
                "    print(f\"\\nTest BLEU Score: {bleu_score['bleu']:.2f}\")\n",
                "    \n",
                "    # Show examples\n",
                "    print(\"\\nExample Translations:\")\n",
                "    for i in range(min(3, len(predictions))):\n",
                "        print(f\"\\nSource: {source_texts[i]}\")\n",
                "        print(f\"Reference: {references[i][0]}\")\n",
                "        print(f\"Prediction: {predictions[i]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's test the model on some examples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7bc06868075e429e98c18928ac9a4153",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train size: 22939\n",
                        "Val size: 2867\n",
                        "Test size: 2868\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9e7702511d384a60af23e8c244f89fe2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Interactive Translation Examples:\n",
                        "\n",
                        "ğŸ‡¬ğŸ‡§ English: Hello, how are you today?\n",
                        "ğŸ‡¨ğŸ‡¿ Czech: Ahoj, jak se dnes mÃ¡te?\n",
                        "\n",
                        "ğŸ‡¬ğŸ‡§ English: This is a test of the translation system.\n",
                        "ğŸ‡¨ğŸ‡¿ Czech: Toto je test systÃ©mu pÅ™eklÃ¡dÃ¡nÃ­.\n",
                        "\n",
                        "ğŸ‡¬ğŸ‡§ English: Machine learning is transforming the world.\n",
                        "ğŸ‡¨ğŸ‡¿ Czech: VÃ½voj machine learningu mÄ›nÃ­ svÄ›t.\n"
                    ]
                }
            ],
            "source": [
                "# Setup model and tokenizer\n",
                "_, tokenizer = setup_model_and_tokenizer(config)\n",
                "\n",
                "# Prepare datasets\n",
                "_, _, test_dataset = prepare_datasets(\n",
                "    \"data/processed/paracrawl_alpaca.parquet\", tokenizer, config\n",
                ")\n",
                "\n",
                "# Test the trained model\n",
                "model_path = f\"{config.output_dir}/final\"\n",
                "# test_model(test_dataset, model_path)\n",
                "\n",
                "# Interactive translation example\n",
                "inference = TranslationInference(model_path)\n",
                "\n",
                "examples = [\n",
                "    \"Hello, how are you today?\",\n",
                "    \"This is a test of the translation system.\",\n",
                "    \"Machine learning is transforming the world.\",\n",
                "]\n",
                "\n",
                "print(\"\\nInteractive Translation Examples:\")\n",
                "for text in examples:\n",
                "    translation = inference.translate(text)\n",
                "    print(f\"\\nğŸ‡¬ğŸ‡§ English: {text}\")\n",
                "    print(f\"ğŸ‡¨ğŸ‡¿ Czech: {translation}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_models(test_dataset, num_examples: int = 5):\n",
                "    print(\"Loading models...\")\n",
                "    original = TranslationInference(\"google/gemma-2-2b-it\")\n",
                "    finetuned = TranslationInference(f\"{config.output_dir}/final\")\n",
                "    bleu_metric = evaluate.load(\"bleu\")\n",
                "    \n",
                "    print(\"\\n=== Model Comparison (5 Examples) ===\")\n",
                "    \n",
                "    results = {\n",
                "        \"Original\": {\"predictions\": [], \"bleu\": 0.0},\n",
                "        \"Finetuned\": {\"predictions\": [], \"bleu\": 0.0}\n",
                "    }\n",
                "    references = []\n",
                "    \n",
                "    for i in range(num_examples):\n",
                "        example = test_dataset[i]\n",
                "        mask = example['attention_mask'] == 1\n",
                "        conversation = tokenizer.decode(example['input_ids'][mask])\n",
                "        \n",
                "        try:\n",
                "            # Extract English and reference Czech\n",
                "            parts = conversation.split(\"PÅ™eloÅ¾ tento text z angliÄtiny do ÄeÅ¡tiny\\n\\n\")\n",
                "            english = parts[1].split(\"<end_of_turn>\")[0].strip()\n",
                "            czech = parts[1].split(\"<start_of_turn>model\\n\")[1].split(\"<end_of_turn>\")[0].strip()\n",
                "            \n",
                "            # Get translations from both models\n",
                "            orig_translation = original.translate(english)\n",
                "            fine_translation = finetuned.translate(english)\n",
                "            \n",
                "            # Store results\n",
                "            results[\"Original\"][\"predictions\"].append(orig_translation)\n",
                "            results[\"Finetuned\"][\"predictions\"].append(fine_translation)\n",
                "            references.append([czech])\n",
                "            \n",
                "            # Show each example with word-by-word comparison\n",
                "            print(f\"\\n{'='*80}\")\n",
                "            print(f\"Example {i+1}:\")\n",
                "            print(f\"ğŸ‡¬ğŸ‡§ Source:     {english}\")\n",
                "            print(f\"ğŸ‡¨ğŸ‡¿ Reference:  {czech}\")\n",
                "            print(f\"ğŸ‡¨ğŸ‡¿ Original:   {orig_translation}\")\n",
                "            print(f\"ğŸ‡¨ğŸ‡¿ Finetuned:  {fine_translation}\")\n",
                "            \n",
                "            # Analysis of translations\n",
                "            print(\"\\nAnalysis:\")\n",
                "            print(f\"- Original follows instruction: {'PÅ™eloÅ¾' in orig_translation or 'czech' in orig_translation.lower()}\")\n",
                "            print(f\"- Original length: {len(orig_translation.split())}\")\n",
                "            print(f\"- Finetuned length: {len(fine_translation.split())}\")\n",
                "            print(f\"- Reference length: {len(czech.split())}\")\n",
                "                \n",
                "        except Exception as e:\n",
                "            print(f\"Skipping example {i}, error: {str(e)}\")\n",
                "            continue\n",
                "    \n",
                "    # Calculate BLEU scores\n",
                "    if references:\n",
                "        print(f\"\\n{'='*80}\")\n",
                "        print(\"\\nOverall Statistics:\")\n",
                "        for model_name in results:\n",
                "            predictions = results[model_name][\"predictions\"]\n",
                "            bleu_score = bleu_metric.compute(predictions=predictions, references=references)\n",
                "            results[model_name][\"bleu\"] = bleu_score[\"bleu\"]\n",
                "            \n",
                "            # Calculate average lengths\n",
                "            avg_len = sum(len(p.split()) for p in predictions) / len(predictions)\n",
                "            print(f\"\\n{model_name} model:\")\n",
                "            print(f\"- BLEU Score: {results[model_name]['bleu']:.2f}\")\n",
                "            print(f\"- Average output length: {avg_len:.1f} words\")\n",
                "            \n",
                "        # Reference statistics\n",
                "        avg_ref_len = sum(len(r[0].split()) for r in references) / len(references)\n",
                "        print(f\"\\nReference statistics:\")\n",
                "        print(f\"- Average length: {avg_ref_len:.1f} words\")\n",
                "    else:\n",
                "        print(\"\\nNo valid examples found for evaluation\")\n",
                "\n",
                "# Run comparison\n",
                "compare_models(test_dataset)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gemma-global-competetion",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
