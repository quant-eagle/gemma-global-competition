{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (1.18.0)\n",
      "Requirement already satisfied: datasets in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (4.47.1)\n",
      "Requirement already satisfied: sentence_transformers in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: unbabel-comet in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: keras-nlp in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: jax in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.4.38)\n",
      "Requirement already satisfied: tensorflow in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-text in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (2.18.1)\n",
      "Requirement already satisfied: tf-keras in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: matplotlib in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: fasttext in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (0.9.3)\n",
      "Requirement already satisfied: wget in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (3.2)\n",
      "Requirement already satisfied: filelock in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scipy in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\n",
      "Requirement already satisfied: Pillow in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (4.25.5)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (2.5.0.post0)\n",
      "Requirement already satisfied: sacrebleu<3.0.0,>=2.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (2.5.1)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: absl-py in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: keras-hub==0.18.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras-nlp) (0.18.1)\n",
      "Requirement already satisfied: kagglehub in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (0.3.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: jaxlib<=0.4.38,>=0.4.38 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from jax) (0.4.38)\n",
      "Requirement already satisfied: opt_einsum in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: setuptools in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.17.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.11.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: portalocker in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.9.0)\n",
      "Requirement already satisfied: colorama in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.4.6)\n",
      "Requirement already satisfied: lxml in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (5.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: networkx in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jirka/Library/Caches/pypoetry/virtualenvs/gemma-czech-adaptation-iLYBdvQN-py3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install polars datasets sentencepiece transformers sentence_transformers unbabel-comet keras keras-nlp scikit-learn jax tensorflow tensorflow-text tf-keras matplotlib fasttext wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable HF Tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set the backend\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    token_limit: int = 128\n",
    "    dataset_limit: int = 10\n",
    "    batch_size: int = 1\n",
    "    lora_rank: int = 4\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 1e-4\n",
    "    model_id = \"gemma2_instruct_2b_en\"\n",
    "\n",
    "training_config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the Europarl dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema([('translation', Struct({'cs': String, 'en': String}))])\n",
      "\n",
      "First row:\n",
      "shape: (1, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ translation                     │\n",
      "│ ---                             │\n",
      "│ struct[2]                       │\n",
      "╞═════════════════════════════════╡\n",
      "│ {\"Následný postup na základě u… │\n",
      "└─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import polars as pl\n",
    "\n",
    "ds = load_dataset(\"Helsinki-NLP/europarl\", \"cs-en\")\n",
    "df = ds[\"train\"].to_polars()\n",
    "\n",
    "# Reduce the dataset to our training limit\n",
    "df = df.head(training_config.dataset_limit)\n",
    "\n",
    "# Let's check the structure\n",
    "print(df.schema)\n",
    "print(\"\\nFirst row:\")\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load an auto-translated Alpaca style dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad33018fa364fba875c81e50872acfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691a9d0472d143658457990b408329d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-74f56a6f7963eaf6.parquet:   0%|          | 0.00/21.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab802071ad3041c499946961c703421d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-619baf04657a4a76.parquet:   0%|          | 0.00/5.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4393ee826e34feeafb90ad50f12df35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/41601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9c91bb87904c6e961ca35a7a4e668a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema([('instruction', String), ('input', String), ('output', String)])\n",
      "\n",
      "First row:\n",
      "shape: (1, 3)\n",
      "┌─────────────────────────────────┬───────┬─────────────────────────────────┐\n",
      "│ instruction                     ┆ input ┆ output                          │\n",
      "│ ---                             ┆ ---   ┆ ---                             │\n",
      "│ str                             ┆ str   ┆ str                             │\n",
      "╞═════════════════════════════════╪═══════╪═════════════════════════════════╡\n",
      "│  Uveďte 5 běžných přídavných j… ┆ nan   ┆  1. Ohromující 2. Úchvatné 3. … │\n",
      "└─────────────────────────────────┴───────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"saillab/alpaca-czech-cleaned\")\n",
    "df_alpaca = ds[\"train\"].to_polars()\n",
    "\n",
    "# Let's check the structure\n",
    "print(df_alpaca.schema)\n",
    "print(\"\\nFirst row:\")\n",
    "print(df_alpaca.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the translation column\n",
    "df_norm = df.select(\n",
    "    [\n",
    "        pl.col(\"translation\").struct.field(\"en\").alias(\"en\"),\n",
    "        pl.col(\"translation\").struct.field(\"cs\").alias(\"cs\"),\n",
    "        pl.col(\"translation\").struct.field(\"cs\").str.len_chars().alias(\"cs_len\"),\n",
    "        pl.col(\"translation\").struct.field(\"en\").str.len_chars().alias(\"en_len\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Let's check the normalized structure\n",
    "print(df_norm.schema)\n",
    "print(\"\\nFirst row:\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to preprocess the data to get achieve high quality results.\n",
    "\n",
    "First, let's clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(df, min_length=3, max_distance_ratio=2.5):\n",
    "    \"\"\"\n",
    "    Clean the texts by replacing multiple spaces with a single space and stripping leading and trailing spaces.\n",
    "    We also filter out very short texts (less than 3 characters) and texts where one language is more than 2.5x longer than the other.\n",
    "    \"\"\"\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            # Clean the English text\n",
    "            pl.col(\"en\")\n",
    "            .str.replace_all(r\"\\s+\", \" \")\n",
    "            .str.strip_chars()\n",
    "            .alias(\"en_clean\"),\n",
    "            # Clean the Czech text\n",
    "            pl.col(\"cs\")\n",
    "            .str.replace_all(r\"\\s+\", \" \")\n",
    "            .str.strip_chars()\n",
    "            .alias(\"cs_clean\"),\n",
    "        ]\n",
    "    ).filter(\n",
    "        # Filter out rows with non a-Z characters\n",
    "        ~pl.col(\"cs_clean\").str.contains(r\"^[a-zA-Z]+$\")\n",
    "        & ~pl.col(\"en_clean\").str.contains(r\"^[a-zA-Z]+$\")\n",
    "        # Filter out very short texts (less than 3 characters)\n",
    "        & (pl.col(\"cs_len\") >= min_length)\n",
    "        & (pl.col(\"en_len\") >= min_length)\n",
    "        # This helps remove poor quality or misaligned translations\n",
    "        & (pl.col(\"cs_len\") / pl.col(\"en_len\") <= max_distance_ratio)  # Czech text not too long compared to English\n",
    "        & (pl.col(\"en_len\") / pl.col(\"cs_len\") <= max_distance_ratio)  # English text not too long compared to Czech\n",
    "    )\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = clean_texts(df_norm)\n",
    "print(f\"Dataset shape after cleaning: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a language detection check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import wget\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detect_language(df, threshold=0.8, batch_size=100):\n",
    "    \"\"\"\n",
    "    Detect the language of the text.\n",
    "    \"\"\"\n",
    "    print(f\"Detecting language of the text with threshold {threshold} and batch size {batch_size}\")\n",
    "\n",
    "    # Load the fasttext model\n",
    "    model_path = \"models/fasttext/lid.176.bin\"\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        wget.download(\"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\", out=model_path)\n",
    "\n",
    "    model = fasttext.load_model(model_path)\n",
    "\n",
    "    def predict_language(text):\n",
    "        \"\"\"\n",
    "        Check if the text is in the expected language.\n",
    "        \"\"\"\n",
    "        prediction = model.predict(text, k=1)\n",
    "        return prediction\n",
    "\n",
    "    # Process in batches\n",
    "    cs_clean = df['cs_clean'].to_list()\n",
    "    en_clean = df['en_clean'].to_list()\n",
    "\n",
    "    cs_results = []\n",
    "    en_results = []\n",
    "    bad_languages = []\n",
    "\n",
    "    for i in range(0, len(cs_clean), batch_size):\n",
    "        batch_cs = cs_clean[i:i+batch_size]\n",
    "        batch_en = en_clean[i:i+batch_size]\n",
    "\n",
    "        # Check if the text is in the expected language\n",
    "        for text in tqdm(batch_cs, desc=\"Checking Czech language\"):\n",
    "            result = predict_language(text)\n",
    "            if result[0][0] == \"__label__cs\":\n",
    "                cs_results.append(True)\n",
    "            else:\n",
    "                cs_results.append(False)\n",
    "                bad_languages.append(result[0][0])\n",
    "\n",
    "        for text in tqdm(batch_en, desc=\"Checking English language\"):\n",
    "            result = predict_language(text)\n",
    "            if result[0][0] == \"__label__en\":\n",
    "                en_results.append(True)\n",
    "            else:\n",
    "                en_results.append(False)\n",
    "                bad_languages.append(result[0][0])\n",
    "\n",
    "    print(f\"Detected bad languages: {bad_languages}\")\n",
    "\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            pl.Series(\"is_valid_cs\", cs_results),\n",
    "            pl.Series(\"is_valid_en\", en_results)\n",
    "        ]\n",
    "    ).filter(pl.col(\"is_valid_cs\") & pl.col(\"is_valid_en\"))\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = detect_language(df_norm)\n",
    "print(f\"Dataset shape after language detection: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out the rows based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "def filter_similar_texts(df, threshold=0.8, batch_size=100):\n",
    "    \"\"\"\n",
    "    Filter out the rows based on similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Filtering out rows with similarity less than {threshold}\")\n",
    "    print(f\"Processing in batches of {batch_size} rows\")\n",
    "    \n",
    "    # Process in batches\n",
    "    cs_clean = df['cs_clean'].to_list()\n",
    "    en_clean = df['en_clean'].to_list()\n",
    "    \n",
    "    similarities = []\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for i in range(0, len(cs_clean), batch_size):\n",
    "        batch_cs = cs_clean[i:i+batch_size]\n",
    "        batch_en = en_clean[i:i+batch_size]\n",
    "        \n",
    "        # Compute the embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings_cs = model.encode(batch_cs, convert_to_tensor=True)\n",
    "            embeddings_en = model.encode(batch_en, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute the similarity\n",
    "        batch_similarities = torch.nn.functional.cosine_similarity(embeddings_cs, embeddings_en, dim=1)\n",
    "        similarities.extend(batch_similarities.cpu().numpy())\n",
    "        \n",
    "    return df.with_columns(pl.Series(\"similarity\", similarities)).filter(pl.col(\"similarity\") > threshold)\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = filter_similar_texts(df_norm)\n",
    "print(f\"Filtered dataset shape: {df_norm.shape}\")\n",
    "print(df_norm.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform an alignment check. We will use COMET to check the quality of the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from comet import load_from_checkpoint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the COMET model\n",
    "model_path = snapshot_download(\"Unbabel/wmt22-comet-da\")\n",
    "model_checkpoint_path = f\"{model_path}/checkpoints/model.ckpt\"\n",
    "model = load_from_checkpoint(model_checkpoint_path)\n",
    "\n",
    "def filter_by_quality(df, treshold = 0.4, batch_size = 100):\n",
    "    \"\"\"\n",
    "    Filter out the rows based on the quality of the alignment.\n",
    "    \"\"\"\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Get texts\n",
    "    cs_clean = df[\"cs_clean\"].to_list()\n",
    "    en_clean = df[\"en_clean\"].to_list()\n",
    "\n",
    "    # Prepare data for COMET\n",
    "    data = [\n",
    "        {\"src\": cs, \"mt\": en, \"ref\": None}\n",
    "        for cs, en in zip(cs_clean, en_clean)\n",
    "    ]\n",
    "    \n",
    "    # Get scores from COMET\n",
    "    predictions = model.predict(data, batch_size=batch_size)\n",
    "    scores = predictions.scores\n",
    "\n",
    "    return df.with_columns(\n",
    "        [pl.Series(\"quality_score\", scores)]\n",
    "    ).filter(pl.col(\"quality_score\") > treshold)\n",
    "\n",
    "\n",
    "print(f\"Dataset shape: {df_norm.shape}\")\n",
    "df_norm = filter_by_quality(df_norm)\n",
    "print(f\"Filtered dataset shape: {df_norm.shape}\")\n",
    "print(df_norm.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the distribution of the quality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distribution\n",
    "print(\n",
    "    df_norm.select(\n",
    "        [\n",
    "            pl.col(\"quality_score\").quantile(0.25).alias(\"25th_percentile\"),\n",
    "            pl.col(\"quality_score\").quantile(0.5).alias(\"median\"),\n",
    "            pl.col(\"quality_score\").quantile(0.75).alias(\"75th_percentile\"),\n",
    "            pl.col(\"quality_score\").mean().alias(\"mean\"),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the quality of the alignment is pretty good for most of the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's normalize the dataset to the instruction format and save the dataset to a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction_format(x):\n",
    "    \"\"\"Format input text into an instruction format for translation.\n",
    "\n",
    "    Args:\n",
    "        x: Dictionary containing 'en_clean' (English text) and 'cs_clean' (Czech translation)\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted instruction string with the following structure:\n",
    "            <start_of_turn>user\n",
    "            Přelož tento text z angličtiny do češtiny.\n",
    "            \"{English text}\"\n",
    "            <end_of_turn>\n",
    "            <start_of_turn>model\n",
    "            {Czech translation}<end_of_turn>\n",
    "    \"\"\"\n",
    "    return f\"<start_of_turn>user\\nPřelož tento text z angličtiny do češtiny.\\n\\\"{x['en_clean']}\\\"<end_of_turn>\\n<start_of_turn>model\\n{x['cs_clean']}<end_of_turn>\"\n",
    "\n",
    "finetune_df = df_norm.with_columns(\n",
    "    [\n",
    "        pl.struct([\"en_clean\", \"cs_clean\"])\n",
    "        .map_elements(instruction_format, return_dtype=pl.Utf8)\n",
    "        .alias(\"instruction\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(finetune_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we need to tokenize the dataset. First we will load the tokenizer and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "\n",
    "# Load the Gemma2 tokenizer\n",
    "tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(\n",
    "    training_config.model_id\n",
    ")\n",
    "\n",
    "# Test the tokenizer\n",
    "print(tokenizer(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {finetune_df.shape}\")\n",
    "print(f\"Token limit: {training_config.token_limit}\")\n",
    "\n",
    "finetune_df = finetune_df.with_columns(\n",
    "    [\n",
    "        pl.col(\"instruction\")\n",
    "        .map_elements(lambda x: tokenizer(x).tolist(), return_dtype=pl.List(pl.Int32))\n",
    "        .alias(\"tokens\")\n",
    "    ]\n",
    ").filter(\n",
    "    pl.col(\"tokens\").list.len() <= training_config.token_limit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the token distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "token_lengths = finetune_df[\"tokens\"].map_elements(len, return_dtype=pl.Int32)\n",
    "print(\"Token length statistics:\")\n",
    "print(f\"Mean: {token_lengths.mean()}\")\n",
    "print(f\"Median: {token_lengths.median()}\")\n",
    "print(f\"95th percentile: {token_lengths.quantile(0.95)}\")\n",
    "print(f\"Max: {token_lengths.max()}\")\n",
    "\n",
    "# Plot the token length distribution\n",
    "plt.hist(token_lengths, bins=50)\n",
    "plt.title(\"Token Length Distribution\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, filter to the token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_df = finetune_df.select(\n",
    "    [\n",
    "        pl.col(\"instruction\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape post-tokenization: {finetune_df.shape}\")\n",
    "print(finetune_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally, let's calculate the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout rate is the percentage of rows that are filtered out.\n",
    "dropout_rate = (len(df) - len(finetune_df)) / len(df)\n",
    "print(f\"Dropout rate: {dropout_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout rate is pretty high, but it's a good thing that we have a lot of data to work with and we aim for high quality results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is ready for the next step. We will use it to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split into train and temp (80/20)\n",
    "train_df, temp_df = train_test_split(finetune_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then split temp into validation and test (50/50, resulting in 10/10 of original)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save all splits to parquet files\n",
    "train_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_train.parquet\", compression=\"zstd\"\n",
    ")\n",
    "valid_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_valid.parquet\", compression=\"zstd\"\n",
    ")\n",
    "test_df.write_parquet(\n",
    "    file=\"data/processed/gemma_cs_test.parquet\", compression=\"zstd\"\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(valid_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(training_config.model_id)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the inference of our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Přelož tento text z angličtiny do češtiny:\\n\"Hello, world!\"'\n",
    "print(\n",
    "    gemma_lm.generate(\n",
    "        prompt,\n",
    "        max_length=training_config.token_limit,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LoRA on the model\n",
    "gemma_lm.backbone.enable_lora(rank=training_config.lora_rank)\n",
    "gemma_lm.summary()\n",
    "\n",
    "# Control memory usage\n",
    "gemma_lm.preprocessor.sequence_length = training_config.token_limit\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=training_config.learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "# Setup loss\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Setup metrics\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "# Compile the model\n",
    "gemma_lm.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the dataset to Keras expected format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_df[\"instruction\"].to_list()\n",
    "valid_dataset = valid_df[\"instruction\"].to_list()\n",
    "\n",
    "print(f\"Train dataset shape: {len(train_dataset)}\")\n",
    "print(train_dataset[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Valid dataset shape: {len(valid_dataset)}\")\n",
    "print(valid_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now ready for training. Let's train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "import wandb\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch} finished\")\n",
    "        model_name = f\"gemma_cs_{training_config.lora_rank}_epoch{epoch+1}.lora.h5\"\n",
    "        gemma_lm.backbone.save_lora_weights(model_name)\n",
    "        print(f\"Model {model_name} saved\")\n",
    "\n",
    "        # Eval on prompt\n",
    "        prompt = \"Přelož tento text z angličtiny do češtiny.\\n\\\"Hello, world!\\\"\"\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(\n",
    "            f\"Response: {gemma_lm.generate(prompt, max_length=training_config.token_limit)}\"\n",
    "        )\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(config={\n",
    "    \"learning_rate\": training_config.learning_rate,\n",
    "    \"epochs\": training_config.epochs,\n",
    "    \"batch_size\": training_config.batch_size,\n",
    "    \"lora_rank\": training_config.lora_rank,\n",
    "    \"token_limit\": training_config.token_limit,\n",
    "    \"model\": \"gemma2_instruct_2b_en\",\n",
    "    \"task\": \"czech-context\"\n",
    "})\n",
    "\n",
    "# Convert lists to TensorFlow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices(valid_dataset)\n",
    "\n",
    "# Configure datasets for performance\n",
    "train_ds = train_ds.batch(training_config.batch_size)\n",
    "valid_ds = valid_ds.batch(training_config.batch_size)\n",
    "\n",
    "# Enable prefetching to improve performance\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "history = gemma_lm.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=training_config.epochs,\n",
    "    callbacks=[CustomCallback(), WandbMetricsLogger()],\n",
    ")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.plot(history.history[\"sparse_categorical_accuracy\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-czech-adaptation-iLYBdvQN-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
