{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Czech Language Adaptation of Gemma Language Model\n",
    "\n",
    "**Author:** Jirka Helmich\n",
    "\n",
    "**Last Updated:** 2025-01-06\n",
    "\n",
    "**License:** MIT\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the fine-tuning process of the Gemma language model for Czech language understanding and generation. We focus on creating a robust multilingual model capable of handling various Czech-specific NLP tasks.\n",
    "\n",
    "### Key Objectives\n",
    "\n",
    "1. üéØ **Primary Goal**: Adapt Gemma for superior Czech language processing\n",
    "2. üîÑ **Tasks**: Translation, sentiment analysis, text generation\n",
    "3. üìä **Evaluation**: Comprehensive benchmarking on Czech-specific metrics\n",
    "\n",
    "### Technical Requirements\n",
    "\n",
    "```\n",
    "Python >= 3.10\n",
    "polars >= 0.20.0\n",
    "datasets >= 2.15.0\n",
    "tqdm >= 4.66.0\n",
    "```\n",
    "\n",
    "### Dataset Sources\n",
    "\n",
    "We utilize multiple high-quality Czech datasets:\n",
    "\n",
    "1. **ParaCrawl v9**\n",
    "   - Parallel corpus for EN-CS translation\n",
    "   - ~52M sentence pairs\n",
    "   - [Source](https://paracrawl.eu/v9)\n",
    "\n",
    "2. **Czech Books Descriptions**\n",
    "   - Book descriptions in Czech\n",
    "   - [Source](https://huggingface.co/datasets/vojtam/czech_books_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's install required dependencies. We use specific versions to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets>=2.15.0 polars>=0.20.0 tqdm>=4.66.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Components\n",
    "\n",
    "### 1. ParaCrawl Dataset Loader\n",
    "\n",
    "The `ParaCrawlDataLoader` class handles downloading and processing of the ParaCrawl translation dataset. Key features:\n",
    "\n",
    "- Automatic download and decompression\n",
    "- Progress tracking\n",
    "- Data cleaning and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "This section implements a robust data loader for the ParaCrawl dataset with the following features:\n",
    "\n",
    "- ‚ú® Automatic download with progress tracking\n",
    "- üîç Data validation and integrity checks\n",
    "- üìä Efficient processing using Polars\n",
    "- üíæ Caching of processed data\n",
    "\n",
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import gzip\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParaCrawl Data Loader Class\n",
    "\n",
    "The main class implementation with detailed documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaCrawlDataLoader:\n",
    "    \"\"\"Handles downloading and processing of ParaCrawl translation datasets.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_lang: str = \"en\",\n",
    "        target_lang: str = \"cs\",\n",
    "        data_dir: Optional[str] = None,\n",
    "        cache_dir: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Initialize the ParaCrawl data loader.\"\"\"\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.base_url = \"https://web-language-models.s3.amazonaws.com/paracrawl/release9\"\n",
    "\n",
    "        # Setup directories\n",
    "        self.data_dir = Path(data_dir or \"./data\")\n",
    "        self.cache_dir = Path(cache_dir or \"./cache\")\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Construct file paths\n",
    "        self.filename = f\"{source_lang}-{target_lang}.txt.gz\"\n",
    "        self.filepath = self.data_dir / self.filename\n",
    "        self.processed_path = self.cache_dir / f\"{source_lang}-{target_lang}.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Validation Methods\n",
    "\n",
    "Methods for downloading data with progress tracking and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_with_progress(self, url: str, filepath: Path) -> None:\n",
    "    \"\"\"Download a file with progress bar.\"\"\"\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url)\n",
    "        total_size = int(response.headers['Content-Length'])\n",
    "        print(f\"Total size: {total_size}\")\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"Downloading {filepath.name}\") as pbar:\n",
    "            urllib.request.urlretrieve(\n",
    "                url,\n",
    "                filepath,\n",
    "                reporthook=lambda count, block_size, total_size: pbar.update(block_size)\n",
    "            )\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error downloading file: {e}\")\n",
    "        raise\n",
    "\n",
    "def _validate_file(self, filepath: Path) -> bool:\n",
    "    \"\"\"Validate downloaded file integrity.\"\"\"\n",
    "    if not filepath.exists():\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "            for _ in range(5):\n",
    "                line = f.readline()\n",
    "                if not '\\t' in line:\n",
    "                    return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "ParaCrawlDataLoader._download_with_progress = _download_with_progress\n",
    "ParaCrawlDataLoader._validate_file = _validate_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing Methods\n",
    "\n",
    "Methods for processing and loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_raw_file(self) -> None:\n",
    "    \"\"\"Process raw gzipped file into Parquet format.\"\"\"\n",
    "    if self.processed_path.exists():\n",
    "        self.logger.info(\"Using cached processed data\")\n",
    "        return\n",
    "\n",
    "    self.logger.info(\"Processing raw data file...\")\n",
    "\n",
    "    chunk_size = 100_000\n",
    "    chunks = []\n",
    "\n",
    "    with gzip.open(self.filepath, \"rt\", encoding=\"utf-8\") as f:\n",
    "        with tqdm(desc=\"Processing chunks\") as pbar:\n",
    "            while True:\n",
    "                lines = [next(f, None) for _ in range(chunk_size)]\n",
    "                lines = [line for line in lines if line is not None]\n",
    "\n",
    "                if not lines:\n",
    "                    break\n",
    "\n",
    "                pairs = [line.strip().split(\"\\t\") for line in lines]\n",
    "                # Filter out invalid pairs\n",
    "                pairs = [p for p in pairs if len(p) == 2]\n",
    "\n",
    "                if not pairs:\n",
    "                    continue\n",
    "\n",
    "                # Pre-filter by length before creating DataFrame\n",
    "                pairs = [\n",
    "                    p\n",
    "                    for p in pairs\n",
    "                    if (0 < len(p[0]) < 1000 and 0 < len(p[1]) < 1000)\n",
    "                ]\n",
    "\n",
    "                if not pairs:\n",
    "                    continue\n",
    "\n",
    "                chunk_df = pl.DataFrame(\n",
    "                    pairs,\n",
    "                    schema=[self.source_lang, self.target_lang],\n",
    "                    orient=\"row\",  # Explicitly specify orientation\n",
    "                )\n",
    "\n",
    "                if len(chunk_df) > 0:\n",
    "                    chunks.append(chunk_df)\n",
    "                pbar.update(1)\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(\"No valid data found in the input file\")\n",
    "\n",
    "    df = pl.concat(chunks)\n",
    "    df.write_parquet(self.processed_path)\n",
    "    self.logger.info(f\"Processed data saved to {self.processed_path}\")\n",
    "\n",
    "\n",
    "ParaCrawlDataLoader._process_raw_file = _process_raw_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Interface Methods\n",
    "\n",
    "Methods for downloading and loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(self) -> None:\n",
    "    \"\"\"Download ParaCrawl dataset if not already present.\"\"\"\n",
    "    if self.filepath.exists() and self._validate_file(self.filepath):\n",
    "        self.logger.info(\"Using existing download\")\n",
    "        return\n",
    "        \n",
    "    url = f\"{self.base_url}/{self.source_lang}-{self.target_lang}/{self.filename}\"\n",
    "    self.logger.info(f\"Downloading from {url}\")\n",
    "    \n",
    "    self._download_with_progress(url, self.filepath)\n",
    "    \n",
    "    if not self._validate_file(self.filepath):\n",
    "        raise ValueError(\"Downloaded file appears to be corrupt\")\n",
    "\n",
    "def load_dataframe(self) -> pl.DataFrame:\n",
    "    \"\"\"Load the processed ParaCrawl dataset.\"\"\"\n",
    "    self.download_data()\n",
    "    self._process_raw_file()\n",
    "    \n",
    "    df = pl.read_parquet(self.processed_path)\n",
    "    self.logger.info(f\"Loaded {len(df):,} translation pairs\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_sample(self, n: int = 5) -> pl.DataFrame:\n",
    "    \"\"\"Get a sample of n translation pairs.\"\"\"\n",
    "    df = self.load_dataframe()\n",
    "    return df.sample(n)\n",
    "\n",
    "ParaCrawlDataLoader.download_data = download_data\n",
    "ParaCrawlDataLoader.load_dataframe = load_dataframe\n",
    "ParaCrawlDataLoader.get_sample = get_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Alpaca Format Converter Implementation\n",
    "\n",
    "This section implements a robust converter for transforming datasets into the Alpaca instruction format, which is optimized for fine-tuning language models. Key features:\n",
    "\n",
    "- üîÑ Flexible input handling\n",
    "- üìù Customizable instruction templates\n",
    "- üíæ Efficient JSONL output\n",
    "- ‚ú® Data validation and cleaning\n",
    "\n",
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Union\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpaca Data Converter Class\n",
    "\n",
    "Main class for converting datasets to Alpaca instruction format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlpacaConverter:\n",
    "    \"\"\"Converts datasets to Alpaca instruction format for fine-tuning.\n",
    "    \n",
    "    This class handles the conversion of various dataset formats into the\n",
    "    Alpaca instruction format, which is suitable for fine-tuning language models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        instruction_templates: Optional[Dict[str, str]] = None,\n",
    "        max_length: int = 2048,\n",
    "        min_length: int = 3\n",
    "    ):\n",
    "        \"\"\"Initialize the Alpaca converter.\n",
    "        \n",
    "        Args:\n",
    "            instruction_templates: Dictionary of task types to instruction templates\n",
    "            max_length: Maximum length of input/output text\n",
    "            min_length: Minimum length of input/output text\n",
    "        \"\"\"\n",
    "        self.instruction_templates = instruction_templates or {\n",
    "            'translation': \"P≈ôelo≈æ tento text z {source_lang} do {target_lang}\",\n",
    "            'book_description': \"Popi≈° tuto knihu\",\n",
    "        }\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation Methods\n",
    "\n",
    "Methods for validating and cleaning input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_text(self, text: str) -> bool:\n",
    "    \"\"\"Validate text length and content.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to validate\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if text is valid\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "        \n",
    "    text = text.strip()\n",
    "    length = len(text)\n",
    "    \n",
    "    return (length >= self.min_length and \n",
    "            length <= self.max_length and\n",
    "            not text.isspace())\n",
    "\n",
    "def _clean_text(self, text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    return \" \".join(text.strip().split())\n",
    "\n",
    "AlpacaConverter._validate_text = _validate_text\n",
    "AlpacaConverter._clean_text = _clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Conversion Methods\n",
    "\n",
    "Core methods for converting data to Alpaca format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_instruction(self, task_type: str, **kwargs) -> str:\n",
    "    \"\"\"Create instruction from template.\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of task (e.g., 'translation')\n",
    "        **kwargs: Format parameters for instruction template\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted instruction\n",
    "    \"\"\"\n",
    "    template = self.instruction_templates.get(task_type)\n",
    "    if not template:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "    return template.format(**kwargs)\n",
    "\n",
    "def _create_example(self,\n",
    "    instruction: str,\n",
    "    output: str,\n",
    "    input_text: Optional[str] = None\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Create a single Alpaca format example.\n",
    "    \n",
    "    Args:\n",
    "        instruction: Task instruction\n",
    "        output: Expected output text\n",
    "        input_text: Optional input text\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Alpaca format example\n",
    "    \"\"\"\n",
    "    example = {\n",
    "        \"instruction\": instruction,\n",
    "        \"output\": self._clean_text(output)\n",
    "    }\n",
    "    \n",
    "    if input_text:\n",
    "        example[\"input\"] = self._clean_text(input_text)\n",
    "        \n",
    "    return example\n",
    "\n",
    "AlpacaConverter._create_instruction = _create_instruction\n",
    "AlpacaConverter._create_example = _create_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Interface Methods\n",
    "\n",
    "Methods for converting different types of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_translations(\n",
    "    self,\n",
    "    df: pl.DataFrame,\n",
    "    source_lang: str,\n",
    "    target_lang: str, \n",
    "    output_path: Union[str, Path]\n",
    ") -> None:\n",
    "    \"\"\"Convert translation pairs to Alpaca format.\"\"\"\n",
    "    instruction = self._create_instruction(\n",
    "        'translation',\n",
    "        source_lang=source_lang,\n",
    "        target_lang=target_lang\n",
    "    )\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process in chunks for memory efficiency\n",
    "    chunk_size = 10000\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        with tqdm(total=len(df), desc=\"Processing translation rows\") as pbar:\n",
    "            for i in range(0, len(df), chunk_size):\n",
    "                # Get chunk\n",
    "                chunk = df.slice(i, chunk_size)\n",
    "                \n",
    "                # Process chunk\n",
    "                valid_rows = []\n",
    "                for row in chunk.iter_rows():\n",
    "                    source = row[chunk.get_column_index(source_lang)]\n",
    "                    target = row[chunk.get_column_index(target_lang)]\n",
    "                    \n",
    "                    if self._validate_text(source) and self._validate_text(target):\n",
    "                        example = {\n",
    "                            \"instruction\": instruction,\n",
    "                            \"input\": self._clean_text(source),\n",
    "                            \"output\": self._clean_text(target)\n",
    "                        }\n",
    "                        valid_rows.append(json.dumps(example, ensure_ascii=False))\n",
    "                \n",
    "                # Write valid rows\n",
    "                if valid_rows:\n",
    "                    f.write('\\n'.join(valid_rows) + '\\n')\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(len(chunk))\n",
    "                pbar.set_postfix({'valid_rows': len(valid_rows)})\n",
    "\n",
    "def convert_descriptions(\n",
    "    self,\n",
    "    df: pl.DataFrame,\n",
    "    title_col: str,\n",
    "    desc_col: str,\n",
    "    output_path: Union[str, Path]\n",
    ") -> None:\n",
    "    \"\"\"Convert title-description pairs to Alpaca format.\"\"\"\n",
    "    instruction = self._create_instruction('book_description')\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process in chunks for memory efficiency\n",
    "    chunk_size = 10000\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        with tqdm(total=len(df), desc=\"Processing book description rows\") as pbar:\n",
    "            for i in range(0, len(df), chunk_size):\n",
    "                # Get chunk\n",
    "                chunk = df.slice(i, chunk_size)\n",
    "                \n",
    "                # Process chunk\n",
    "                valid_rows = []\n",
    "                for row in chunk.iter_rows():\n",
    "                    title = row[chunk.get_column_index(title_col)]\n",
    "                    desc = row[chunk.get_column_index(desc_col)]\n",
    "                    \n",
    "                    if self._validate_text(title) and self._validate_text(desc):\n",
    "                        example = {\n",
    "                            \"instruction\": instruction,\n",
    "                            \"input\": self._clean_text(title),\n",
    "                            \"output\": self._clean_text(desc)\n",
    "                        }\n",
    "                        valid_rows.append(json.dumps(example, ensure_ascii=False))\n",
    "                \n",
    "                # Write valid rows\n",
    "                if valid_rows:\n",
    "                    f.write('\\n'.join(valid_rows) + '\\n')\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(len(chunk))\n",
    "                pbar.set_postfix({'valid_rows': len(valid_rows)})\n",
    "\n",
    "AlpacaConverter.convert_translations = convert_translations\n",
    "AlpacaConverter.convert_descriptions = convert_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline üîÑ\n",
    "\n",
    "This section implements the main data processing pipeline for preparing our training data. We'll walk through each step to ensure high-quality training data.\n",
    "\n",
    "### Pipeline Overview üìã\n",
    "\n",
    "1. üì• **Load ParaCrawl Corpus**\n",
    "   - Download EN-CS parallel data\n",
    "   - Clean and validate entries\n",
    "   - Remove low-quality pairs\n",
    "\n",
    "2. üìö **Process Book Descriptions**\n",
    "   - Load Czech book dataset\n",
    "   - Extract titles and descriptions\n",
    "   - Filter and clean text\n",
    "\n",
    "3. üîÑ **Format Conversion**\n",
    "   - Transform to Alpaca format\n",
    "   - Add instruction templates\n",
    "   - Validate final structure\n",
    "\n",
    "4. üíæ **Save Training Data**\n",
    "   - Export to JSONL format\n",
    "   - Create data splits\n",
    "   - Verify data integrity\n",
    "\n",
    "### Key Features ‚ú®\n",
    "\n",
    "- üßπ Robust data cleaning\n",
    "- ‚ö° Efficient Polars processing\n",
    "- üîç Quality validation steps\n",
    "- üìä Progress tracking\n",
    "- üí™ Scalable pipeline\n",
    "\n",
    "### 1. Load and Process ParaCrawl Dataset üåê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loader\n",
    "loader = ParaCrawlDataLoader(source_lang=\"en\", target_lang=\"cs\")\n",
    "\n",
    "# Load ParaCrawl EN-CS dataset\n",
    "df_paracrawl = loader.load_dataframe()\n",
    "print(f\"Loaded {len(df_paracrawl):,} translation pairs\")\n",
    "df_paracrawl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Process Book Descriptions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Czech book descriptions\n",
    "ds = load_dataset(\"vojtam/czech_books_descriptions\")\n",
    "books_df = ds['train'].to_polars()\n",
    "print(f\"Loaded {len(books_df):,} book descriptions\")\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Training Format\n",
    "\n",
    "Convert our processed datasets to the Alpaca instruction format for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_converter = AlpacaConverter()\n",
    "\n",
    "alpaca_converter.convert_translations(\n",
    "    df_paracrawl,\n",
    "    source_lang=\"en\",\n",
    "    target_lang=\"cs\",\n",
    "    output_path=\"data/translation/dataset/paracrawl.jsonl\"\n",
    ")\n",
    "\n",
    "alpaca_converter.convert_descriptions(\n",
    "    books_df,\n",
    "    title_col=\"title\",\n",
    "    desc_col=\"description\",\n",
    "    output_path=\"data/translation/dataset/czech_books.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Data Validation**\n",
    "   - Implement quality checks\n",
    "   - Remove potential noise\n",
    "\n",
    "2. **Model Fine-tuning**\n",
    "   - Configure training parameters\n",
    "   - Set up evaluation metrics\n",
    "\n",
    "3. **Evaluation**\n",
    "   - Benchmark on Czech NLP tasks\n",
    "   - Compare with baseline models\n",
    "\n",
    "## References\n",
    "\n",
    "1. ParaCrawl (2023). ParaCrawl v9.0. https://paracrawl.eu/v9\n",
    "2. Gemma (2024). Google AI. https://blog.google/technology/ai/gemma-open-models/\n",
    "3. Czech Books Descriptions Dataset. https://huggingface.co/datasets/vojtam/czech_books_descriptions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-global-competetion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
