{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Czech Language Adaptation of Gemma Language Model\n",
    "\n",
    "**Author:** Jirka Helmich\n",
    "\n",
    "**Last Updated:** 2025-01-06\n",
    "\n",
    "**License:** MIT\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the fine-tuning process of the Gemma language model for Czech language understanding and generation. We focus on creating a robust multilingual model capable of handling various Czech-specific NLP tasks.\n",
    "\n",
    "### Key Objectives\n",
    "\n",
    "1. 🎯 **Primary Goal**: Adapt Gemma for superior Czech language processing\n",
    "2. 🔄 **Tasks**: Translation, sentiment analysis, text generation\n",
    "3. 📊 **Evaluation**: Comprehensive benchmarking on Czech-specific metrics\n",
    "\n",
    "### Technical Requirements\n",
    "\n",
    "```\n",
    "Python >= 3.10\n",
    "polars >= 0.20.0\n",
    "datasets >= 2.15.0\n",
    "tqdm >= 4.66.0\n",
    "```\n",
    "\n",
    "### Dataset Sources\n",
    "\n",
    "We utilize multiple high-quality Czech datasets:\n",
    "\n",
    "1. **ParaCrawl v9**\n",
    "   - Parallel corpus for EN-CS translation\n",
    "   - ~52M sentence pairs\n",
    "   - [Source](https://paracrawl.eu/v9)\n",
    "\n",
    "2. **Czech Books Descriptions**\n",
    "   - Book descriptions in Czech\n",
    "   - [Source](https://huggingface.co/datasets/vojtam/czech_books_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's install required dependencies. We use specific versions to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets polars tqdm fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Components\n",
    "\n",
    "### 1. ParaCrawl Dataset Loader\n",
    "\n",
    "The `ParaCrawlDataLoader` class handles downloading and processing of the ParaCrawl translation dataset. Key features:\n",
    "\n",
    "- Automatic download and decompression\n",
    "- Progress tracking\n",
    "- Data cleaning and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "This section implements a robust data loader for the ParaCrawl dataset with the following features:\n",
    "\n",
    "- ✨ Automatic download with progress tracking\n",
    "- 🔍 Data validation and integrity checks\n",
    "- 📊 Efficient processing using Polars\n",
    "- 💾 Caching of processed data\n",
    "\n",
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import gzip\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParaCrawl Data Loader Class\n",
    "\n",
    "The main class implementation with detailed documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaCrawlDataLoader:\n",
    "    \"\"\"Handles downloading and processing of ParaCrawl translation datasets.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_lang: str = \"en\",\n",
    "        target_lang: str = \"cs\",\n",
    "        data_dir: Optional[str] = None,\n",
    "        cache_dir: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Initialize the ParaCrawl data loader.\"\"\"\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.base_url = \"https://web-language-models.s3.amazonaws.com/paracrawl/release9\"\n",
    "\n",
    "        # Setup directories\n",
    "        self.data_dir = Path(data_dir or \"./data\")\n",
    "        self.cache_dir = Path(cache_dir or \"./cache\")\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Construct file paths\n",
    "        self.filename = f\"{source_lang}-{target_lang}.txt.gz\"\n",
    "        self.filepath = self.data_dir / self.filename\n",
    "        self.processed_path = self.cache_dir / f\"{source_lang}-{target_lang}.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Validation Methods\n",
    "\n",
    "Methods for downloading data with progress tracking and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_with_progress(self, url: str, filepath: Path) -> None:\n",
    "    \"\"\"Download a file with progress bar.\"\"\"\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url)\n",
    "        total_size = int(response.headers['Content-Length'])\n",
    "        print(f\"Total size: {total_size}\")\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"Downloading {filepath.name}\") as pbar:\n",
    "            urllib.request.urlretrieve(\n",
    "                url,\n",
    "                filepath,\n",
    "                reporthook=lambda count, block_size, total_size: pbar.update(block_size)\n",
    "            )\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error downloading file: {e}\")\n",
    "        raise\n",
    "\n",
    "def _validate_file(self, filepath: Path) -> bool:\n",
    "    \"\"\"Validate downloaded file integrity.\"\"\"\n",
    "    if not filepath.exists():\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "            for _ in range(5):\n",
    "                line = f.readline()\n",
    "                if not '\\t' in line:\n",
    "                    return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "ParaCrawlDataLoader._download_with_progress = _download_with_progress\n",
    "ParaCrawlDataLoader._validate_file = _validate_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing Methods\n",
    "\n",
    "Methods for processing and loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_raw_file(self) -> None:\n",
    "    \"\"\"Process raw gzipped file into Parquet format.\"\"\"\n",
    "    if self.processed_path.exists():\n",
    "        self.logger.info(\"Using cached processed data\")\n",
    "        return\n",
    "\n",
    "    self.logger.info(\"Processing raw data file...\")\n",
    "\n",
    "    chunk_size = 100_000\n",
    "    chunks = []\n",
    "\n",
    "    with gzip.open(self.filepath, \"rt\", encoding=\"utf-8\") as f:\n",
    "        with tqdm(desc=\"Processing chunks\") as pbar:\n",
    "            while True:\n",
    "                lines = [next(f, None) for _ in range(chunk_size)]\n",
    "                lines = [line for line in lines if line is not None]\n",
    "\n",
    "                if not lines:\n",
    "                    break\n",
    "\n",
    "                pairs = [line.strip().split(\"\\t\") for line in lines]\n",
    "                # Filter out invalid pairs\n",
    "                pairs = [p for p in pairs if len(p) == 2]\n",
    "\n",
    "                if not pairs:\n",
    "                    continue\n",
    "\n",
    "                # Pre-filter by length before creating DataFrame\n",
    "                pairs = [\n",
    "                    p\n",
    "                    for p in pairs\n",
    "                    if (0 < len(p[0]) < 1000 and 0 < len(p[1]) < 1000)\n",
    "                ]\n",
    "\n",
    "                if not pairs:\n",
    "                    continue\n",
    "\n",
    "                chunk_df = pl.DataFrame(\n",
    "                    pairs,\n",
    "                    schema=[self.source_lang, self.target_lang],\n",
    "                    orient=\"row\",  # Explicitly specify orientation\n",
    "                )\n",
    "\n",
    "                if len(chunk_df) > 0:\n",
    "                    chunks.append(chunk_df)\n",
    "                pbar.update(1)\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(\"No valid data found in the input file\")\n",
    "\n",
    "    df = pl.concat(chunks)\n",
    "    df.write_parquet(self.processed_path)\n",
    "    self.logger.info(f\"Processed data saved to {self.processed_path}\")\n",
    "\n",
    "\n",
    "ParaCrawlDataLoader._process_raw_file = _process_raw_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Interface Methods\n",
    "\n",
    "Methods for downloading and loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(self) -> None:\n",
    "    \"\"\"Download ParaCrawl dataset if not already present.\"\"\"\n",
    "    if self.filepath.exists() and self._validate_file(self.filepath):\n",
    "        self.logger.info(\"Using existing download\")\n",
    "        return\n",
    "        \n",
    "    url = f\"{self.base_url}/{self.source_lang}-{self.target_lang}/{self.filename}\"\n",
    "    self.logger.info(f\"Downloading from {url}\")\n",
    "    \n",
    "    self._download_with_progress(url, self.filepath)\n",
    "    \n",
    "    if not self._validate_file(self.filepath):\n",
    "        raise ValueError(\"Downloaded file appears to be corrupt\")\n",
    "\n",
    "def load_dataframe(self) -> pl.DataFrame:\n",
    "    \"\"\"Load the processed ParaCrawl dataset.\"\"\"\n",
    "    self.download_data()\n",
    "    self._process_raw_file()\n",
    "    \n",
    "    df = pl.read_parquet(self.processed_path)\n",
    "    self.logger.info(f\"Loaded {len(df):,} translation pairs\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_sample(self, n: int = 5) -> pl.DataFrame:\n",
    "    \"\"\"Get a sample of n translation pairs.\"\"\"\n",
    "    df = self.load_dataframe()\n",
    "    return df.sample(n)\n",
    "\n",
    "ParaCrawlDataLoader.download_data = download_data\n",
    "ParaCrawlDataLoader.load_dataframe = load_dataframe\n",
    "ParaCrawlDataLoader.get_sample = get_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Alpaca Format Converter Implementation\n",
    "\n",
    "This section implements a robust converter for transforming datasets into the Alpaca instruction format, which is optimized for fine-tuning language models. Key features:\n",
    "\n",
    "- 🔄 Flexible input handling\n",
    "- 📝 Customizable instruction templates\n",
    "- 💾 Efficient JSONL output\n",
    "- ✨ Data validation and cleaning\n",
    "\n",
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Union\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpaca Data Converter Class\n",
    "\n",
    "Main class for converting datasets to Alpaca instruction format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlpacaConverter:\n",
    "    \"\"\"Converts datasets to Alpaca instruction format for fine-tuning.\n",
    "    \n",
    "    This class handles the conversion of various dataset formats into the\n",
    "    Alpaca instruction format, which is suitable for fine-tuning language models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        instruction_templates: Optional[Dict[str, str]] = None,\n",
    "        max_length: int = 2048,\n",
    "        min_length: int = 3\n",
    "    ):\n",
    "        \"\"\"Initialize the Alpaca converter.\n",
    "        \n",
    "        Args:\n",
    "            instruction_templates: Dictionary of task types to instruction templates\n",
    "            max_length: Maximum length of input/output text\n",
    "            min_length: Minimum length of input/output text\n",
    "        \"\"\"\n",
    "        self.iso_to_lang = {\n",
    "            'en': 'Angličtiny',\n",
    "            'cs': 'Češtiny',\n",
    "        }\n",
    "        self.instruction_templates = instruction_templates or {\n",
    "            'translation': \"Přelož tento text z {iso_to_lang[source_lang]} do {iso_to_lang[target_lang]}\",\n",
    "            'book_description': \"Popiš tuto knihu\",\n",
    "        }\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation Methods\n",
    "\n",
    "Methods for validating and cleaning input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_text(self, text: str) -> bool:\n",
    "    \"\"\"Validate text length and content.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to validate\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if text is valid\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "        \n",
    "    text = text.strip()\n",
    "    length = len(text)\n",
    "    \n",
    "    return (length >= self.min_length and \n",
    "            length <= self.max_length and\n",
    "            not text.isspace())\n",
    "\n",
    "def _clean_text(self, text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    return \" \".join(text.strip().split())\n",
    "\n",
    "AlpacaConverter._validate_text = _validate_text\n",
    "AlpacaConverter._clean_text = _clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Conversion Methods\n",
    "\n",
    "Core methods for converting data to Alpaca format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_instruction(self, task_type: str, **kwargs) -> str:\n",
    "    \"\"\"Create instruction from template.\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of task (e.g., 'translation')\n",
    "        **kwargs: Format parameters for instruction template\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted instruction\n",
    "    \"\"\"\n",
    "    template = self.instruction_templates.get(task_type)\n",
    "    if not template:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "    return template.format(**kwargs)\n",
    "\n",
    "def _create_example(self,\n",
    "    instruction: str,\n",
    "    output: str,\n",
    "    input_text: Optional[str] = None\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Create a single Alpaca format example.\n",
    "    \n",
    "    Args:\n",
    "        instruction: Task instruction\n",
    "        output: Expected output text\n",
    "        input_text: Optional input text\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Alpaca format example\n",
    "    \"\"\"\n",
    "    example = {\n",
    "        \"instruction\": instruction,\n",
    "        \"output\": self._clean_text(output)\n",
    "    }\n",
    "    \n",
    "    if input_text:\n",
    "        example[\"input\"] = self._clean_text(input_text)\n",
    "        \n",
    "    return example\n",
    "\n",
    "AlpacaConverter._create_instruction = _create_instruction\n",
    "AlpacaConverter._create_example = _create_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Interface Methods\n",
    "\n",
    "Methods for converting different types of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_translations(\n",
    "    self,\n",
    "    df: pl.DataFrame,\n",
    "    source_lang: str,\n",
    "    target_lang: str, \n",
    "    output_path: Union[str, Path]\n",
    ") -> None:\n",
    "    \"\"\"Convert translation pairs to Alpaca format and save as Parquet.\n",
    "    \n",
    "    This optimized version processes data in chunks and saves to Parquet format\n",
    "    for better performance and compression.\n",
    "    \"\"\"\n",
    "    instruction = self._create_instruction(\n",
    "        'translation',\n",
    "        source_lang=source_lang,\n",
    "        target_lang=target_lang\n",
    "    )\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Convert output path to parquet\n",
    "    output_path = output_path.with_suffix('.parquet')\n",
    "    \n",
    "    # Process in chunks for memory efficiency\n",
    "    chunk_size = 50000  # Increased chunk size for Parquet\n",
    "    \n",
    "    # Initialize empty list to store processed chunks\n",
    "    processed_chunks = []\n",
    "    \n",
    "    # Create a separate progress bar for translations\n",
    "    trans_pbar = tqdm(total=len(df), desc=\"Processing translation rows\", position=0)\n",
    "    \n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        # Get chunk\n",
    "        chunk = df.slice(i, chunk_size)\n",
    "        \n",
    "        # Process chunk using vectorized operations\n",
    "        source_texts = chunk.select(pl.col(source_lang)).to_series()\n",
    "        target_texts = chunk.select(pl.col(target_lang)).to_series()\n",
    "        \n",
    "        # Create DataFrame with processed rows\n",
    "        processed_df = pl.DataFrame({\n",
    "            \"instruction\": [instruction] * len(chunk),\n",
    "            \"input\": source_texts.map_elements(self._clean_text, return_dtype=pl.Utf8),\n",
    "            \"output\": target_texts.map_elements(self._clean_text, return_dtype=pl.Utf8)\n",
    "        })\n",
    "        \n",
    "        # Filter valid rows using vectorized operations\n",
    "        mask = (processed_df[\"input\"].map_elements(self._validate_text, return_dtype=pl.Boolean) & \n",
    "               processed_df[\"output\"].map_elements(self._validate_text, return_dtype=pl.Boolean))\n",
    "        processed_df = processed_df.filter(mask)\n",
    "        \n",
    "        # Append to list\n",
    "        processed_chunks.append(processed_df)\n",
    "        \n",
    "        # Update progress\n",
    "        trans_pbar.update(len(chunk))\n",
    "        trans_pbar.set_postfix({'valid_rows': len(processed_df)})\n",
    "    \n",
    "    trans_pbar.close()\n",
    "    \n",
    "    # Combine all chunks\n",
    "    final_df = pl.concat(processed_chunks)\n",
    "    \n",
    "    # Write to Parquet with compression\n",
    "    final_df.write_parquet(\n",
    "        output_path,\n",
    "        compression=\"zstd\",  # Use ZSTD compression for better ratio/speed balance\n",
    "        statistics=True,     # Include statistics for better query performance\n",
    "        row_group_size=100000  # Optimize row groups for typical query patterns\n",
    "    )\n",
    "\n",
    "def convert_descriptions(\n",
    "    self,\n",
    "    df: pl.DataFrame,\n",
    "    title_col: str,\n",
    "    desc_col: str,\n",
    "    output_path: Union[str, Path]\n",
    ") -> None:\n",
    "    \"\"\"Convert title-description pairs to Alpaca format and save as Parquet.\"\"\"\n",
    "    instruction = self._create_instruction('book_description')\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Convert output path to parquet\n",
    "    output_path = output_path.with_suffix('.parquet')\n",
    "    \n",
    "    # Process in chunks for memory efficiency\n",
    "    chunk_size = 50000\n",
    "    \n",
    "    # Initialize empty list to store processed chunks\n",
    "    processed_chunks = []\n",
    "    \n",
    "    # Create a separate progress bar for descriptions\n",
    "    desc_pbar = tqdm(total=len(df), desc=\"Processing book description rows\", position=1)\n",
    "    \n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        # Get chunk\n",
    "        chunk = df.slice(i, chunk_size)\n",
    "        \n",
    "        # Process chunk using vectorized operations\n",
    "        titles = chunk.select(pl.col(title_col)).to_series()\n",
    "        descriptions = chunk.select(pl.col(desc_col)).to_series()\n",
    "        \n",
    "        # Create DataFrame with processed rows\n",
    "        processed_df = pl.DataFrame({\n",
    "            \"instruction\": [instruction] * len(chunk),\n",
    "            \"input\": titles.map_elements(self._clean_text, return_dtype=pl.Utf8),\n",
    "            \"output\": descriptions.map_elements(self._clean_text, return_dtype=pl.Utf8)\n",
    "        })\n",
    "        \n",
    "        # Filter valid rows using vectorized operations\n",
    "        mask = (processed_df[\"input\"].map_elements(self._validate_text, return_dtype=pl.Boolean) & \n",
    "               processed_df[\"output\"].map_elements(self._validate_text, return_dtype=pl.Boolean))\n",
    "        processed_df = processed_df.filter(mask)\n",
    "        \n",
    "        # Append to list\n",
    "        processed_chunks.append(processed_df)\n",
    "        \n",
    "        # Update progress\n",
    "        desc_pbar.update(len(chunk))\n",
    "        desc_pbar.set_postfix({'valid_rows': len(processed_df)})\n",
    "    \n",
    "    desc_pbar.close()\n",
    "    \n",
    "    # Combine all chunks\n",
    "    final_df = pl.concat(processed_chunks)\n",
    "    \n",
    "    # Write to Parquet with compression\n",
    "    final_df.write_parquet(\n",
    "        output_path,\n",
    "        compression=\"zstd\",\n",
    "        statistics=True,\n",
    "        row_group_size=100000\n",
    "    )\n",
    "\n",
    "AlpacaConverter.convert_translations = convert_translations\n",
    "AlpacaConverter.convert_descriptions = convert_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline 🔄\n",
    "\n",
    "This section implements the main data processing pipeline for preparing our training data. We'll walk through each step to ensure high-quality training data.\n",
    "\n",
    "### Pipeline Overview 📋\n",
    "\n",
    "1. 📥 **Load ParaCrawl Corpus**\n",
    "   - Download EN-CS parallel data\n",
    "   - Clean and validate entries\n",
    "   - Remove low-quality pairs\n",
    "\n",
    "2. 📚 **Process Book Descriptions**\n",
    "   - Load Czech book dataset\n",
    "   - Extract titles and descriptions\n",
    "   - Filter and clean text\n",
    "\n",
    "3. 🔄 **Format Conversion**\n",
    "   - Transform to Alpaca format\n",
    "   - Add instruction templates\n",
    "   - Validate final structure\n",
    "\n",
    "4. 💾 **Save Training Data**\n",
    "   - Export to JSONL format\n",
    "   - Create data splits\n",
    "   - Verify data integrity\n",
    "\n",
    "### Key Features ✨\n",
    "\n",
    "- 🧹 Robust data cleaning\n",
    "- ⚡ Efficient Polars processing\n",
    "- 🔍 Quality validation steps\n",
    "- 📊 Progress tracking\n",
    "- 💪 Scalable pipeline\n",
    "\n",
    "### 1. Load and Process ParaCrawl Dataset 🌐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loader\n",
    "loader = ParaCrawlDataLoader(source_lang=\"en\", target_lang=\"cs\")\n",
    "\n",
    "# Load ParaCrawl EN-CS dataset\n",
    "df_paracrawl = loader.load_dataframe()\n",
    "print(f\"Loaded {len(df_paracrawl):,} translation pairs\")\n",
    "df_paracrawl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Process Book Descriptions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Czech book descriptions\n",
    "ds = load_dataset(\"vojtam/czech_books_descriptions\")\n",
    "books_df = ds['train'].to_polars()\n",
    "print(f\"Loaded {len(books_df):,} book descriptions\")\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Training Format\n",
    "\n",
    "Convert our processed datasets to the Alpaca instruction format for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "alpaca_converter = AlpacaConverter()\n",
    "\n",
    "if not os.path.exists(\"data/translation/dataset/paracrawl.parquet\"):\n",
    "    alpaca_converter.convert_translations(\n",
    "        df_paracrawl,\n",
    "        source_lang=\"en\",\n",
    "        target_lang=\"cs\",\n",
    "        output_path=\"data/translation/dataset/paracrawl.parquet\"\n",
    "    )\n",
    "\n",
    "if not os.path.exists(\"data/translation/dataset/czech_books.parquet\"):\n",
    "    alpaca_converter.convert_descriptions(\n",
    "        books_df,\n",
    "        title_col=\"title\",\n",
    "        desc_col=\"text\",\n",
    "        output_path=\"data/translation/dataset/czech_books.parquet\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "\n",
    "In this phase we will load the dataset and split it into train, validation and test sets.\n",
    "\n",
    "This is the end of the data processing phase and we will be proceeding to the model fine-tuning phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "translations_ds = load_dataset(\n",
    "    \"parquet\", data_files=\"data/translation/dataset/paracrawl.parquet\"\n",
    ")\n",
    "books_ds = load_dataset(\n",
    "    \"parquet\", data_files=\"data/translation/dataset/czech_books.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the dataset samples to verify that the data is in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Dataset Samples ===\")\n",
    "print(\"\\nFirst translation sample:\")\n",
    "print(translations_ds[\"train\"][0])\n",
    "print(\"\\nFirst book description sample:\")\n",
    "print(books_ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to examine the dataset statistics to see if there are any potential issues with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Dataset Sizes ===\")\n",
    "print(f\"Translation dataset size: {len(translations_ds['train'])}\")\n",
    "print(f\"Books dataset size: {len(books_ds['train'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Quality Analysis 🧹\n",
    "\n",
    "Before splitting the datasets, we'll perform thorough cleaning and quality analysis to ensure high-quality Czech language data. We'll focus on:\n",
    "\n",
    "1. **Text Quality Checks** 📊\n",
    "   - Remove non-Czech characters\n",
    "   - Fix common encoding issues\n",
    "   - Validate linguistic patterns\n",
    "\n",
    "2. **Statistical Analysis** 📈\n",
    "   - Length distributions\n",
    "   - Character frequency analysis\n",
    "   - Quality metrics visualization\n",
    "\n",
    "3. **Cleaning Pipeline** 🔄\n",
    "   - Remove invalid entries\n",
    "   - Fix common errors\n",
    "   - Normalize text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "\n",
    "class CzechTextCleaner:\n",
    "    \"\"\"Handles cleaning and validation of Czech text data with permissive validation but strict correction.\"\"\"\n",
    "\n",
    "    FASTTEXT_MODEL_URL = (\n",
    "        \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\"\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        # Core Czech characters (strict)\n",
    "        self.czech_chars = set('aábcčdďeéěfghiíjklmnňoópqrřsštťuúůvwxyýzžAÁBCČDĎEÉĚFGHIÍJKLMNŇOÓPQRŘSŠTŤUÚŮVWXYÝZŽ')\n",
    "        self.czech_punctuation = set(',.!?-–—()[]{}/\\\\\"\\'»«„\"‟\"\\'')\n",
    "        self.czech_numbers = set('0123456789')\n",
    "        self.stats = {}\n",
    "        self.corrections_made = 0\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Critical error patterns that require dropping\n",
    "        self.critical_patterns = [\n",
    "            r'[^a-zA-ZáéíóúůýěščřžďťňÁÉÍÓÚŮÝĚŠČŘŽĎŤŇ\\s,.!?0-9-–—()[]{}/\\\\\"\\'»«„\"‟\"\\']',  # Non-Czech characters\n",
    "            r'[áéíóúůýě]{3,}',  # Three or more consecutive diacritics\n",
    "            r'\\s{3,}'  # Excessive whitespace\n",
    "        ]\n",
    "\n",
    "        # Setup FastText model\n",
    "        self.model_dir = Path(\"models\")\n",
    "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.model_path = self.model_dir / \"lid.176.ftz\"\n",
    "\n",
    "        # Download and load model\n",
    "        self._setup_fasttext()\n",
    "\n",
    "    def _download_with_progress(self, url: str, filepath: Path) -> None:\n",
    "        \"\"\"Download file with progress bar.\"\"\"\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            total_size = int(response.headers[\"Content-Length\"])\n",
    "\n",
    "            with tqdm(\n",
    "                total=total_size,\n",
    "                unit=\"B\",\n",
    "                unit_scale=True,\n",
    "                desc=f\"Downloading FastText model\",\n",
    "            ) as pbar:\n",
    "                urllib.request.urlretrieve(\n",
    "                    url,\n",
    "                    filepath,\n",
    "                    reporthook=lambda count, block_size, total_size: pbar.update(\n",
    "                        block_size\n",
    "                    ),\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading FastText model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_fasttext(self) -> None:\n",
    "        \"\"\"Download and load FastText model if needed.\"\"\"\n",
    "        try:\n",
    "            import fasttext\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"FastText is not installed. Please restart the kernel and try again.\"\n",
    "            )\n",
    "\n",
    "        if not self.model_path.exists():\n",
    "            self.logger.info(\n",
    "                \"Downloading FastText language detection model...\"\n",
    "            )\n",
    "            self._download_with_progress(\n",
    "                self.FASTTEXT_MODEL_URL, self.model_path\n",
    "            )\n",
    "            self.logger.info(\"Download complete!\")\n",
    "\n",
    "        self.logger.info(\"Loading FastText model...\")\n",
    "        self.fasttext_model = fasttext.load_model(str(self.model_path))\n",
    "        self.logger.info(\"FastText model loaded successfully!\")\n",
    "\n",
    "    def is_valid_czech(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validate Czech text using language detection and basic checks.\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return False\n",
    "\n",
    "        # Basic length check\n",
    "        if len(text) < 2 or len(text) > 10000:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            # Use fasttext for language detection\n",
    "            prediction = self.fasttext_model.predict(text.replace(\"\\n\", \" \"))\n",
    "            lang = prediction[0][0].replace(\"__label__\", \"\")\n",
    "            confidence = prediction[1][0]\n",
    "\n",
    "            # Accept text if it's confidently detected as Czech\n",
    "            if lang == \"cs\" and confidence > 0.8:\n",
    "                return True\n",
    "            \n",
    "            # NOTE: We could possibly accept Slovak as it's very similar and might be mixed in and majority of Czech speakers are also fluent in Slovak\n",
    "            \n",
    "            return False\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Language detection failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Very strictly clean Czech text - drop text that can't be confidently corrected.\n",
    "        \"\"\"\n",
    "        # Check for critical issues that require dropping\n",
    "        for pattern in self.critical_patterns:\n",
    "            if re.search(pattern, text):\n",
    "                return \"\"\n",
    "\n",
    "        original = text\n",
    "        # Normalize unicode characters\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "        # Strict quote and dash normalization\n",
    "        text = text.replace('\"', '„').replace('\"', '\"')\n",
    "        text = text.replace('-', '–')\n",
    "\n",
    "        # Strict whitespace and punctuation cleanup\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[\\s,.!?]+([,.!?])', r'\\1', text)\n",
    "        text = re.sub(r'([,.!?])([^\\s])', r'\\1 \\2', text)\n",
    "\n",
    "        # Remove control characters\n",
    "        text = ''.join(char for char in text if unicodedata.category(char)[0] != 'C')\n",
    "\n",
    "        # Additional strict cleanups\n",
    "        text = re.sub(r'\\.{2,}', '...', text)  # Normalize ellipsis\n",
    "        text = re.sub(r'[\\u200b\\ufeff\\u200e\\u200f]', '', text)  # Remove zero-width chars\n",
    "\n",
    "        cleaned = text.strip()\n",
    "\n",
    "        # Drop if significant changes were needed\n",
    "        if abs(len(cleaned) - len(original)) > len(original) * 0.2:\n",
    "            return \"\"\n",
    "\n",
    "        if cleaned != original:\n",
    "            self.corrections_made += 1\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    def clean_text_batch(self, texts: List[str], pbar: tqdm) -> List[str]:\n",
    "        \"\"\"Process a batch of texts with progress tracking.\"\"\"\n",
    "        cleaned = []\n",
    "        for text in texts:\n",
    "            if self.is_valid_czech(text):  # Permissive validation\n",
    "                cleaned_text = self.clean_text(text)  # Strict cleaning\n",
    "                if cleaned_text:  # Only keep if cleaning succeeded\n",
    "                    cleaned.append(cleaned_text)\n",
    "            pbar.update(1)\n",
    "        return cleaned\n",
    "\n",
    "    def analyze_dataset(self, dataset, field: str) -> Dict:\n",
    "        \"\"\"Perform detailed linguistic analysis of dataset.\"\"\"\n",
    "        lengths = [len(item[field]) for item in dataset]\n",
    "        valid_count = sum(1 for item in dataset if self.is_valid_czech(item[field]))\n",
    "        word_counts = [len(item[field].split()) for item in dataset]\n",
    "\n",
    "        return {\n",
    "            'total': len(dataset),\n",
    "            'valid': valid_count,\n",
    "            'invalid': len(dataset) - valid_count,\n",
    "            'corrected': self.corrections_made,\n",
    "            'avg_length': sum(lengths) / len(lengths),\n",
    "            'max_length': max(lengths),\n",
    "            'min_length': min(lengths),\n",
    "            'avg_words': sum(word_counts) / len(word_counts),\n",
    "            'max_words': max(word_counts),\n",
    "            'min_words': min(word_counts)\n",
    "        }\n",
    "\n",
    "    def plot_statistics(self, stats: Dict, title: str) -> None:\n",
    "        \"\"\"Visualize dataset quality metrics.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # First subplot - Valid vs Invalid\n",
    "        ax1.bar(['Valid Czech', 'Invalid/Non-Czech'], \n",
    "                [stats['valid'], stats['invalid']], \n",
    "                color=['green', 'red'])\n",
    "        ax1.set_title(f'{title} - Czech Language Quality Analysis')\n",
    "        ax1.set_ylabel('Number of Entries')\n",
    "\n",
    "        total = stats['total']\n",
    "        for i, v in enumerate([stats['valid'], stats['invalid']]):\n",
    "            ax1.text(i, v, f'{v:,}\\n({v/total*100:.1f}%)', \n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "        # Second subplot - Corrections Made\n",
    "        ax2.bar(['Original', 'Corrected'], \n",
    "                [stats['total'] - stats['corrected'], stats['corrected']], \n",
    "                color=['blue', 'orange'])\n",
    "        ax2.set_title(f'{title} - Text Corrections')\n",
    "        ax2.set_ylabel('Number of Entries')\n",
    "\n",
    "        for i, v in enumerate([stats['total'] - stats['corrected'], stats['corrected']]):\n",
    "            ax2.text(i, v, f'{v:,}\\n({v/total*100:.1f}%)', \n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Initialize cleaner\n",
    "cleaner = CzechTextCleaner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Analyze the Translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Analyze Books Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze original books dataset\n",
    "books_stats_before = cleaner.analyze_dataset(books_ds['train'], 'output')\n",
    "print(\"\\n=== Books Dataset Statistics (Before Cleaning) ===\")\n",
    "print(f\"Total entries: {books_stats_before['total']:,}\")\n",
    "print(f\"Valid entries: {books_stats_before['valid']:,}\")\n",
    "print(f\"Average length: {books_stats_before['avg_length']:.1f}\")\n",
    "\n",
    "# Clean books dataset\n",
    "def clean_books(example):\n",
    "    example['output'] = cleaner.clean_text(example['output'])\n",
    "    return example\n",
    "\n",
    "books_ds['train'] = books_ds['train'].filter(\n",
    "    lambda x: cleaner.is_valid_czech(x['output'])\n",
    ").map(clean_books)\n",
    "\n",
    "# Analyze cleaned dataset\n",
    "books_stats_after = cleaner.analyze_dataset(books_ds['train'], 'output')\n",
    "cleaner.plot_statistics(books_stats_after, 'Books Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot length distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Translation dataset lengths\n",
    "trans_lengths = [len(item['output']) for item in translations_ds['train']]\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(trans_lengths, bins=50)\n",
    "plt.title('Translation Text Lengths')\n",
    "plt.xlabel('Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Books dataset lengths\n",
    "book_lengths = [len(item['output']) for item in books_ds['train']]\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(book_lengths, bins=50)\n",
    "plt.title('Book Description Lengths')\n",
    "plt.xlabel('Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the datasets\n",
    "\n",
    "Now that the data is cleaned and ready, we can split the datasets into train, validation and test sets.\n",
    "\n",
    "This is the end of the data processing phase and we will be proceeding to the model fine-tuning phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSplitting datasets...\")\n",
    "\n",
    "# Split translations dataset\n",
    "translations_ds = translations_ds[\"train\"].train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42\n",
    ")\n",
    "translations_test_val = translations_ds[\"test\"].train_test_split(\n",
    "    test_size=0.5, shuffle=True, seed=42\n",
    ")\n",
    "\n",
    "translations_ds = {\n",
    "    \"train\": translations_ds[\"train\"],\n",
    "    \"validation\": translations_test_val[\"train\"],\n",
    "    \"test\": translations_test_val[\"test\"],\n",
    "}\n",
    "\n",
    "# Split books dataset\n",
    "books_ds = books_ds[\"train\"].train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42\n",
    ")\n",
    "books_test_val = books_ds[\"test\"].train_test_split(\n",
    "    test_size=0.5, shuffle=True, seed=42\n",
    ")\n",
    "\n",
    "books_ds = {\n",
    "    \"train\": books_ds[\"train\"],\n",
    "    \"validation\": books_test_val[\"train\"],\n",
    "    \"test\": books_test_val[\"test\"],\n",
    "}\n",
    "\n",
    "# 5. Print split sizes\n",
    "print(\"\\n=== Split Sizes ===\")\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"\\nTranslations {split}:\")\n",
    "    print(f\"Size: {len(translations_ds[split])}\")\n",
    "    print(f\"\\nBooks {split}:\")\n",
    "    print(f\"Size: {len(books_ds[split])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the data is now prepared and we need to combine the datasets into a single dataset later used for model fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "combined_datasets = {\n",
    "    split: concatenate_datasets([translations_ds[split], books_ds[split]])\n",
    "    for split in [\"train\", \"validation\", \"test\"]\n",
    "}\n",
    "\n",
    "print(\"\\n=== Combined Dataset Sizes ===\")\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"\\n{split}:\")\n",
    "    print(f\"Size: {len(combined_datasets[split])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Model Fine-Tuning\n",
    "\n",
    "In this phase, we will fine-tune the model on the combined dataset using **PyTorch** and the **Hugging Face Transformers** library. The `gemma-2-2b-it` model will serve as the base model. \n",
    "\n",
    "The datasets are already split into train/validation/test sets (80/10/10 ratio) for proper evaluation. Let's break down the process into key phases:\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Model Architecture\n",
    "\n",
    "We will use **Google's Gemma 2B** as the base model, fine-tuned with **PyTorch Lightning** for efficient training. The architecture includes:\n",
    "\n",
    "- 📂 **Custom Dataset Class** for handling the specific data format\n",
    "- 📊 **Lightning DataModule** for data management\n",
    "- 🛠️ **Lightning Module** for training logic\n",
    "- ⚡ **Mixed Precision Training** (bfloat16) for improved performance\n",
    "- 🧹 **Gradient Accumulation and Clipping** for stability during training\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Training Process\n",
    "\n",
    "The training will proceed through several stages:\n",
    "\n",
    "1. **Data Batching and Tokenization** 🗃️:\n",
    "   - Efficiently preprocess and batch the input data.\n",
    "   \n",
    "2. **Forward Pass** 🔄:\n",
    "   - Pass the tokenized data through the `gemma-2b` model.\n",
    "   \n",
    "3. **Loss Calculation** 🎯:\n",
    "   - Use **Cross Entropy** as the loss function.\n",
    "\n",
    "4. **Backpropagation** 🔙:\n",
    "   - Perform backpropagation with **Gradient Accumulation** to stabilize updates.\n",
    "\n",
    "5. **Optimization** 🛠️:\n",
    "   - Use **AdamW** optimizer with **Cosine Learning Rate Scheduling** for smooth convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Evaluation\n",
    "\n",
    "We will evaluate the model's performance using:\n",
    "\n",
    "- 📉 **Validation Loss** during training to track progress\n",
    "- ✅ **Test Set Performance** to assess generalization\n",
    "- 🔍 **Practical Examples** from both tasks to verify real-world applicability\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Monitoring\n",
    "\n",
    "Training progress will be tracked using **Weights & Biases (W&B)** 📈. Model checkpoints will be saved based on **validation loss improvements** 💾.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Next Steps\n",
    "\n",
    "After successful training, the following steps will be taken:\n",
    "\n",
    "1. **Evaluate Model Performance** 🏆:\n",
    "   - Assess results on both translation and book description tasks.\n",
    "   \n",
    "2. **Fine-Tune Hyperparameters** 🎛️:\n",
    "   - Adjust as necessary to optimize performance.\n",
    "   \n",
    "3. **Test Real-World Examples** 🌍:\n",
    "   - Validate the model with practical scenarios.\n",
    "   \n",
    "4. **Deploy the Model** 🚀:\n",
    "   - Make the model available for use.\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 Documentation\n",
    "\n",
    "Each phase will be thoroughly documented with:\n",
    "\n",
    "- 📊 **Results**\n",
    "- 📝 **Observations**\n",
    "- 💡 **Insights for Improvement**\n",
    "\n",
    "This ensures progress is clearly tracked and potential areas for enhancement are identified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Evaluation**\n",
    "   - Benchmark on Czech NLP tasks\n",
    "   - Compare with baseline models\n",
    "\n",
    "## References\n",
    "\n",
    "1. ParaCrawl (2023). ParaCrawl v9.0. https://paracrawl.eu/v9\n",
    "2. Gemma (2024). Google AI. https://blog.google/technology/ai/gemma-open-models/\n",
    "3. Czech Books Descriptions Dataset. https://huggingface.co/datasets/vojtam/czech_books_descriptions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-global-competetion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
