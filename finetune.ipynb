{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Gemma Czech Adaptation\n",
    "\n",
    "### Overview\n",
    "This notebook is part of the **Gemma Czech Adaptation** project, which aims to fine-tune the Gemma language model for enhanced understanding and generation of the Czech language. The project is independent of the original dataset from the Gemma competition and instead compiles custom datasets from the Hugging Face datasets hub, tailored to the nuances of Czech text.\n",
    "\n",
    "### Aims\n",
    "- **Fine-tuning:** Optimize the Gemma model for Czech text by using diverse datasets that include conversational language, formal writing, and domain-specific terminology.\n",
    "- **Task Specialization:** Enhance performance in specific tasks such as:\n",
    "  - Translation from and to Czech.\n",
    "  - Sentiment analysis on Czech text.\n",
    "  - Generative language tasks specific to the Czech context.\n",
    "- **Adaptability:** Make the model robust for varied Czech applications, such as chatbots, content summarization, and text classification.\n",
    "\n",
    "### Goals\n",
    "1. Compile and preprocess high-quality Czech text datasets from Hugging Face.\n",
    "2. Fine-tune the Gemma model using the compiled data to improve language-specific accuracy.\n",
    "3. Evaluate the fine-tuned model on a variety of tasks and benchmarks to measure its performance.\n",
    "4. Provide actionable insights for further improvements and adaptations.\n",
    "\n",
    "### Workflow\n",
    "1. **Dataset Compilation:** Identify and gather Czech-language datasets from Hugging Face.\n",
    "2. **Data Preprocessing:** Clean and tokenize the text data, preparing it for model fine-tuning.\n",
    "3. **Model Training:** Fine-tune the Gemma model using preprocessed datasets and suitable hyperparameters.\n",
    "4. **Evaluation:** Use a robust evaluation framework to measure the model's performance across multiple tasks.\n",
    "5. **Deployment:** Save the fine-tuned model in a deployable format for integration into Czech-specific applications.\n",
    "\n",
    "This notebook serves as a comprehensive guide to achieving these aims and goals while ensuring transparency and reproducibility of the fine-tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "Brief introduction to the dataset preparation.\n",
    "Break down of dataset preparation into smaller steps combining multiple datasets.\n",
    "\n",
    "Dataset preparation is a crucial part of the fine-tuning process. We will use the following datasets:\n",
    "\n",
    "- Europarl: https://www.statmt.org/europarl/v10/training/\n",
    "https://www.statmt.org/europarl/v10/training/europarl-v10.cs-en.tsv.gz\n",
    "- Paracrawl: https://web-language-models.s3.amazonaws.com/paracrawl/release9/en-cs/en-cs.txt.gz\n",
    "- Czech News Simple: https://huggingface.co/datasets/CIIRC-NLP/czech_news_simple-cs\\n\n",
    "- Czech News Simple is a dataset of Czech news articles, which is a good starting point for our dataset preparation.\n",
    "\n",
    "We will also use the following datasets:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "%pip install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we preprocess our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def count_quotes(column):\n",
    "    return pl.col(column).str.count_matches('\"')\n",
    "\n",
    "europarl_filepath = 'data/translation/europarl.tsv'\n",
    "\n",
    "df_europarl = pl.read_csv(europarl_filepath, separator='\\t', quote_char=None, ignore_errors=True, truncate_ragged_lines=True)\n",
    "\n",
    "string_columns = [col for col in df_europarl.columns if df_europarl[col].dtype == pl.Utf8] \n",
    "\n",
    "mask = reduce(operator.or_, [(count_quotes(col) % 2 != 0) for col in string_columns])\n",
    "df_europarl = df_europarl.filter(~mask)\n",
    "df_europarl = df_europarl.rename({\n",
    "    df_europarl.columns[0]: \"lhs\",\n",
    "    df_europarl.columns[1]: \"rhs\",\n",
    "})\n",
    "\n",
    "df_europarl.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def count_quotes(column):\n",
    "    return pl.col(column).str.count_matches('\"')\n",
    "\n",
    "paracrawl_filepath = 'data/translation/paracrawl.txt'\n",
    "\n",
    "df_paracrawl = pl.read_csv(paracrawl_filepath, separator='\\t', quote_char=None, ignore_errors=True, truncate_ragged_lines=True)\n",
    "\n",
    "string_columns = [col for col in df_paracrawl.columns if df_paracrawl[col].dtype == pl.Utf8] \n",
    "\n",
    "mask = reduce(operator.or_, [(count_quotes(col) % 2 != 0) for col in string_columns])\n",
    "df_paracrawl = df_paracrawl.filter(~mask)\n",
    "df_paracrawl = df_paracrawl.rename({\n",
    "    df_paracrawl.columns[0]: \"lhs\",\n",
    "    df_paracrawl.columns[1]: \"rhs\",\n",
    "})\n",
    "\n",
    "df_paracrawl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function that converts our datasets to alpaca format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "def convert_to_alpaca(df: pl.DataFrame, instruction: str, output_col: str, input_col: Optional[str] = None, output_path: str = \"dataset.jsonl\"):\n",
    "    records: List[Dict] = []\n",
    "\n",
    "    for row in df.iter_rows(named=True):\n",
    "        record = {\n",
    "            \"instruction\": instruction\n",
    "        }\n",
    "\n",
    "        if input_col and row[input_col]:\n",
    "            record[\"input\"] = row[input_col]\n",
    "\n",
    "        record[\"output\"] = row[output_col]\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Saved to path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally convert our datasets to alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europarl_jsonl_outputpath = 'data/translation/dataset/europarl.jsonl'\n",
    "\n",
    "convert_to_alpaca(df_europarl, instruction=\"Přelož tento text z angličtiny do češtiny\", input_col=\"rhs\", output_col=\"lhs\", output_path=europarl_jsonl_outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paracrawl_jsonl_outputpath = 'data/translation/dataset/paracrawl.jsonl'\n",
    "\n",
    "convert_to_alpaca(df_paracrawl, instruction=\"Přelož tento text z angličtiny do češtiny\", input_col=\"lhs\", output_col=\"rhs\", output_path=paracrawl_jsonl_outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to path: data/translation/dataset/books.jsonl\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('vojtam/czech_books_descriptions', split='train')\n",
    "\n",
    "dataset = dataset.rename_columns({\n",
    "  \"title\": \"lhs\",\n",
    "  \"text\": \"rhs\"\n",
    "})\n",
    "\n",
    "polars_dataset = dataset.to_polars()\n",
    "\n",
    "books_jsonl_outputpath = 'data/translation/dataset/books.jsonl'\n",
    "\n",
    "convert_to_alpaca(polars_ataset, instruction=\"Řekni mi něco o knize\", input_col=\"lhs\", output_col=\"rhs\", output_path=books_jsonl_outputpath)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-global-competetion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
