{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåê Czech Language Adaptation of Gemma Language Model\n",
                "\n",
                "**Author:** Jirka Helmich  \n",
                "**Last Updated:** 2024-01-06  \n",
                "**License:** MIT\n",
                "\n",
                "## üìã Overview\n",
                "\n",
                "This notebook demonstrates the fine-tuning process of the Gemma language model for Czech language understanding and generation. We focus on creating a robust multilingual model capable of handling various Czech-specific NLP tasks.\n",
                "\n",
                "### üéØ Key Objectives\n",
                "\n",
                "1. Adapt Gemma for superior Czech language processing\n",
                "2. Support translation and text generation tasks\n",
                "3. Comprehensive benchmarking on Czech-specific metrics\n",
                "\n",
                "### üìä Data Sources\n",
                "\n",
                "1. **ParaCrawl v9**\n",
                "   - EN-CS parallel corpus (~52M pairs)\n",
                "   - [Source](https://paracrawl.eu/v9)\n",
                "\n",
                "2. **Czech Books Descriptions**\n",
                "   - Book descriptions in Czech\n",
                "   - [Source](https://huggingface.co/datasets/vojtam/czech_books_descriptions)\n",
                "\n",
                "### üõ†Ô∏è Technical Requirements\n",
                "\n",
                "```python\n",
                "Python >= 3.10\n",
                "polars >= 0.20.0\n",
                "datasets >= 2.15.0\n",
                "tqdm >= 4.66.0\n",
                "fasttext >= 0.9.2\n",
                "torch >= 2.0.0\n",
                "transformers >= 4.36.0\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Environment Setup\n",
                "\n",
                "First, let's set up our environment with all required dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install core dependencies\n",
                "%pip install -q datasets polars tqdm fasttext torch transformers>=4.47.1 wandb seaborn matplotlib numpy peft>=0.14.0 evaluate huggingface_hub bitsandbytes>=0.45.0\n",
                "\n",
                "# Import common libraries\n",
                "import polars as pl\n",
                "from pathlib import Path\n",
                "import logging\n",
                "from tqdm.auto import tqdm\n",
                "from typing import Optional, Dict, List, Union\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Configure plotting\n",
                "plt.style.use('seaborn-v0_8-paper')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Data Processing Pipeline\n",
                "\n",
                "Our data processing pipeline is optimized for handling large-scale text data efficiently:\n",
                "\n",
                "1. üì• **Data Loading**: Streaming large files with chunked processing\n",
                "2. üßπ **Text Cleaning**: Efficient Czech text validation and normalization\n",
                "3. üîÑ **Format Conversion**: Optimized Alpaca format transformation\n",
                "4. üíæ **Storage**: Compressed Parquet format with optimal chunk sizes\n",
                "\n",
                "### 2.1 Core Data Processing Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gzip\n",
                "\n",
                "class ParaCrawlDataLoader:\n",
                "    \"\"\"Optimized loader for ParaCrawl dataset with chunked processing.\"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        source_lang: str = \"en\",\n",
                "        target_lang: str = \"cs\",\n",
                "        chunk_size: int = 500_000,  # Increased for better throughput\n",
                "        data_dir: Optional[str] = None,\n",
                "        cache_dir: Optional[str] = None\n",
                "    ):\n",
                "        self.source_lang = source_lang\n",
                "        self.target_lang = target_lang\n",
                "        self.chunk_size = chunk_size\n",
                "        self.base_url = \"https://web-language-models.s3.amazonaws.com/paracrawl/release9\"\n",
                "        \n",
                "        # Setup directories\n",
                "        self.data_dir = Path(data_dir or \"./data\")\n",
                "        self.cache_dir = Path(cache_dir or \"./cache\")\n",
                "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
                "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        # File paths\n",
                "        self.filename = f\"{source_lang}-{target_lang}.txt.gz\"\n",
                "        self.filepath = self.data_dir / self.filename\n",
                "        self.processed_path = self.cache_dir / f\"{source_lang}-{target_lang}.parquet\"\n",
                "        \n",
                "        self.logger = logging.getLogger(__name__)\n",
                "    \n",
                "    def _validate_file(self, filepath: Path) -> bool:\n",
                "        \"\"\"Validate downloaded file integrity.\"\"\"\n",
                "        if not filepath.exists():\n",
                "            return False\n",
                "            \n",
                "        try:\n",
                "            with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
                "                # Try to read first few lines\n",
                "                for _ in range(5):\n",
                "                    line = f.readline()\n",
                "                    if not line or '\\t' not in line:\n",
                "                        return False\n",
                "            return True\n",
                "        except Exception:\n",
                "            return False\n",
                "    \n",
                "    def download_data(self) -> None:\n",
                "        \"\"\"Download dataset with progress tracking.\"\"\"\n",
                "        if self.filepath.exists() and self._validate_file(self.filepath):\n",
                "            self.logger.info(\"Using existing valid download\")\n",
                "            return\n",
                "            \n",
                "        url = f\"{self.base_url}/{self.source_lang}-{self.target_lang}/{self.filename}\"\n",
                "        self.logger.info(f\"Downloading from {url}\")\n",
                "        \n",
                "        try:\n",
                "            import urllib.request\n",
                "            response = urllib.request.urlopen(url)\n",
                "            total_size = int(response.headers['Content-Length'])\n",
                "            \n",
                "            with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
                "                urllib.request.urlretrieve(\n",
                "                    url,\n",
                "                    self.filepath,\n",
                "                    reporthook=lambda count, block_size, _: pbar.update(block_size)\n",
                "                )\n",
                "                \n",
                "            if not self._validate_file(self.filepath):\n",
                "                raise ValueError(\"Downloaded file appears to be corrupt\")\n",
                "                \n",
                "        except Exception as e:\n",
                "            self.logger.error(f\"Download failed: {e}\")\n",
                "            if self.filepath.exists():\n",
                "                self.filepath.unlink()\n",
                "            raise\n",
                "    \n",
                "    def process_chunk(self, chunk: List[str]) -> pl.DataFrame:\n",
                "        \"\"\"Process a chunk of text data efficiently.\"\"\"\n",
                "        if not chunk:\n",
                "            return pl.DataFrame()\n",
                "            \n",
                "        # Split and filter in one pass\n",
                "        pairs = [\n",
                "            line.strip().split(\"\\t\") \n",
                "            for line in chunk \n",
                "            if \"\\t\" in line\n",
                "        ]\n",
                "        \n",
                "        # Filter invalid pairs\n",
                "        valid_pairs = [\n",
                "            p for p in pairs \n",
                "            if len(p) == 2 and all(0 < len(text) < 1000 for text in p)\n",
                "        ]\n",
                "        \n",
                "        if not valid_pairs:\n",
                "            return pl.DataFrame()\n",
                "        \n",
                "        # Create DataFrame efficiently\n",
                "        return pl.DataFrame(\n",
                "            valid_pairs,\n",
                "            schema=[self.source_lang, self.target_lang],\n",
                "            orient=\"row\"\n",
                "        )\n",
                "    \n",
                "    def load_dataframe(self) -> pl.DataFrame:\n",
                "        \"\"\"Load and process data in memory-efficient chunks.\"\"\"\n",
                "        if self.processed_path.exists():\n",
                "            self.logger.info(f\"Loading cached processed data from {self.processed_path}\")\n",
                "            return pl.read_parquet(self.processed_path)\n",
                "        \n",
                "        self.download_data()\n",
                "        chunks = []\n",
                "        total_rows = 0\n",
                "        \n",
                "        self.logger.info(\"Processing raw data file...\")\n",
                "        with gzip.open(self.filepath, \"rt\", encoding=\"utf-8\") as f:\n",
                "            with tqdm(desc=\"Processing chunks\") as pbar:\n",
                "                while True:\n",
                "                    chunk = []\n",
                "                    for _ in range(self.chunk_size):\n",
                "                        line = f.readline()\n",
                "                        if not line:\n",
                "                            break\n",
                "                        chunk.append(line)\n",
                "                    \n",
                "                    if not chunk:\n",
                "                        break\n",
                "                        \n",
                "                    df_chunk = self.process_chunk(chunk)\n",
                "                    if not df_chunk.is_empty():\n",
                "                        chunks.append(df_chunk)\n",
                "                        total_rows += len(df_chunk)\n",
                "                    \n",
                "                    pbar.update(len(chunk))\n",
                "                    pbar.set_postfix({\"valid_rows\": total_rows})\n",
                "        \n",
                "        # Combine chunks and save\n",
                "        self.logger.info(f\"Combining {len(chunks)} chunks with {total_rows:,} total rows\")\n",
                "        df = pl.concat(chunks)\n",
                "        \n",
                "        self.logger.info(f\"Saving processed data to {self.processed_path}\")\n",
                "        df.write_parquet(\n",
                "            self.processed_path,\n",
                "            compression=\"zstd\",\n",
                "            compression_level=3\n",
                "        )\n",
                "        \n",
                "        return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from concurrent.futures import ThreadPoolExecutor\n",
                "import unicodedata\n",
                "from typing import Optional, List\n",
                "import polars as pl\n",
                "from pathlib import Path\n",
                "import logging\n",
                "from tqdm.notebook import tqdm\n",
                "import fasttext\n",
                "from dataclasses import dataclass\n",
                "from enum import Enum\n",
                "\n",
                "\n",
                "class TextIssue(Enum):\n",
                "    INVALID_CHARS = \"invalid_characters\"\n",
                "    NON_CZECH = \"non_czech_language\"\n",
                "    LOW_CONFIDENCE = \"low_language_confidence\"\n",
                "    TOO_SHORT = \"too_short\"\n",
                "    NO_ISSUES = \"no_issues\"\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class TextQuality:\n",
                "    original: str\n",
                "    cleaned: Optional[str]\n",
                "    issues: List[TextIssue]\n",
                "    confidence: float\n",
                "\n",
                "    @property\n",
                "    def is_valid(self) -> bool:\n",
                "        return TextIssue.NO_ISSUES in self.issues\n",
                "\n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "import unicodedata\n",
                "from typing import Optional, List\n",
                "import polars as pl\n",
                "from pathlib import Path\n",
                "import logging\n",
                "from tqdm.notebook import tqdm\n",
                "import fasttext\n",
                "\n",
                "\n",
                "class CzechTextCleaner:\n",
                "    \"\"\"Efficient Czech text validation and cleaning for large datasets.\"\"\"\n",
                "\n",
                "    FASTTEXT_MODEL_URL = (\n",
                "        \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\"\n",
                "    )\n",
                "\n",
                "    def __init__(self, model_dir: Optional[str] = None):\n",
                "        # Character sets for validation\n",
                "        self.czech_chars = frozenset(\n",
                "            \"a√°bcƒçdƒèe√©ƒõfghi√≠jklmn≈ào√≥pqr≈ôs≈°t≈•u√∫≈Øvwxy√Ωz≈æA√ÅBCƒåDƒéE√âƒöFGHI√çJKLMN≈áO√ìPQR≈òS≈†T≈§U√ö≈ÆVWXY√ùZ≈Ω\"\n",
                "        )\n",
                "        self.czech_punctuation = frozenset(',.!?-‚Äì‚Äî()[]{}/\\\\\"\\'¬ª¬´‚Äû\"‚Äü\"\\'')\n",
                "        self.czech_numbers = frozenset(\"0123456789\")\n",
                "        self.valid_chars = (\n",
                "            self.czech_chars\n",
                "            | self.czech_punctuation\n",
                "            | self.czech_numbers\n",
                "            | {\" \"}\n",
                "        )\n",
                "\n",
                "        # Setup model and logging\n",
                "        self.model_dir = Path(model_dir or \"models\")\n",
                "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
                "        self.model_path = self.model_dir / \"lid.176.ftz\"\n",
                "        self.logger = logging.getLogger(__name__)\n",
                "\n",
                "        # Load FastText model\n",
                "        self._setup_fasttext()\n",
                "\n",
                "    def _setup_fasttext(self) -> None:\n",
                "        \"\"\"Initialize FastText model.\"\"\"\n",
                "        if not self.model_path.exists():\n",
                "            self._download_model()\n",
                "        self.model = fasttext.load_model(str(self.model_path))\n",
                "\n",
                "    def _download_model(self) -> None:\n",
                "        \"\"\"Download language model with progress tracking.\"\"\"\n",
                "        import urllib.request\n",
                "\n",
                "        response = urllib.request.urlopen(self.FASTTEXT_MODEL_URL)\n",
                "        total_size = int(response.headers[\"Content-Length\"])\n",
                "\n",
                "        with tqdm(\n",
                "            total=total_size,\n",
                "            unit=\"B\",\n",
                "            unit_scale=True,\n",
                "            desc=\"Downloading model\",\n",
                "        ) as pbar:\n",
                "            urllib.request.urlretrieve(\n",
                "                self.FASTTEXT_MODEL_URL,\n",
                "                self.model_path,\n",
                "                lambda count, block_size, _: pbar.update(block_size),\n",
                "            )\n",
                "\n",
                "    def _is_valid_czech(self, text: str) -> bool:\n",
                "        \"\"\"Check if text is valid Czech with good confidence.\"\"\"\n",
                "        if not isinstance(text, str) or len(text.strip()) < 2:\n",
                "            return False\n",
                "\n",
                "        # Check characters\n",
                "        if not all(c in self.valid_chars or c.isspace() for c in text):\n",
                "            return False\n",
                "\n",
                "        # Detect language\n",
                "        text = \" \".join(text.split())\n",
                "        pred = self.model.predict(text)\n",
                "        lang, conf = pred[0][0].replace(\"__label__\", \"\"), pred[1][0]\n",
                "\n",
                "        return lang == \"cs\" and conf >= 0.8\n",
                "\n",
                "    def _clean_text(self, text: str) -> Optional[str]:\n",
                "        \"\"\"Clean text if valid, return None if invalid.\"\"\"\n",
                "        if not self._is_valid_czech(text):\n",
                "            return None\n",
                "\n",
                "        # Unicode normalization\n",
                "        text = unicodedata.normalize(\"NFKC\", text)\n",
                "\n",
                "        # Quote and dash normalization\n",
                "        text = text.replace('\"', \"‚Äû\").replace('\"', '\"').replace(\"-\", \"‚Äì\")\n",
                "\n",
                "        # Initial whitespace normalization\n",
                "        text = \" \".join(text.split())\n",
                "\n",
                "        # Thorough punctuation cleanup\n",
                "        for punct in \",.!?\":\n",
                "            # Remove all spaces before punctuation\n",
                "            text = text.replace(f\" {punct}\", punct)\n",
                "            # Replace any runs of spaces after punctuation with a single space\n",
                "            # First add space if missing\n",
                "            text = text.replace(f\"{punct}\", f\"{punct} \")\n",
                "            # Then collapse multiple spaces\n",
                "            while f\"{punct}  \" in text:\n",
                "                text = text.replace(f\"{punct}  \", f\"{punct} \")\n",
                "\n",
                "        # Final whitespace cleanup\n",
                "        text = \" \".join(text.split())\n",
                "        return text.strip()\n",
                "\n",
                "    def clean_dataframe(\n",
                "        self, df: pl.DataFrame, text_columns: List[str], num_threads: int = 8\n",
                "    ) -> pl.DataFrame:\n",
                "        \"\"\"Clean text columns and drop rows with invalid texts.\"\"\"\n",
                "        pl.Config.set_streaming_chunk_size(10000)\n",
                "\n",
                "        for col in text_columns:\n",
                "            self.logger.info(f\"Processing column: {col}\")\n",
                "\n",
                "            # Process texts in parallel\n",
                "            texts = df[col].to_list()\n",
                "            with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
                "                with tqdm(total=len(texts), desc=f\"Cleaning {col}\") as pbar:\n",
                "                    futures = []\n",
                "                    for text in texts:\n",
                "                        future = executor.submit(self._clean_text, text)\n",
                "                        future.add_done_callback(lambda p: pbar.update(1))\n",
                "                        futures.append(future)\n",
                "\n",
                "                    cleaned_texts = [future.result() for future in futures]\n",
                "\n",
                "            # Update column with cleaned texts\n",
                "            df = df.with_columns([pl.Series(col, cleaned_texts)])\n",
                "\n",
                "            # Drop rows where cleaning failed (null values)\n",
                "            initial_rows = len(df)\n",
                "            df = df.filter(~pl.col(col).is_null())\n",
                "            kept_rows = len(df)\n",
                "\n",
                "            self.logger.info(\n",
                "                f\"Kept {kept_rows:,} valid rows out of {initial_rows:,} \"\n",
                "                f\"({kept_rows/initial_rows:.1%})\"\n",
                "            )\n",
                "\n",
                "        return df\n",
                "\n",
                "    def analyze_parallel_stats(self, df: pl.DataFrame) -> dict:\n",
                "        \"\"\"Analyze parallel corpus statistics\"\"\"\n",
                "        return {\n",
                "            \"total_pairs\": len(df),\n",
                "            \"unique_cs\": df[\"cs\"].n_unique(),\n",
                "            \"unique_en\": df[\"en\"].n_unique(),\n",
                "            \"avg_cs_len\": df[\"cs\"].str.len_chars().mean(),\n",
                "            \"avg_en_len\": df[\"en\"].str.len_chars().mean(),\n",
                "            \"cs_vocab_size\": df[\"cs\"].str.split(\" \").explode().n_unique(),\n",
                "            \"en_vocab_size\": df[\"en\"].str.split(\" \").explode().n_unique(),\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AlpacaConverter:\n",
                "    \"\"\"Memory-efficient converter to Alpaca instruction format.\"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        instruction_templates: Optional[Dict[str, str]] = None,\n",
                "        chunk_size: int = 100_000\n",
                "    ):\n",
                "        self.chunk_size = chunk_size\n",
                "        self.iso_to_lang = {\n",
                "            \"en\": \"angliƒçtiny\",\n",
                "            \"cs\": \"ƒçe≈°tiny\",\n",
                "        }\n",
                "        self.instruction_templates = instruction_templates or {\n",
                "            'translation': \"P≈ôelo≈æ tento text z {source_lang} do {target_lang}\",\n",
                "        }\n",
                "        self.logger = logging.getLogger(__name__)\n",
                "    \n",
                "    def create_instruction(self, task_type: str, **kwargs) -> str:\n",
                "        \"\"\"Create instruction from template.\"\"\"\n",
                "        template = self.instruction_templates.get(task_type)\n",
                "        if not template:\n",
                "            raise ValueError(f\"Unknown task type: {task_type}\")\n",
                "        return template.format(**kwargs)\n",
                "    \n",
                "    def create_translation_examples(\n",
                "        self,\n",
                "        df: pl.DataFrame,\n",
                "        source_lang: str,\n",
                "        target_lang: str,\n",
                "        output_path: Union[str, Path]\n",
                "    ) -> None:\n",
                "        \"\"\"Convert translation pairs to Alpaca format.\"\"\"\n",
                "        output_path = Path(output_path)\n",
                "        \n",
                "        # Check if file already exists\n",
                "        if output_path.exists():\n",
                "            self.logger.info(f\"Using existing processed file: {output_path}\")\n",
                "            return\n",
                "            \n",
                "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        iso_to_lang = {\n",
                "            \"en\": \"angliƒçtiny\",\n",
                "            \"cs\": \"ƒçe≈°tiny\",\n",
                "        }\n",
                "        instruction = self.create_instruction(\n",
                "            'translation',\n",
                "            source_lang=iso_to_lang[source_lang],\n",
                "            target_lang=iso_to_lang[target_lang]\n",
                "        )\n",
                "        \n",
                "        # Process in chunks and collect all chunks\n",
                "        chunks = []\n",
                "        total_rows = 0\n",
                "        with tqdm(total=len(df), desc=\"Converting translations\") as pbar:\n",
                "            for i in range(0, len(df), self.chunk_size):\n",
                "                chunk = df.slice(i, min(self.chunk_size, len(df) - i))\n",
                "                \n",
                "                # Create Alpaca format efficiently\n",
                "                alpaca_df = pl.DataFrame({\n",
                "                    \"instruction\": [instruction] * len(chunk),\n",
                "                    \"input\": chunk[source_lang],\n",
                "                    \"output\": chunk[target_lang]\n",
                "                })\n",
                "                chunks.append(alpaca_df)\n",
                "                total_rows += len(chunk)\n",
                "                pbar.update(len(chunk))\n",
                "                pbar.set_postfix({\"total_rows\": total_rows})\n",
                "        \n",
                "        # Combine all chunks and save at once\n",
                "        self.logger.info(f\"Combining {len(chunks)} chunks with {total_rows:,} total rows\")\n",
                "        final_df = pl.concat(chunks)\n",
                "        \n",
                "        self.logger.info(f\"Saving {total_rows:,} examples to {output_path}\")\n",
                "        final_df.write_parquet(\n",
                "            output_path,\n",
                "            compression=\"zstd\",\n",
                "            compression_level=3\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Data Processing Pipeline\n",
                "\n",
                "Now let's use our optimized classes to process the datasets:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
                "\n",
                "# Initialize components with optimized settings\n",
                "loader = ParaCrawlDataLoader(chunk_size=500_000)\n",
                "cleaner = CzechTextCleaner()\n",
                "converter = AlpacaConverter(chunk_size=100_000)\n",
                "\n",
                "# Setup paths\n",
                "data_dir = Path(\"data\")\n",
                "processed_dir = data_dir / \"processed\"\n",
                "processed_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "paracrawl_path = processed_dir / \"paracrawl_alpaca.parquet\"\n",
                "books_path = processed_dir / \"books_alpaca.parquet\"\n",
                "\n",
                "# Load ParaCrawl\n",
                "print(\"Loading and analyzing ParaCrawl dataset...\")\n",
                "df_paracrawl = loader.load_dataframe()\n",
                "df_paracrawl = df_paracrawl.slice(0, 50000)\n",
                "\n",
                "print(\"Raw ParaCrawl dataset:\")\n",
                "print(df_paracrawl.head())\n",
                "\n",
                "# Store raw data for comparison\n",
                "raw_paracrawl = df_paracrawl.clone()\n",
                "\n",
                "# Clean Czech texts\n",
                "print(\"\\nCleaning Czech texts...\")\n",
                "df_paracrawl = cleaner.clean_dataframe(\n",
                "    df_paracrawl,\n",
                "    text_columns=[\"cs\"],\n",
                "    num_threads=24\n",
                ")\n",
                "\n",
                "# Analyze stats\n",
                "stats = cleaner.analyze_parallel_stats(df_paracrawl)\n",
                "print(stats)\n",
                "\n",
                "print(\"Cleaned ParaCrawl dataset:\")\n",
                "print(df_paracrawl.head())\n",
                "\n",
                "# Convert to Alpaca format with progress tracking\n",
                "print(\"\\nConverting to Alpaca format...\")\n",
                "\n",
                "# Process translations\n",
                "print(\"Processing translations...\")\n",
                "converter.create_translation_examples(\n",
                "    df_paracrawl,\n",
                "    source_lang=\"en\",\n",
                "    target_lang=\"cs\",\n",
                "    output_path=paracrawl_path,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's examine the processed dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed datasets\n",
                "print(\"Loading processed datasets...\")\n",
                "translations_ds = load_dataset(\n",
                "    \"parquet\",\n",
                "    data_files=str(paracrawl_path)\n",
                ")\n",
                "\n",
                "print(\"Translations dataset:\")\n",
                "print(translations_ds[\"train\"].to_polars().head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Model Training Setup\n",
                "\n",
                "Now that our data is prepared, we'll set up the model training pipeline.\n",
                "\n",
                "The training pipeline will include:\n",
                "\n",
                "1. ü§ñ **Model Configuration**\n",
                "   - Gemma 2B base model\n",
                "   - Mixed precision (bfloat16)\n",
                "   - Gradient accumulation\n",
                "\n",
                "2. üìä **Training Loop**\n",
                "   - Custom data collation\n",
                "   - Efficient batching\n",
                "   - Progress tracking\n",
                "\n",
                "3. üìà **Evaluation**\n",
                "   - Translation metrics\n",
                "   - Text quality assessment\n",
                "   - Error analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First let's configure the hardware settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "# Configure hardware settings\n",
                "DEVICE = \"cuda\"\n",
                "DTYPE = torch.bfloat16"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TrainingConfig:\n",
                "    \"\"\"Configuration for Gemma fine-tuning\"\"\"\n",
                "\n",
                "    # Model settings\n",
                "    model_name: str = \"google/gemma-2-2b-it\"\n",
                "    max_length: int = 512\n",
                "\n",
                "    # LoRA settings\n",
                "    lora_r: int = 8\n",
                "    lora_alpha: int = 16\n",
                "    lora_dropout: float = 0.1\n",
                "\n",
                "    # Early stopping settings\n",
                "    early_stopping_patience: int = 3\n",
                "    early_stopping_threshold: float = 0.01\n",
                "\n",
                "    # Training settings (A40) - Experimenting with smaller batch size\n",
                "    batch_size: int = 16\n",
                "    gradient_accumulation_steps: int = 4\n",
                "    max_grad_norm: float = 1.0\n",
                "    num_epochs: int = 2\n",
                "    eval_steps: int = 2000\n",
                "\n",
                "    # Training settings (H200) - Full fine-tuning\n",
                "    # batch_size: int = 64\n",
                "    # gradient_accumulation_steps: int = 2\n",
                "    # num_epochs: int = 3\n",
                "    # eval_steps: int = 500\n",
                "\n",
                "    learning_rate: float = 3e-4\n",
                "    weight_decay: float = 0.01\n",
                "    warmup_ratio: float = 0.03\n",
                "\n",
                "    # Paths\n",
                "    output_dir: str = \"models/gemma-cs-translator\"\n",
                "\n",
                "\n",
                "config = TrainingConfig()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's prepare the dataset with specific format for the model:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset\n",
                "\n",
                "class GemmaChatDataset(Dataset):\n",
                "    \"\"\"Dataset for Gemma chat format\"\"\"\n",
                "\n",
                "    def __init__(self, data, tokenizer, max_length=512):\n",
                "        self.data = data\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        item = self.data[idx]\n",
                "\n",
                "        # Create chat format\n",
                "        chat = [\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": f\"{item['instruction']}\\n\\n{item['input']}\",\n",
                "            }\n",
                "        ]\n",
                "\n",
                "        # Apply chat template\n",
                "        input_text = self.tokenizer.apply_chat_template(\n",
                "            chat, tokenize=False, add_generation_prompt=True\n",
                "        )\n",
                "\n",
                "        # Add expected output\n",
                "        full_text = f\"{input_text}{item['output']}<end_of_turn>\"\n",
                "\n",
                "        # Tokenize\n",
                "        encodings = self.tokenizer(\n",
                "            full_text,\n",
                "            max_length=self.max_length,\n",
                "            padding=\"max_length\",\n",
                "            truncation=True,\n",
                "            return_tensors=\"pt\",\n",
                "        )\n",
                "\n",
                "        # Create attention mask and labels\n",
                "        input_ids = encodings[\"input_ids\"][0]\n",
                "        attention_mask = encodings[\"attention_mask\"][0]\n",
                "\n",
                "        return {\n",
                "            \"input_ids\": input_ids,\n",
                "            \"attention_mask\": attention_mask,\n",
                "            \"labels\": input_ids.clone(),\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we need to prepare the model and tokenizer for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType\n",
                "\n",
                "def setup_model_and_tokenizer(config: TrainingConfig):\n",
                "    \"\"\"Initialize model with LoRA and quantization\"\"\"\n",
                "\n",
                "    # Initialize tokenizer\n",
                "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
                "\n",
                "    # Quantization config\n",
                "    quant_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    )\n",
                "\n",
                "    # Load base model\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        config.model_name,\n",
                "        quantization_config=quant_config,\n",
                "        device_map=\"auto\",\n",
                "        torch_dtype=DTYPE,\n",
                "        trust_remote_code=True,\n",
                "    )\n",
                "\n",
                "    # Prepare model for k-bit training\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "    # LoRA configuration\n",
                "    lora_config = LoraConfig(\n",
                "        r=config.lora_r,\n",
                "        lora_alpha=config.lora_alpha,\n",
                "        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "        lora_dropout=config.lora_dropout,\n",
                "        bias=\"none\",\n",
                "        task_type=TaskType.CAUSAL_LM,\n",
                "    )\n",
                "\n",
                "    # Apply LoRA\n",
                "    model = get_peft_model(model, lora_config)\n",
                "\n",
                "    return model, tokenizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we need to prepare the dataset for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_datasets(data_path: str, tokenizer, config: TrainingConfig):\n",
                "    \"\"\"Load and split datasets for training\"\"\"\n",
                "\n",
                "    # Load the processed dataset\n",
                "    dataset = load_dataset(\"parquet\", data_files=data_path)[\"train\"]\n",
                "\n",
                "    # Split into train/val/test\n",
                "    splits = dataset.train_test_split(test_size=0.2, seed=42)\n",
                "    train_data = splits[\"train\"]\n",
                "\n",
                "    # Further split test into val/test\n",
                "    test_splits = splits[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
                "    val_data = test_splits[\"train\"]\n",
                "    test_data = test_splits[\"test\"]\n",
                "\n",
                "    print(f\"Train size: {len(train_data)}\")\n",
                "    print(f\"Val size: {len(val_data)}\")\n",
                "    print(f\"Test size: {len(test_data)}\")\n",
                "\n",
                "    # Create custom datasets\n",
                "    train_dataset = GemmaChatDataset(train_data, tokenizer, config.max_length)\n",
                "    val_dataset = GemmaChatDataset(val_data, tokenizer, config.max_length)\n",
                "    test_dataset = GemmaChatDataset(test_data, tokenizer, config.max_length)\n",
                "\n",
                "    return train_dataset, val_dataset, test_dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's prepare the training loop with custom data collation and efficient batching."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "import evaluate\n",
                "import numpy as np\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "\n",
                "def get_compute_metrics(tokenizer):\n",
                "    \"\"\"Create compute_metrics function with access to tokenizer\"\"\"\n",
                "\n",
                "    def compute_metrics(eval_preds):\n",
                "        \"\"\"Compute BLEU and other metrics for translation evaluation\"\"\"\n",
                "        bleu_metric = evaluate.load(\"bleu\")\n",
                "\n",
                "        predictions, labels = eval_preds\n",
                "\n",
                "        # Decode predictions\n",
                "        predictions = np.where(\n",
                "            predictions != -100, predictions, tokenizer.pad_token_id\n",
                "        )\n",
                "        decoded_preds = tokenizer.batch_decode(\n",
                "            predictions, skip_special_tokens=True\n",
                "        )\n",
                "\n",
                "        # Clean up predictions (remove template parts)\n",
                "        cleaned_preds = []\n",
                "        for pred in decoded_preds:\n",
                "            # Extract only the translation part after the template\n",
                "            try:\n",
                "                translation = (\n",
                "                    pred.split(\"<start_of_turn>model\\n\")[1]\n",
                "                    .split(\"<end_of_turn>\")[0]\n",
                "                    .strip()\n",
                "                )\n",
                "            except IndexError:\n",
                "                translation = pred  # Fallback if splitting fails\n",
                "            cleaned_preds.append(translation)\n",
                "\n",
                "        # Decode labels\n",
                "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "        decoded_labels = tokenizer.batch_decode(\n",
                "            labels, skip_special_tokens=True\n",
                "        )\n",
                "\n",
                "        # Clean up references (remove template parts)\n",
                "        cleaned_refs = []\n",
                "        for ref in decoded_labels:\n",
                "            try:\n",
                "                translation = (\n",
                "                    ref.split(\"<start_of_turn>model\\n\")[1]\n",
                "                    .split(\"<end_of_turn>\")[0]\n",
                "                    .strip()\n",
                "                )\n",
                "            except IndexError:\n",
                "                translation = ref  # Fallback if splitting fails\n",
                "            cleaned_refs.append([translation])\n",
                "\n",
                "        # Compute BLEU\n",
                "        bleu_score = bleu_metric.compute(\n",
                "            predictions=cleaned_preds, references=cleaned_refs\n",
                "        )\n",
                "\n",
                "        return {\n",
                "            \"bleu\": bleu_score[\"bleu\"],\n",
                "        }\n",
                "\n",
                "    return compute_metrics\n",
                "\n",
                "\n",
                "def get_training_args(config: TrainingConfig):\n",
                "    \"\"\"Configure training arguments optimized for translation task\"\"\"\n",
                "    return TrainingArguments(\n",
                "        output_dir=config.output_dir,\n",
                "        num_train_epochs=config.num_epochs,\n",
                "        per_device_train_batch_size=config.batch_size,\n",
                "        per_device_eval_batch_size=config.batch_size * 2,\n",
                "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
                "        learning_rate=config.learning_rate,\n",
                "        weight_decay=config.weight_decay,\n",
                "        warmup_ratio=config.warmup_ratio,\n",
                "        max_grad_norm=config.max_grad_norm,\n",
                "        # Evaluation settings\n",
                "        evaluation_strategy=\"steps\",\n",
                "        eval_steps=config.eval_steps,\n",
                "        save_strategy=\"steps\",\n",
                "        save_steps=config.eval_steps,\n",
                "        # Hardware optimization\n",
                "        bf16=True,\n",
                "        gradient_checkpointing=True,\n",
                "        # Logging\n",
                "        logging_strategy=\"steps\",\n",
                "        logging_steps=50,\n",
                "        logging_first_step=True,\n",
                "        report_to=\"wandb\",\n",
                "        # Model selection\n",
                "        load_best_model_at_end=True,\n",
                "        metric_for_best_model=\"bleu\",\n",
                "        greater_is_better=True,\n",
                "        optim=\"paged_adamw_32bit\",\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now before proceeding with the training, let's validate the pipeline with small sample of data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def validate_data_pipeline():\n",
                "    \"\"\"Validate the entire data pipeline\"\"\"\n",
                "    # Load a small subset\n",
                "    dataset = load_dataset(\n",
                "        \"parquet\", data_files=\"data/processed/paracrawl_alpaca.parquet\"\n",
                "    )[\"train\"].select(range(5))\n",
                "\n",
                "    # Initialize components\n",
                "    _, tokenizer = setup_model_and_tokenizer(config)\n",
                "    train_dataset = GemmaChatDataset(dataset, tokenizer)\n",
                "\n",
                "    # Check a sample\n",
                "    sample = train_dataset[0]\n",
                "\n",
                "    print(\"=== Data Pipeline Validation ===\")\n",
                "    print(\"\\n1. Input IDs shape:\", sample[\"input_ids\"].shape)\n",
                "    print(\"\\n2. Decoded input:\")\n",
                "    print(tokenizer.decode(sample[\"input_ids\"]))\n",
                "\n",
                "    # Test compute_metrics\n",
                "    dummy_preds = (\n",
                "        sample[\"input_ids\"].unsqueeze(0),\n",
                "        sample[\"labels\"].unsqueeze(0),\n",
                "    )\n",
                "    metrics = get_compute_metrics(tokenizer)(dummy_preds)\n",
                "\n",
                "    print(\"\\n3. Metrics computation test:\")\n",
                "    print(metrics)\n",
                "\n",
                "    return True\n",
                "\n",
                "\n",
                "validate_data_pipeline()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's define the training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import wandb\n",
                "from transformers import Trainer\n",
                "from transformers import EarlyStoppingCallback\n",
                "\n",
                "def train_translation_model():\n",
                "    \"\"\"Main training function with full pipeline\"\"\"\n",
                "    # Initialize wandb\n",
                "    wandb.init(\n",
                "        project=\"gemma-cs-translator\",\n",
                "        config={\n",
                "            \"model\": config.model_name,\n",
                "            \"lora_r\": config.lora_r,\n",
                "            \"batch_size\": config.batch_size,\n",
                "            \"learning_rate\": config.learning_rate,\n",
                "            \"early_stopping_patience\": config.early_stopping_patience,\n",
                "            \"early_stopping_threshold\": config.early_stopping_threshold,\n",
                "        },\n",
                "    )\n",
                "\n",
                "    # Setup model and tokenizer\n",
                "    model, tokenizer = setup_model_and_tokenizer(config)\n",
                "\n",
                "    # Prepare datasets\n",
                "    train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
                "        \"data/processed/paracrawl_alpaca.parquet\", tokenizer, config\n",
                "    )\n",
                "\n",
                "    # Setup training arguments\n",
                "    training_args = get_training_args(config)\n",
                "\n",
                "    # Create early stopping callback\n",
                "    early_stopping_callback = EarlyStoppingCallback(\n",
                "        early_stopping_patience=config.early_stopping_patience,\n",
                "        early_stopping_threshold=config.early_stopping_threshold,\n",
                "    )\n",
                "\n",
                "    # Initialize trainer\n",
                "    trainer = Trainer(\n",
                "        model=model,\n",
                "        args=training_args,\n",
                "        train_dataset=train_dataset,\n",
                "        eval_dataset=val_dataset,\n",
                "        compute_metrics=get_compute_metrics(tokenizer),\n",
                "        callbacks=[early_stopping_callback],\n",
                "    )\n",
                "\n",
                "    # Train\n",
                "    train_result = trainer.train()\n",
                "\n",
                "    # Save final model\n",
                "    trainer.save_model(f\"{config.output_dir}/final\")\n",
                "\n",
                "    # Log metrics\n",
                "    metrics = train_result.metrics\n",
                "    trainer.log_metrics(\"train\", metrics)\n",
                "\n",
                "    # Evaluate on test set\n",
                "    test_metrics = trainer.evaluate(test_dataset)\n",
                "    trainer.log_metrics(\"test\", test_metrics)\n",
                "\n",
                "    return trainer, test_metrics, tokenizer\n",
                "\n",
                "\n",
                "# Run training\n",
                "trainer, final_metrics, tokenizer = train_translation_model()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now post training we can evaluate the model on the test dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TranslationInference:\n",
                "    def __init__(self, model_path: str):\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
                "        self.model = AutoModelForCausalLM.from_pretrained(\n",
                "            model_path,\n",
                "            device_map=\"auto\",\n",
                "            torch_dtype=DTYPE,\n",
                "            trust_remote_code=True,\n",
                "        )\n",
                "        self.model.eval()\n",
                "\n",
                "    def translate(self, text: str, max_length: int = 512) -> str:\n",
                "        chat = [\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": f\"P≈ôelo≈æ tento text z angliƒçtiny do ƒçe≈°tiny:\\n\\n{text}\",\n",
                "            }\n",
                "        ]\n",
                "\n",
                "        prompt = self.tokenizer.apply_chat_template(\n",
                "            chat, tokenize=False, add_generation_prompt=True\n",
                "        )\n",
                "\n",
                "        inputs = self.tokenizer(\n",
                "            prompt,\n",
                "            return_tensors=\"pt\",\n",
                "            padding=True,\n",
                "            truncation=True,\n",
                "            max_length=max_length,\n",
                "        ).to(self.model.device)\n",
                "\n",
                "        with torch.no_grad():\n",
                "            outputs = self.model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=max_length,\n",
                "                do_sample=True,\n",
                "                temperature=0.7,\n",
                "                top_p=0.9,\n",
                "            )\n",
                "\n",
                "        translation = self.tokenizer.decode(\n",
                "            outputs[0], skip_special_tokens=True\n",
                "        )\n",
                "        return translation.split(\"<end_of_turn>\")[0].strip()\n",
                "\n",
                "\n",
                "# Test the model\n",
                "def test_model(test_dataset, model_path: str):\n",
                "    \"\"\"Run comprehensive tests on the model\"\"\"\n",
                "    inference = TranslationInference(model_path)\n",
                "    bleu_metric = evaluate.load(\"bleu\")\n",
                "\n",
                "    predictions = []\n",
                "    references = []\n",
                "\n",
                "    print(\"Running inference on test set...\")\n",
                "    for i, example in enumerate(tqdm(test_dataset)):\n",
                "        if i > 100:  # Test on subset for speed\n",
                "            break\n",
                "\n",
                "        # Get original text\n",
                "        input_text = tokenizer.decode(\n",
                "            example[\"input_ids\"], skip_special_tokens=True\n",
                "        )\n",
                "        source_text = input_text.split(\"\\n\\n\")[1]  # Extract source text\n",
                "\n",
                "        # Get model prediction\n",
                "        translation = inference.translate(source_text)\n",
                "        predictions.append(translation)\n",
                "\n",
                "        # Get reference translation\n",
                "        reference = tokenizer.decode(\n",
                "            example[\"labels\"], skip_special_tokens=True\n",
                "        )\n",
                "        references.append([reference])\n",
                "\n",
                "    # Calculate BLEU\n",
                "    bleu_score = bleu_metric.compute(\n",
                "        predictions=predictions, references=references\n",
                "    )\n",
                "\n",
                "    print(f\"\\nTest BLEU Score: {bleu_score['bleu']:.2f}\")\n",
                "\n",
                "    # Show some examples\n",
                "    print(\"\\nExample Translations:\")\n",
                "    for i in range(min(3, len(predictions))):\n",
                "        print(f\"\\nSource: {references[i][0]}\")\n",
                "        print(f\"Prediction: {predictions[i]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's test the model on some examples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 4.6 Example Usage\n",
                "\n",
                "# Test the trained model\n",
                "model_path = f\"{config.output_dir}/final\"\n",
                "test_model(test_dataset, model_path)\n",
                "\n",
                "# Interactive translation example\n",
                "inference = TranslationInference(model_path)\n",
                "\n",
                "examples = [\n",
                "    \"Hello, how are you today?\",\n",
                "    \"This is a test of the translation system.\",\n",
                "    \"Machine learning is transforming the world.\",\n",
                "]\n",
                "\n",
                "print(\"\\nInteractive Translation Examples:\")\n",
                "for text in examples:\n",
                "    translation = inference.translate(text)\n",
                "    print(f\"\\nEnglish: {text}\")\n",
                "    print(f\"Czech: {translation}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gemma-global-competetion",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.15"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
