{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Czech Language Adaptation of Gemma Language Model\n",
    "\n",
    "**Author:** Jirka Helmich\n",
    "\n",
    "**Last Updated:** 2025-01-06\n",
    "\n",
    "**License:** MIT\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the fine-tuning process of the Gemma language model for Czech language understanding and generation. We focus on creating a robust multilingual model capable of handling various Czech-specific NLP tasks.\n",
    "\n",
    "### Key Objectives\n",
    "\n",
    "1. 🎯 **Primary Goal**: Adapt Gemma for superior Czech language processing\n",
    "2. 🔄 **Tasks**: Translation, sentiment analysis, text generation\n",
    "3. 📊 **Evaluation**: Comprehensive benchmarking on Czech-specific metrics\n",
    "\n",
    "### Technical Requirements\n",
    "\n",
    "```\n",
    "Python >= 3.10\n",
    "polars >= 0.20.0\n",
    "datasets >= 2.15.0\n",
    "tqdm >= 4.66.0\n",
    "```\n",
    "\n",
    "### Dataset Sources\n",
    "\n",
    "We utilize multiple high-quality Czech datasets:\n",
    "\n",
    "1. **ParaCrawl v9**\n",
    "   - Parallel corpus for EN-CS translation\n",
    "   - ~52M sentence pairs\n",
    "   - [Source](https://paracrawl.eu/v9)\n",
    "\n",
    "2. **Czech Books Descriptions**\n",
    "   - Book descriptions in Czech\n",
    "   - [Source](https://huggingface.co/datasets/vojtam/czech_books_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's install required dependencies. We use specific versions to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 2.15.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets>=2.15.0 polars>=0.20.0 tqdm>=4.66.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Components\n",
    "\n",
    "### 1. ParaCrawl Dataset Loader\n",
    "\n",
    "The `ParaCrawlDataLoader` class handles downloading and processing of the ParaCrawl translation dataset. Key features:\n",
    "\n",
    "- Automatic download and decompression\n",
    "- Progress tracking\n",
    "- Data cleaning and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "This section implements a robust data loader for the ParaCrawl dataset with the following features:\n",
    "\n",
    "- ✨ Automatic download with progress tracking\n",
    "- 🔍 Data validation and integrity checks\n",
    "- 📊 Efficient processing using Polars\n",
    "- 💾 Caching of processed data\n",
    "\n",
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import gzip\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParaCrawl Data Loader Class\n",
    "\n",
    "The main class implementation with detailed documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaCrawlDataLoader:\n",
    "    \"\"\"Handles downloading and processing of ParaCrawl translation datasets.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_lang: str = \"en\",\n",
    "        target_lang: str = \"cs\",\n",
    "        data_dir: Optional[str] = None,\n",
    "        cache_dir: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Initialize the ParaCrawl data loader.\"\"\"\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.base_url = \"https://web-language-models.s3.amazonaws.com/paracrawl/release9\"\n",
    "\n",
    "        # Setup directories\n",
    "        self.data_dir = Path(data_dir or \"./data\")\n",
    "        self.cache_dir = Path(cache_dir or \"./cache\")\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Construct file paths\n",
    "        self.filename = f\"{source_lang}-{target_lang}.txt.gz\"\n",
    "        self.filepath = self.data_dir / self.filename\n",
    "        self.processed_path = self.cache_dir / f\"{source_lang}-{target_lang}.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Validation Methods\n",
    "\n",
    "Methods for downloading data with progress tracking and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_with_progress(self, url: str, filepath: Path) -> None:\n",
    "    \"\"\"Download a file with progress bar.\"\"\"\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url)\n",
    "        total_size = int(response.headers['Content-Length'])\n",
    "        print(f\"Total size: {total_size}\")\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"Downloading {filepath.name}\") as pbar:\n",
    "            urllib.request.urlretrieve(\n",
    "                url,\n",
    "                filepath,\n",
    "                reporthook=lambda count, block_size, total_size: pbar.update(block_size)\n",
    "            )\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Error downloading file: {e}\")\n",
    "        raise\n",
    "\n",
    "def _validate_file(self, filepath: Path) -> bool:\n",
    "    \"\"\"Validate downloaded file integrity.\"\"\"\n",
    "    if not filepath.exists():\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "            for _ in range(5):\n",
    "                line = f.readline()\n",
    "                if not '\\t' in line:\n",
    "                    return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "ParaCrawlDataLoader._download_with_progress = _download_with_progress\n",
    "ParaCrawlDataLoader._validate_file = _validate_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing Methods\n",
    "\n",
    "Methods for processing and loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_raw_file(self) -> None:\n",
    "    \"\"\"Process raw gzipped file into Parquet format.\"\"\"\n",
    "    if self.processed_path.exists():\n",
    "        self.logger.info(\"Using cached processed data\")\n",
    "        return\n",
    "\n",
    "    self.logger.info(\"Processing raw data file...\")\n",
    "\n",
    "    chunk_size = 100_000\n",
    "    chunks = []\n",
    "\n",
    "    with gzip.open(self.filepath, \"rt\", encoding=\"utf-8\") as f:\n",
    "        with tqdm(desc=\"Processing chunks\") as pbar:\n",
    "            while True:\n",
    "                lines = [next(f, None) for _ in range(chunk_size)]\n",
    "                lines = [line for line in lines if line is not None]\n",
    "\n",
    "                if not lines:\n",
    "                    break\n",
    "\n",
    "                pairs = [line.strip().split(\"\\t\") for line in lines]\n",
    "                # Filter out invalid pairs\n",
    "                pairs = [p for p in pairs if len(p) == 2]\n",
    "\n",
    "                if not pairs:\n",
    "                    continue\n",
    "\n",
    "                # Pre-filter by length before creating DataFrame\n",
    "                pairs = [\n",
    "                    p\n",
    "                    for p in pairs\n",
    "                    if (0 < len(p[0]) < 1000 and 0 < len(p[1]) < 1000)\n",
    "                ]\n",
    "\n",
    "                if not pairs:\n",
    "                    continue\n",
    "\n",
    "                chunk_df = pl.DataFrame(\n",
    "                    pairs,\n",
    "                    schema=[self.source_lang, self.target_lang],\n",
    "                    orient=\"row\",  # Explicitly specify orientation\n",
    "                )\n",
    "\n",
    "                if len(chunk_df) > 0:\n",
    "                    chunks.append(chunk_df)\n",
    "                pbar.update(1)\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(\"No valid data found in the input file\")\n",
    "\n",
    "    df = pl.concat(chunks)\n",
    "    df.write_parquet(self.processed_path)\n",
    "    self.logger.info(f\"Processed data saved to {self.processed_path}\")\n",
    "\n",
    "\n",
    "ParaCrawlDataLoader._process_raw_file = _process_raw_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Interface Methods\n",
    "\n",
    "Methods for downloading and loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(self) -> None:\n",
    "    \"\"\"Download ParaCrawl dataset if not already present.\"\"\"\n",
    "    if self.filepath.exists() and self._validate_file(self.filepath):\n",
    "        self.logger.info(\"Using existing download\")\n",
    "        return\n",
    "        \n",
    "    url = f\"{self.base_url}/{self.source_lang}-{self.target_lang}/{self.filename}\"\n",
    "    self.logger.info(f\"Downloading from {url}\")\n",
    "    \n",
    "    self._download_with_progress(url, self.filepath)\n",
    "    \n",
    "    if not self._validate_file(self.filepath):\n",
    "        raise ValueError(\"Downloaded file appears to be corrupt\")\n",
    "\n",
    "def load_dataframe(self) -> pl.DataFrame:\n",
    "    \"\"\"Load the processed ParaCrawl dataset.\"\"\"\n",
    "    self.download_data()\n",
    "    self._process_raw_file()\n",
    "    \n",
    "    df = pl.read_parquet(self.processed_path)\n",
    "    self.logger.info(f\"Loaded {len(df):,} translation pairs\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_sample(self, n: int = 5) -> pl.DataFrame:\n",
    "    \"\"\"Get a sample of n translation pairs.\"\"\"\n",
    "    df = self.load_dataframe()\n",
    "    return df.sample(n)\n",
    "\n",
    "ParaCrawlDataLoader.download_data = download_data\n",
    "ParaCrawlDataLoader.load_dataframe = load_dataframe\n",
    "ParaCrawlDataLoader.get_sample = get_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Alpaca Format Converter Implementation\n",
    "\n",
    "This section implements a robust converter for transforming datasets into the Alpaca instruction format, which is optimized for fine-tuning language models. Key features:\n",
    "\n",
    "- 🔄 Flexible input handling\n",
    "- 📝 Customizable instruction templates\n",
    "- 💾 Efficient JSONL output\n",
    "- ✨ Data validation and cleaning\n",
    "\n",
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Union\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpaca Data Converter Class\n",
    "\n",
    "Main class for converting datasets to Alpaca instruction format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlpacaConverter:\n",
    "    \"\"\"Converts datasets to Alpaca instruction format for fine-tuning.\n",
    "    \n",
    "    This class handles the conversion of various dataset formats into the\n",
    "    Alpaca instruction format, which is suitable for fine-tuning language models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        instruction_templates: Optional[Dict[str, str]] = None,\n",
    "        max_length: int = 2048,\n",
    "        min_length: int = 3\n",
    "    ):\n",
    "        \"\"\"Initialize the Alpaca converter.\n",
    "        \n",
    "        Args:\n",
    "            instruction_templates: Dictionary of task types to instruction templates\n",
    "            max_length: Maximum length of input/output text\n",
    "            min_length: Minimum length of input/output text\n",
    "        \"\"\"\n",
    "        self.instruction_templates = instruction_templates or {\n",
    "            'translation': \"Přelož tento text z {source_lang} do {target_lang}\",\n",
    "            'book_description': \"Popiš tuto knihu\",\n",
    "        }\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation Methods\n",
    "\n",
    "Methods for validating and cleaning input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_text(self, text: str) -> bool:\n",
    "    \"\"\"Validate text length and content.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to validate\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if text is valid\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "        \n",
    "    text = text.strip()\n",
    "    length = len(text)\n",
    "    \n",
    "    return (length >= self.min_length and \n",
    "            length <= self.max_length and\n",
    "            not text.isspace())\n",
    "\n",
    "def _clean_text(self, text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    return \" \".join(text.strip().split())\n",
    "\n",
    "AlpacaConverter._validate_text = _validate_text\n",
    "AlpacaConverter._clean_text = _clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Conversion Methods\n",
    "\n",
    "Core methods for converting data to Alpaca format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_instruction(self, task_type: str, **kwargs) -> str:\n",
    "    \"\"\"Create instruction from template.\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of task (e.g., 'translation')\n",
    "        **kwargs: Format parameters for instruction template\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted instruction\n",
    "    \"\"\"\n",
    "    template = self.instruction_templates.get(task_type)\n",
    "    if not template:\n",
    "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "    return template.format(**kwargs)\n",
    "\n",
    "def _create_example(self,\n",
    "    instruction: str,\n",
    "    output: str,\n",
    "    input_text: Optional[str] = None\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Create a single Alpaca format example.\n",
    "    \n",
    "    Args:\n",
    "        instruction: Task instruction\n",
    "        output: Expected output text\n",
    "        input_text: Optional input text\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Alpaca format example\n",
    "    \"\"\"\n",
    "    example = {\n",
    "        \"instruction\": instruction,\n",
    "        \"output\": self._clean_text(output)\n",
    "    }\n",
    "    \n",
    "    if input_text:\n",
    "        example[\"input\"] = self._clean_text(input_text)\n",
    "        \n",
    "    return example\n",
    "\n",
    "AlpacaConverter._create_instruction = _create_instruction\n",
    "AlpacaConverter._create_example = _create_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Interface Methods\n",
    "\n",
    "Methods for converting different types of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_translations(\n",
    "    self,\n",
    "    df: pl.DataFrame,\n",
    "    source_lang: str,\n",
    "    target_lang: str, \n",
    "    output_path: Union[str, Path]\n",
    ") -> None:\n",
    "    \"\"\"Convert translation pairs to Alpaca format.\"\"\"\n",
    "    instruction = self._create_instruction(\n",
    "        'translation',\n",
    "        source_lang=source_lang,\n",
    "        target_lang=target_lang\n",
    "    )\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process in chunks for memory efficiency\n",
    "    chunk_size = 10000\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        with tqdm(total=len(df), desc=\"Processing translation rows\") as pbar:\n",
    "            for i in range(0, len(df), chunk_size):\n",
    "                # Get chunk\n",
    "                chunk = df.slice(i, chunk_size)\n",
    "                \n",
    "                # Process chunk\n",
    "                valid_rows = []\n",
    "                for row in chunk.iter_rows():\n",
    "                    source = row[chunk.get_column_index(source_lang)]\n",
    "                    target = row[chunk.get_column_index(target_lang)]\n",
    "                    \n",
    "                    if self._validate_text(source) and self._validate_text(target):\n",
    "                        example = {\n",
    "                            \"instruction\": instruction,\n",
    "                            \"input\": self._clean_text(source),\n",
    "                            \"output\": self._clean_text(target)\n",
    "                        }\n",
    "                        valid_rows.append(json.dumps(example, ensure_ascii=False))\n",
    "                \n",
    "                # Write valid rows\n",
    "                if valid_rows:\n",
    "                    f.write('\\n'.join(valid_rows) + '\\n')\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(len(chunk))\n",
    "                pbar.set_postfix({'valid_rows': len(valid_rows)})\n",
    "\n",
    "def convert_descriptions(\n",
    "    self,\n",
    "    df: pl.DataFrame,\n",
    "    title_col: str,\n",
    "    desc_col: str,\n",
    "    output_path: Union[str, Path]\n",
    ") -> None:\n",
    "    \"\"\"Convert title-description pairs to Alpaca format.\"\"\"\n",
    "    instruction = self._create_instruction('book_description')\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process in chunks for memory efficiency\n",
    "    chunk_size = 10000\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        with tqdm(total=len(df), desc=\"Processing book description rows\") as pbar:\n",
    "            for i in range(0, len(df), chunk_size):\n",
    "                # Get chunk\n",
    "                chunk = df.slice(i, chunk_size)\n",
    "                \n",
    "                # Process chunk\n",
    "                valid_rows = []\n",
    "                for row in chunk.iter_rows():\n",
    "                    title = row[chunk.get_column_index(title_col)]\n",
    "                    desc = row[chunk.get_column_index(desc_col)]\n",
    "                    \n",
    "                    if self._validate_text(title) and self._validate_text(desc):\n",
    "                        example = {\n",
    "                            \"instruction\": instruction,\n",
    "                            \"input\": self._clean_text(title),\n",
    "                            \"output\": self._clean_text(desc)\n",
    "                        }\n",
    "                        valid_rows.append(json.dumps(example, ensure_ascii=False))\n",
    "                \n",
    "                # Write valid rows\n",
    "                if valid_rows:\n",
    "                    f.write('\\n'.join(valid_rows) + '\\n')\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(len(chunk))\n",
    "                pbar.set_postfix({'valid_rows': len(valid_rows)})\n",
    "\n",
    "AlpacaConverter.convert_translations = convert_translations\n",
    "AlpacaConverter.convert_descriptions = convert_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline 🔄\n",
    "\n",
    "This section implements the main data processing pipeline for preparing our training data. We'll walk through each step to ensure high-quality training data.\n",
    "\n",
    "### Pipeline Overview 📋\n",
    "\n",
    "1. 📥 **Load ParaCrawl Corpus**\n",
    "   - Download EN-CS parallel data\n",
    "   - Clean and validate entries\n",
    "   - Remove low-quality pairs\n",
    "\n",
    "2. 📚 **Process Book Descriptions**\n",
    "   - Load Czech book dataset\n",
    "   - Extract titles and descriptions\n",
    "   - Filter and clean text\n",
    "\n",
    "3. 🔄 **Format Conversion**\n",
    "   - Transform to Alpaca format\n",
    "   - Add instruction templates\n",
    "   - Validate final structure\n",
    "\n",
    "4. 💾 **Save Training Data**\n",
    "   - Export to JSONL format\n",
    "   - Create data splits\n",
    "   - Verify data integrity\n",
    "\n",
    "### Key Features ✨\n",
    "\n",
    "- 🧹 Robust data cleaning\n",
    "- ⚡ Efficient Polars processing\n",
    "- 🔍 Quality validation steps\n",
    "- 📊 Progress tracking\n",
    "- 💪 Scalable pipeline\n",
    "\n",
    "### 1. Load and Process ParaCrawl Dataset 🌐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using existing download\n",
      "INFO:__main__:Using cached processed data\n",
      "INFO:__main__:Loaded 50,631,690 translation pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50,631,690 translation pairs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>en</th><th>cs</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Offering various dining option…</td><td>&quot;Hosté se mohou najíst v restau…</td></tr><tr><td>&quot;As families grow in size, so t…</td><td>&quot;Čím větší rodina, tím více pož…</td></tr><tr><td>&quot;Weather in Barueri: no precipi…</td><td>&quot;Počasí v Barueri: přeháňky - 0…</td></tr><tr><td>&quot;Local Time: Sīdī Sālim, Egypt&quot;</td><td>&quot;Místní čas: Al Husayniyah, Egy…</td></tr><tr><td>&quot;Then let him patiently wait an…</td><td>&quot;Pak nechť trpělivě čeká a pečl…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────────────────┬─────────────────────────────────┐\n",
       "│ en                              ┆ cs                              │\n",
       "│ ---                             ┆ ---                             │\n",
       "│ str                             ┆ str                             │\n",
       "╞═════════════════════════════════╪═════════════════════════════════╡\n",
       "│ Offering various dining option… ┆ Hosté se mohou najíst v restau… │\n",
       "│ As families grow in size, so t… ┆ Čím větší rodina, tím více pož… │\n",
       "│ Weather in Barueri: no precipi… ┆ Počasí v Barueri: přeháňky - 0… │\n",
       "│ Local Time: Sīdī Sālim, Egypt   ┆ Místní čas: Al Husayniyah, Egy… │\n",
       "│ Then let him patiently wait an… ┆ Pak nechť trpělivě čeká a pečl… │\n",
       "└─────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize loader\n",
    "loader = ParaCrawlDataLoader(source_lang=\"en\", target_lang=\"cs\")\n",
    "\n",
    "# Load ParaCrawl EN-CS dataset\n",
    "df_paracrawl = loader.load_dataframe()\n",
    "print(f\"Loaded {len(df_paracrawl):,} translation pairs\")\n",
    "df_paracrawl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Process Book Descriptions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8,072 book descriptions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>title</th><th>author</th><th>text</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Za vším hledej Bridgertonovy&quot;</td><td>&quot;Julia Quinn&quot;</td><td>&quot;Oblíbená série o rodině Bridge…</td></tr><tr><td>&quot;Kdo s ohněm zachází&quot;</td><td>&quot;Pavel Hrdlička&quot;</td><td>&quot;Volné pokračování historické d…</td></tr><tr><td>&quot;Pozitronový muž&quot;</td><td>&quot;Isaac Asimov&quot;</td><td>&quot;Román vypráví o inteligentním …</td></tr><tr><td>&quot;Anděl smrti&quot;</td><td>&quot;Vlastimil Vondruška&quot;</td><td>&quot;Jubilejní třicátý román s Oldř…</td></tr><tr><td>&quot;Kvalitativní výzkum: Základní …</td><td>&quot;Jan Hendl&quot;</td><td>&quot;Metody kvalitativního výzkumu …</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────────────────────────────┬─────────────────────┬─────────────────────────────────┐\n",
       "│ title                           ┆ author              ┆ text                            │\n",
       "│ ---                             ┆ ---                 ┆ ---                             │\n",
       "│ str                             ┆ str                 ┆ str                             │\n",
       "╞═════════════════════════════════╪═════════════════════╪═════════════════════════════════╡\n",
       "│ Za vším hledej Bridgertonovy    ┆ Julia Quinn         ┆ Oblíbená série o rodině Bridge… │\n",
       "│ Kdo s ohněm zachází             ┆ Pavel Hrdlička      ┆ Volné pokračování historické d… │\n",
       "│ Pozitronový muž                 ┆ Isaac Asimov        ┆ Román vypráví o inteligentním … │\n",
       "│ Anděl smrti                     ┆ Vlastimil Vondruška ┆ Jubilejní třicátý román s Oldř… │\n",
       "│ Kvalitativní výzkum: Základní … ┆ Jan Hendl           ┆ Metody kvalitativního výzkumu … │\n",
       "└─────────────────────────────────┴─────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Czech book descriptions\n",
    "ds = load_dataset(\"vojtam/czech_books_descriptions\")\n",
    "books_df = ds['train'].to_polars()\n",
    "print(f\"Loaded {len(books_df):,} book descriptions\")\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Training Format\n",
    "\n",
    "Convert our processed datasets to the Alpaca instruction format for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36f3cc10f664118a474ddd2e377bfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing translation rows:   0%|          | 0/50631690 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626f730274874842838e06218640c47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing book description rows:   0%|          | 0/8072 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "alpaca_converter = AlpacaConverter()\n",
    "\n",
    "if not os.path.exists(\"data/translation/dataset/paracrawl.jsonl\"):\n",
    "    alpaca_converter.convert_translations(\n",
    "        df_paracrawl,\n",
    "        source_lang=\"en\",\n",
    "        target_lang=\"cs\",\n",
    "        output_path=\"data/translation/dataset/paracrawl.jsonl\"\n",
    "    )\n",
    "\n",
    "if not os.path.exists(\"data/translation/dataset/czech_books.jsonl\"):\n",
    "    alpaca_converter.convert_descriptions(\n",
    "        books_df,\n",
    "        title_col=\"title\",\n",
    "        desc_col=\"text\",\n",
    "        output_path=\"data/translation/dataset/czech_books.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "\n",
    "In this phase we will load the dataset and split it into train, validation and test sets.\n",
    "\n",
    "This is the end of the data processing phase and we will be proceeding to the model fine-tuning phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d326db7d9264c25a3a8d93d3787b82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import polars as pl\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "translations_ds = load_dataset(\n",
    "    \"json\", data_files=\"data/translation/dataset/paracrawl.jsonl\"\n",
    ")\n",
    "books_ds = load_dataset(\n",
    "    \"json\", data_files=\"data/translation/dataset/czech_books.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the dataset samples to verify that the data is in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Samples ===\n",
      "\n",
      "First translation sample:\n",
      "{'instruction': 'Přelož tento text z en do cs', 'input': 'Inicial version of ClueMaker', 'output': 'Iniciální verze ClueMakeru'}\n",
      "\n",
      "First book description sample:\n",
      "{'instruction': 'Popiš tuto knihu', 'input': 'PS: Je to výzva', 'output': 'Drahá slečno Keanová,dříve, než tato směšná domluva započne, dovolte mi, abych se dostatečně jasně vyjádřil: vím, kdo jste, vím, že Vás můj otec najal, a vím, proč Vás najal, nejpodstatnější však je, že Vaše služby nejsou zapotřebí.Ve skutečnosti nestojím o žádnou část miliardového impéria svého otce, a nic na tom nezmění ani skutečnost, že mi „dopřeje“ jednu z „nejlepších asistentek v zemi.“ Jen plýtvá penězi. A Vy svým časem.Nicméně jelikož jste tak pošetile podepsala neprůstřelnou smlouvu s klauzulí o zásahu vyšší moci a jelikož mě můj otec dotlačil do přijetí této funkce, zdá se, že jsme tu společně uvízli – alespoň do příštího měsíce, než Vaše smlouva vyprší.Naše spolupráce ve WellesTechu by měla proběhnout relativně bez problémů, avšak nenamlouvejte si, že jsem si nevšimnul toho, jak mě Vaše krásné oči sledují nebo jak zadržujete dech, když se naše ruce střetnou. Fascinuji Vás, což Vás ničí, protože se mnou stěží dokážete vydržet v jedné místnosti.Máte mě za problém, který stojí za vyřešení? Za nerozluštitelnou hádanku? Zajisté, do toho. Vypočítejte x. Prolomte kód. Možná to bude i legrace (ale pouze pro mě, pro Vás ne).S úctouCalder Welles IIPS: Je to výzva!'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Dataset Samples ===\")\n",
    "print(\"\\nFirst translation sample:\")\n",
    "print(translations_ds[\"train\"][0])\n",
    "print(\"\\nFirst book description sample:\")\n",
    "print(books_ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to examine the dataset statistics to see if there are any potential issues with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Sizes ===\n",
      "Translation dataset size: 40505352\n",
      "Books dataset size: 6405\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Dataset Sizes ===\")\n",
    "print(f\"Translation dataset size: {len(translations_ds['train'])}\")\n",
    "print(f\"Books dataset size: {len(books_ds['train'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all seems to be in order, we can create the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting datasets...\n",
      "\n",
      "=== Split Sizes ===\n",
      "\n",
      "Translations train:\n",
      "Size: 40505352\n",
      "\n",
      "Books train:\n",
      "Size: 6405\n",
      "\n",
      "Translations validation:\n",
      "Size: 5063169\n",
      "\n",
      "Books validation:\n",
      "Size: 801\n",
      "\n",
      "Translations test:\n",
      "Size: 5063169\n",
      "\n",
      "Books test:\n",
      "Size: 801\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSplitting datasets...\")\n",
    "\n",
    "# Split translations dataset\n",
    "translations_ds = translations_ds[\"train\"].train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42\n",
    ")\n",
    "translations_test_val = translations_ds[\"test\"].train_test_split(\n",
    "    test_size=0.5, shuffle=True, seed=42\n",
    ")\n",
    "\n",
    "translations_ds = {\n",
    "    \"train\": translations_ds[\"train\"],\n",
    "    \"validation\": translations_test_val[\"train\"],\n",
    "    \"test\": translations_test_val[\"test\"],\n",
    "}\n",
    "\n",
    "# Split books dataset\n",
    "books_ds = books_ds[\"train\"].train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42\n",
    ")\n",
    "books_test_val = books_ds[\"test\"].train_test_split(\n",
    "    test_size=0.5, shuffle=True, seed=42\n",
    ")\n",
    "\n",
    "books_ds = {\n",
    "    \"train\": books_ds[\"train\"],\n",
    "    \"validation\": books_test_val[\"train\"],\n",
    "    \"test\": books_test_val[\"test\"],\n",
    "}\n",
    "\n",
    "# 5. Print split sizes\n",
    "print(\"\\n=== Split Sizes ===\")\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"\\nTranslations {split}:\")\n",
    "    print(f\"Size: {len(translations_ds[split])}\")\n",
    "    print(f\"\\nBooks {split}:\")\n",
    "    print(f\"Size: {len(books_ds[split])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the data is now prepared and we need to combine the datasets into a single dataset later used for model fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Combined Dataset Sizes ===\n",
      "\n",
      "train:\n",
      "Size: 40511757\n",
      "\n",
      "validation:\n",
      "Size: 5063970\n",
      "\n",
      "test:\n",
      "Size: 5063970\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "combined_datasets = {\n",
    "    split: concatenate_datasets([translations_ds[split], books_ds[split]])\n",
    "    for split in [\"train\", \"validation\", \"test\"]\n",
    "}\n",
    "\n",
    "print(\"\\n=== Combined Dataset Sizes ===\")\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"\\n{split}:\")\n",
    "    print(f\"Size: {len(combined_datasets[split])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Model Fine-Tuning\n",
    "\n",
    "In this phase, we will fine-tune the model on the combined dataset using **PyTorch** and the **Hugging Face Transformers** library. The `gemma-2-2b-it` model will serve as the base model. \n",
    "\n",
    "The datasets are already split into train/validation/test sets (80/10/10 ratio) for proper evaluation. Let's break down the process into key phases:\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Model Architecture\n",
    "\n",
    "We will use **Google's Gemma 2B** as the base model, fine-tuned with **PyTorch Lightning** for efficient training. The architecture includes:\n",
    "\n",
    "- 📂 **Custom Dataset Class** for handling the specific data format\n",
    "- 📊 **Lightning DataModule** for data management\n",
    "- 🛠️ **Lightning Module** for training logic\n",
    "- ⚡ **Mixed Precision Training** (bfloat16) for improved performance\n",
    "- 🧹 **Gradient Accumulation and Clipping** for stability during training\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Training Process\n",
    "\n",
    "The training will proceed through several stages:\n",
    "\n",
    "1. **Data Batching and Tokenization** 🗃️:\n",
    "   - Efficiently preprocess and batch the input data.\n",
    "   \n",
    "2. **Forward Pass** 🔄:\n",
    "   - Pass the tokenized data through the `gemma-2b` model.\n",
    "   \n",
    "3. **Loss Calculation** 🎯:\n",
    "   - Use **Cross Entropy** as the loss function.\n",
    "\n",
    "4. **Backpropagation** 🔙:\n",
    "   - Perform backpropagation with **Gradient Accumulation** to stabilize updates.\n",
    "\n",
    "5. **Optimization** 🛠️:\n",
    "   - Use **AdamW** optimizer with **Cosine Learning Rate Scheduling** for smooth convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Evaluation\n",
    "\n",
    "We will evaluate the model's performance using:\n",
    "\n",
    "- 📉 **Validation Loss** during training to track progress\n",
    "- ✅ **Test Set Performance** to assess generalization\n",
    "- 🔍 **Practical Examples** from both tasks to verify real-world applicability\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Monitoring\n",
    "\n",
    "Training progress will be tracked using **Weights & Biases (W&B)** 📈. Model checkpoints will be saved based on **validation loss improvements** 💾.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Next Steps\n",
    "\n",
    "After successful training, the following steps will be taken:\n",
    "\n",
    "1. **Evaluate Model Performance** 🏆:\n",
    "   - Assess results on both translation and book description tasks.\n",
    "   \n",
    "2. **Fine-Tune Hyperparameters** 🎛️:\n",
    "   - Adjust as necessary to optimize performance.\n",
    "   \n",
    "3. **Test Real-World Examples** 🌍:\n",
    "   - Validate the model with practical scenarios.\n",
    "   \n",
    "4. **Deploy the Model** 🚀:\n",
    "   - Make the model available for use.\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 Documentation\n",
    "\n",
    "Each phase will be thoroughly documented with:\n",
    "\n",
    "- 📊 **Results**\n",
    "- 📝 **Observations**\n",
    "- 💡 **Insights for Improvement**\n",
    "\n",
    "This ensures progress is clearly tracked and potential areas for enhancement are identified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Evaluation**\n",
    "   - Benchmark on Czech NLP tasks\n",
    "   - Compare with baseline models\n",
    "\n",
    "## References\n",
    "\n",
    "1. ParaCrawl (2023). ParaCrawl v9.0. https://paracrawl.eu/v9\n",
    "2. Gemma (2024). Google AI. https://blog.google/technology/ai/gemma-open-models/\n",
    "3. Czech Books Descriptions Dataset. https://huggingface.co/datasets/vojtam/czech_books_descriptions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma-global-competetion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
